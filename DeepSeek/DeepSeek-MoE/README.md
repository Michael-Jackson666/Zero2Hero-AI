# DeepSeek-MoE: æè‡´ä¸“å®¶ç‰¹åŒ–

æœ¬ç›®å½•åŒ…å« DeepSeek è®ºæ–‡ [*DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models*](https://arxiv.org/abs/2401.06066) çš„å­¦ä¹ ç¬”è®°å’Œä»£ç å®ç°ã€‚

## ğŸ“– æ ¸å¿ƒæ€æƒ³

DeepSeekMoE é€šè¿‡ä¸¤å¤§ç­–ç•¥è§£å†³ä¼ ç»Ÿ MoE çš„ä¸“å®¶ç‰¹åŒ–ä¸è¶³é—®é¢˜ï¼š

| é—®é¢˜ | ä¼ ç»Ÿ MoE | DeepSeekMoE è§£å†³æ–¹æ¡ˆ |
|------|----------|---------------------|
| **çŸ¥è¯†æ··åˆ** | å•ä¸ªä¸“å®¶æ‰¿æ‹…å¤šç§çŸ¥è¯† | ç»†ç²’åº¦ä¸“å®¶åˆ†å‰² (1/m å¤§å°) |
| **çŸ¥è¯†å†—ä½™** | å¤šä¸ªä¸“å®¶é‡å¤å­¦ä¹ é€šç”¨çŸ¥è¯† | å…±äº«ä¸“å®¶éš”ç¦» (å§‹ç»ˆæ¿€æ´») |

**ä¸€å¥è¯æ€»ç»“**ï¼šå°†ä¸“å®¶åˆ‡å°ã€åˆ‡å¤šï¼Œå¹¶éš”ç¦»å‡º"é€šç”¨çŸ¥è¯†ä¸“å®¶"ï¼Œè®©è·¯ç”±ä¸“å®¶ä¸“æ³¨äºç‰¹å®šé¢†åŸŸã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
DeepSeek-MoE/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ DeepSeek-MoE.md              # ğŸ“ DeepSeekMoE è¯¦ç»†å­¦ä¹ ç¬”è®°
â”œâ”€â”€ MoEç®€ä»‹.md                   # ğŸ“ MoE åŸºç¡€çŸ¥è¯†ä»‹ç»
â”œâ”€â”€ DeepSeekMoE.png              # ğŸ–¼ï¸ æ¶æ„å›¾
â”œâ”€â”€ MoE Layer.png                # ğŸ–¼ï¸ MoE å±‚ç¤ºæ„å›¾
â””â”€â”€ Code/                        # ğŸ’» PyTorch ä»£ç å®ç°
    â”œâ”€â”€ README.md                # ä»£ç æ–‡æ¡£
    â”œâ”€â”€ experts.py               # ä¸“å®¶ç½‘ç»œ (SwiGLU FFN)
    â”œâ”€â”€ router.py                # Top-K è·¯ç”±ä¸è´Ÿè½½å‡è¡¡
    â”œâ”€â”€ moe_layer.py             # MoE å±‚ (å…±äº«+è·¯ç”±ä¸“å®¶)
    â””â”€â”€ deepseek_moe.py          # å®Œæ•´æ¨¡å‹å®ç°
```

## ğŸ”§ DeepSeekMoE æ¶æ„

```
è¾“å…¥ Token â†’ Self-Attention â†’ MoE Layer â†’ è¾“å‡º
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                           â”‚
              Shared Experts              Routed Experts
              (å§‹ç»ˆæ¿€æ´» K_s ä¸ª)            (Top-K é€‰æ‹© mK-K_s ä¸ª)
                    â”‚                           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                              æ±‚å’Œ + æ®‹å·®
```

## ğŸ“ å…³é”®å…¬å¼

**DeepSeekMoE è¾“å‡º**ï¼š
$$\mathbf{h}_t = \underbrace{\sum_{i=1}^{K_s} \text{FFN}_i(\mathbf{u}_t)}_{\text{å…±äº«ä¸“å®¶}} + \underbrace{\sum_{i=K_s+1}^{mN} g_{i,t} \text{FFN}_i(\mathbf{u}_t)}_{\text{è·¯ç”±ä¸“å®¶}} + \mathbf{u}_t$$

**é—¨æ§æœºåˆ¶**ï¼š
$$s_{i,t} = \text{Softmax}_i(\mathbf{u}_t^T \mathbf{e}_i), \quad g_{i,t} = \begin{cases} s_{i,t}, & \text{if in Top-K} \\ 0, & \text{otherwise} \end{cases}$$

## ğŸ“Š æ¨¡å‹é…ç½®

| æ¨¡å‹ | æ€»å‚æ•° | æ¿€æ´»å‚æ•° | å…±äº«ä¸“å®¶ | è·¯ç”±ä¸“å®¶ | Top-K |
|------|--------|----------|----------|----------|-------|
| DeepSeekMoE-2B | 2.0B | 0.3B | 1 | 63 | 7 |
| DeepSeekMoE-16B | 16.4B | 2.8B | 2 | 64 | 6 |
| DeepSeekMoE-145B | 144.6B | 22.2B | 4 | 128 | 12 |

## ğŸš€ å¿«é€Ÿå¼€å§‹

```python
from Code.deepseek_moe import DeepSeekMoEModel, DeepSeekMoEConfig

config = DeepSeekMoEConfig(
    hidden_size=2048,
    num_shared_experts=2,
    num_routed_experts=64,
    num_experts_per_token=6,
)

model = DeepSeekMoEModel(config)
outputs = model(input_ids)
```

## ğŸ’¡ å…³é”®å‘ç°

1. **ç»„åˆçˆ†ç‚¸**ï¼š64 ä¸ªå°ä¸“å®¶ Top-8 é€‰æ‹©æœ‰ 44 äº¿ç§ç»„åˆï¼ˆvs 16 ä¸“å®¶ Top-2 ä»… 120 ç§ï¼‰
2. **æè‡´ç‰¹åŒ–**ï¼šç§»é™¤ä»»ä¸€ä¸“å®¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ˆè¯´æ˜æ— å†—ä½™ï¼‰
3. **U å‹æœ€ä¼˜**ï¼šçº¯ Dense å’Œçº¯ MoE éƒ½ä¸æ˜¯æœ€ä¼˜ï¼Œæ··åˆç­–ç•¥æ•ˆæœæœ€ä½³

## ğŸ“š å­¦ä¹ è·¯çº¿

1. **MoE åŸºç¡€**ï¼šé˜…è¯» [MoEç®€ä»‹.md](MoEç®€ä»‹.md)
2. **DeepSeekMoE åŸç†**ï¼šé˜…è¯» [DeepSeek-MoE.md](DeepSeek-MoE.md)
3. **ä»£ç å®è·µ**ï¼šè¿è¡Œ [Code/](Code/) ä¸­çš„æ¨¡å—
4. **è®ºæ–‡åŸæ–‡**ï¼š[arXiv:2401.06066](https://arxiv.org/abs/2401.06066)

## ğŸ“„ License

MIT License
