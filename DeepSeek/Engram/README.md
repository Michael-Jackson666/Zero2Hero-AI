# Engram: æ¡ä»¶è®°å¿† â€”â€” LLM çš„æ–°ç¨€ç–æ–¹å‘

æœ¬ç›®å½•åŒ…å« DeepSeek è®ºæ–‡ [*Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models*](https://arxiv.org/abs/2601.07372) çš„å­¦ä¹ ç¬”è®°å’Œä»£ç å®ç°ã€‚

## ğŸ“– æ ¸å¿ƒæ€æƒ³

Engram æå‡ºäº†**æ¡ä»¶è®°å¿† (Conditional Memory)** çš„æ¦‚å¿µï¼Œä½œä¸º MoEï¼ˆæ¡ä»¶è®¡ç®—ï¼‰çš„äº’è¡¥ï¼š

| ç¨€ç–è½´ | åŠŸèƒ½å®šä½ | å¯»å€ä¾èµ– | å¤æ‚åº¦ |
|--------|----------|----------|--------|
| **MoE** | ç»„åˆæ¨ç†ã€é€»è¾‘æ³›åŒ– | åŠ¨æ€ä¾èµ–ï¼ˆRuntime Stateï¼‰ | O(N) |
| **Engram** | çŸ¥è¯†æ£€ç´¢ã€äº‹å®æŸ¥è¡¨ | é™æ€ä¾èµ–ï¼ˆInput Tokenï¼‰ | O(1) |

**ä¸€å¥è¯æ€»ç»“**ï¼šEngram æŠŠ"æ­»è®°ç¡¬èƒŒ"çš„å‚æ•°ä»ç¥ç»ç½‘ç»œä¸­å‰¥ç¦»ï¼Œç”¨ O(1) å“ˆå¸ŒæŸ¥è¡¨å®ç°ï¼Œå¹¶å¯é€šè¿‡é¢„å–å®Œå…¨æ©ç›–é€šä¿¡å»¶è¿Ÿã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
Engram/
â”œâ”€â”€ README.md                              # æœ¬æ–‡ä»¶
â”œâ”€â”€ Engram.md                              # ğŸ“ è¯¦ç»†å­¦ä¹ ç¬”è®°ï¼ˆå…¬å¼æ¨å¯¼ã€æ¶æ„è§£æï¼‰
â”œâ”€â”€ Engram.png                             # ğŸ–¼ï¸ æ¶æ„å›¾
â”œâ”€â”€ Sparsity allocation and Engram scaling.png  # ğŸ“Š ç¨€ç–åˆ†é…å®éªŒå›¾
â””â”€â”€ Code/                                  # ğŸ’» PyTorch ä»£ç å®ç°
    â”œâ”€â”€ README.md                          # ä»£ç æ–‡æ¡£
    â”œâ”€â”€ tokenizer_compression.py           # Token å‹ç¼©ä¸ N-gram æå–
    â”œâ”€â”€ multi_head_hashing.py              # å¤šå¤´å“ˆå¸Œä¸ Embedding æŸ¥æ‰¾
    â”œâ”€â”€ context_aware_gating.py            # ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§
    â”œâ”€â”€ fusion.py                          # æ·±åº¦å·ç§¯èåˆå±‚
    â””â”€â”€ engram.py                          # å®Œæ•´ Engram æ¨¡å—
```

## ğŸ”§ Engram æ¶æ„äº”æ­¥æµç¨‹

```
è¾“å…¥ Token â†’ [1. è¯è¡¨å‹ç¼©] â†’ [2. N-gram æå–] â†’ [3. å¤šå¤´å“ˆå¸Œ]
                                                      â†“
                                              [4. Embedding æŸ¥è¡¨]
                                                      â†“
éšè—çŠ¶æ€ h_t â†’ [5. ä¸Šä¸‹æ–‡é—¨æ§] â† e_t (æ£€ç´¢è®°å¿†)
                    â†“
              [6. èåˆ + æ®‹å·®] â†’ æ›´æ–°åçš„éšè—çŠ¶æ€
```

## ğŸ“ å…³é”®å…¬å¼

**å¤šå¤´å“ˆå¸Œæ£€ç´¢**ï¼š
$$\mathbf{e}_t = \mathop{\Big\Vert}_{n=2}^N \mathop{\Big\Vert}_{k=1}^K \mathbf{E}_{n,k}[\varphi_{n,k}(g_{t,n})]$$

**ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§**ï¼š
$$\alpha_t = \sigma \left( \frac{\text{RMSNorm}(\mathbf{h}_t)^\top \text{RMSNorm}(\mathbf{W}_K \mathbf{e}_t)}{\sqrt{d}} \right)$$

**èåˆè¾“å‡º**ï¼š
$$\mathbf{Y} = \text{SiLU}(\text{Conv1D}(\text{RMSNorm}(\alpha_t \cdot \mathbf{W}_V \mathbf{e}_t))) + \alpha_t \cdot \mathbf{W}_V \mathbf{e}_t$$

## ğŸš€ å¿«é€Ÿå¼€å§‹

```python
from Code.engram import EngramModule, EngramConfig

config = EngramConfig(
    vocab_size=50_000,
    min_n=2, max_n=4,
    num_hash_heads=4,
    embedding_table_size=1_000_000,
    embedding_dim=64,
    hidden_dim=2048,
)

engram = EngramModule(config)

# å‰å‘ä¼ æ’­
updated_states, gating = engram(
    hidden_states=hidden_states,
    input_ids=input_ids,
    return_gating=True
)
```

## ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ï¼šé›¶å¼€é”€é¢„å–

ç”±äº Engram çš„ç´¢å¼•åªä¾èµ–è¾“å…¥ Tokenï¼ˆè€Œéè¿è¡Œæ—¶éšè—çŠ¶æ€ï¼‰ï¼Œå¯ä»¥å®ç°å®Œç¾çš„è®¡ç®—-å­˜å‚¨æµæ°´çº¿ï¼š

- **GPU**ï¼šè®¡ç®— Layer 0-1
- **CPU**ï¼šåŒæ—¶é¢„å– Layer 2 çš„ Engram Embedding
- **ç»“æœ**ï¼šé€šä¿¡å»¶è¿Ÿè¢«è®¡ç®—å®Œå…¨æ©ç›–

## ğŸ“š å­¦ä¹ è·¯çº¿

1. **ç†è®ºç†è§£**ï¼šé˜…è¯» [Engram.md](Engram.md) è¯¦ç»†ç¬”è®°
2. **ä»£ç å®è·µ**ï¼šè¿è¡Œ [Code/](Code/) ä¸­çš„å„æ¨¡å—
3. **è®ºæ–‡åŸæ–‡**ï¼š[arXiv:2601.07372](https://arxiv.org/abs/2601.07372)

## ğŸ“„ License

MIT License
