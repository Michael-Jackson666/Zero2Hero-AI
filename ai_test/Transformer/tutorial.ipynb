{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d30a5da",
   "metadata": {},
   "source": [
    "# 手撕 Transformer：从零实现面试速通教程\n",
    "\n",
    "本教程面向 AI/算法工程师面试，目标是“手撕 Transformer”时能在白板/编辑器中快速、正确、可讲解地实现关键模块与完整骨架。\n",
    "\n",
    "你将学习并实现：\n",
    "- Scaled Dot-Product Attention（带 mask）\n",
    "- Multi-Head Attention（MHA）\n",
    "- Position-wise Feed Forward（FFN）\n",
    "- 残差连接 + LayerNorm\n",
    "- 位置编码（Positional Encoding）\n",
    "- EncoderLayer / DecoderLayer\n",
    "- Transformer Encoder-Decoder 总装\n",
    "- 贪心解码（Greedy Decode）与一个极简玩具任务\n",
    "\n",
    "建议：面试中优先保证“正确 + 清晰 + 注释完善 + 形状无误”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc581d4",
   "metadata": {},
   "source": [
    "# 环境与依赖\n",
    "\n",
    "- Python ≥ 3.8\n",
    "- 推荐使用 PyTorch（面试常用）\n",
    "- 若无 torch，可按需安装或在纸上仅写伪代码/接口签名\n",
    "\n",
    "下面代码会尝试导入 torch 并给出缺失提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da1d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Import and quick check\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch import Tensor\n",
    "    print(torch.__version__)\n",
    "except Exception as e:\n",
    "    print(\"[Warn] torch not available. You can still read/understand the code.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b61bf",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention（带 Mask）\n",
    "\n",
    "## 核心思想\n",
    "注意力机制的本质是**加权求和**：对于每个查询位置，计算它与所有键位置的相似度，然后用这些相似度作为权重对值进行加权平均。\n",
    "\n",
    "## 输入张量及其含义\n",
    "令：\n",
    "- $Q\\in\\mathbb{R}^{B\\times H\\times T_q\\times d_k}$：**查询（Query）张量**\n",
    "  - $B$：批次大小（Batch size）\n",
    "  - $H$：注意力头数（num Heads）\n",
    "  - $T_q$：查询序列长度（Query sequence length）\n",
    "  - $d_k$：每个头的键/查询维度（Key/Query dimension per head）\n",
    "  \n",
    "- $K\\in\\mathbb{R}^{B\\times H\\times T_k\\times d_k}$：**键（Key）张量**\n",
    "  - $T_k$：键序列长度（Key sequence length，通常等于值序列长度）\n",
    "  \n",
    "- $V\\in\\mathbb{R}^{B\\times H\\times T_k\\times d_v}$：**值（Value）张量**\n",
    "  - $d_v$：每个头的值维度（Value dimension per head，通常 $d_v=d_k$）\n",
    "\n",
    "## 计算步骤\n",
    "\n",
    "### 步骤 1: 计算注意力分数（缩放点积）\n",
    "$$\n",
    "\\mathrm{scores} \\,=\\, \\frac{QK^{\\top}}{\\sqrt{d_k}} \\in \\mathbb{R}^{B\\times H\\times T_q\\times T_k}\n",
    "$$\n",
    "\n",
    "**维度分析：**\n",
    "- $Q$: $(B, H, T_q, d_k)$\n",
    "- $K^{\\top}$: $(B, H, d_k, T_k)$ ← 转置最后两维\n",
    "- $QK^{\\top}$: $(B, H, T_q, T_k)$ ← 批量矩阵乘法\n",
    "- 除以 $\\sqrt{d_k}$ 进行缩放，防止点积过大导致梯度消失\n",
    "\n",
    "**物理意义：** `scores[b,h,i,j]` 表示第 $b$ 个样本、第 $h$ 个头中，查询位置 $i$ 对键位置 $j$ 的**相似度得分**。\n",
    "\n",
    "### 步骤 2: 应用掩码（Mask）\n",
    "令 $M\\in\\{0,1\\}^{B\\times 1\\times T_q\\times T_k}$ 为可见性掩码（1=可见，0=不可见）。定义加性掩码：\n",
    "$$\n",
    "\\tilde{M} \\,=\\, (1-M)\\cdot (-\\infty)\n",
    "$$\n",
    "\n",
    "将掩码加到分数上：\n",
    "$$\n",
    "\\mathrm{scores}_{\\text{masked}} = \\mathrm{scores} + \\tilde{M}\n",
    "$$\n",
    "\n",
    "**作用：** 被遮挡位置（$M=0$）的分数变为 $-\\infty$，经过 softmax 后概率趋近 0，实现\"屏蔽\"效果。\n",
    "\n",
    "### 步骤 3: Softmax 归一化\n",
    "$$\n",
    "\\mathrm{attn} \\,=\\, \\mathrm{softmax}(\\mathrm{scores}_{\\text{masked}})\\in \\mathbb{R}^{B\\times H\\times T_q\\times T_k}\n",
    "$$\n",
    "\n",
    "**操作：** 对最后一维（$T_k$ 维）做 softmax，使得对每个查询位置 $i$，所有键位置的权重和为 1：\n",
    "$$\n",
    "\\sum_{j=1}^{T_k} \\mathrm{attn}[b,h,i,j] = 1\n",
    "$$\n",
    "\n",
    "### 步骤 4: 加权求和输出\n",
    "$$\n",
    "\\mathrm{out} \\,=\\, \\mathrm{attn}* V\\in \\mathbb{R}^{B\\times H\\times T_q\\times d_v}\n",
    "$$\n",
    "\n",
    "**维度分析：**\n",
    "- $\\mathrm{attn}$: $(B, H, T_q, T_k)$\n",
    "- $V$: $(B, H, T_k, d_v)$\n",
    "- $\\mathrm{attn} \\cdot V$: $(B, H, T_q, d_v)$ ← 批量矩阵乘法\n",
    "\n",
    "**物理意义：** 输出的每个位置 $i$ 是所有值位置的**加权平均**，权重由注意力分数决定。\n",
    "\n",
    "## 数值稳定性技巧\n",
    "- 使用 `float('-inf')` 近似 $-\\infty$，使被遮挡位置在 softmax 后概率趋近 0\n",
    "- 缩放因子 $\\sqrt{d_k}$ 防止点积值过大，避免 softmax 饱和导致梯度消失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c57c4",
   "metadata": {},
   "source": [
    "**可视化结构：**\n",
    "\n",
    "![Scaled Dot-Product Attention](Scaled_dot-product_attention.png)\n",
    "\n",
    "上图展示了缩放点积注意力的计算流程：输入 Q、K、V 经过矩阵乘法、缩放、Mask、Softmax，最后加权求和得到输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2464279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5, 8]) torch.Size([2, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Q: Tensor, K: Tensor, V: Tensor, mask: Optional[Tensor] = None) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Q: (B, H, T_q, d_k)\n",
    "        K: (B, H, T_k, d_k)\n",
    "        V: (B, H, T_k, d_v)\n",
    "        mask: (B, 1, T_q, T_k) 或 (B, H, T_q, T_k), 1表示可见, 0表示遮挡\n",
    "        返回: (out, attn)\n",
    "          out: (B, H, T_q, d_v)\n",
    "          attn: (B, H, T_q, T_k)\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)  # (B,H,T_q,T_k)\n",
    "        if mask is not None:\n",
    "            # 将不可见位置置为 -inf\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = attn @ V  # (B,H,T_q,d_v)\n",
    "        return out, attn\n",
    "\n",
    "# quick shape test (no torch run here if not installed)\n",
    "if 'torch' in globals():\n",
    "    B, H, T_q, T_k, d_k, d_v = 2, 4, 5, 6, 8, 8\n",
    "    Q = torch.randn(B, H, T_q, d_k)\n",
    "    K = torch.randn(B, H, T_k, d_k)\n",
    "    V = torch.randn(B, H, T_k, d_v)\n",
    "    mask = torch.ones(B, 1, T_q, T_k)\n",
    "    attn = ScaledDotProductAttention()\n",
    "    out, w = attn(Q, K, V, mask)\n",
    "    print(out.shape, w.shape)  # expect: (2,4,5,8) (2,4,5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572cfa0",
   "metadata": {},
   "source": [
    "# Multi-Head Attention（MHA）\n",
    "\n",
    "## 核心思想\n",
    "单头注意力只能学习一种模式，多头注意力通过**并行运行多个注意力头**，让模型同时关注不同的表示子空间，捕获更丰富的特征关系。\n",
    "\n",
    "## 参数设定\n",
    "- $H$：头数（num_heads）\n",
    "- $d_{\\text{model}}$：模型总维度（embedding dimension）\n",
    "- $d_k = d_{\\text{model}}/H$：每个头的维度（dimension per head）\n",
    "\n",
    "**设计原则：** 保持总参数量不变，$H \\times d_k = d_{\\text{model}}$\n",
    "\n",
    "## 输入输出\n",
    "- **输入：** $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$\n",
    "  - $B$：批次大小\n",
    "  - $T$：序列长度\n",
    "  - $d_{\\text{model}}$：特征维度（如 512）\n",
    "  \n",
    "- **输出：** $\\mathrm{MHA}(X)\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$（形状不变）\n",
    "\n",
    "## 计算流程\n",
    "\n",
    "### 步骤 1: 线性投影（生成 Q, K, V）\n",
    "对每个头 $i\\in\\{1,\\dots,H\\}$，分别投影：\n",
    "$$\n",
    "Q_i = X W_Q^{(i)},\\quad K_i = X W_K^{(i)},\\quad V_i = X W_V^{(i)}\n",
    "$$\n",
    "\n",
    "**权重矩阵：**\n",
    "- $W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_k}$\n",
    "\n",
    "**维度变化：**\n",
    "- $X$: $(B, T, d_{\\text{model}})$\n",
    "- $W_Q^{(i)}$: $(d_{\\text{model}}, d_k)$\n",
    "- $Q_i = X W_Q^{(i)}$: $(B, T, d_k)$\n",
    "\n",
    "**实现技巧（本教程）：** 实际代码中使用 $W_Q\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{\\text{model}}}$ 一次性投影，再 reshape 分头：\n",
    "$$\n",
    "Q_{\\text{all}} = X W_Q \\;\\in\\; \\mathbb{R}^{B\\times T\\times d_{\\text{model}}} \\;\\xrightarrow{\\text{reshape}}\\; \\mathbb{R}^{B\\times T\\times H\\times d_k} \\;\\xrightarrow{\\text{transpose}}\\; \\mathbb{R}^{B\\times H\\times T\\times d_k}\n",
    "$$\n",
    "\n",
    "### 步骤 2: 头内注意力（并行计算）\n",
    "对每个头独立计算缩放点积注意力：\n",
    "$$\n",
    "\\mathrm{head}_i = \\mathrm{Attention}(Q_i, K_i, V_i) = \\mathrm{softmax}\\!\\left(\\frac{Q_i K_i^{\\top}}{\\sqrt{d_k}} + \\tilde{M}\\right)V_i\n",
    "$$\n",
    "\n",
    "**维度：**\n",
    "- 输入 $Q_i, K_i, V_i$: $(B, H, T, d_k)$（已包含所有头）\n",
    "- 输出 $\\mathrm{head}_i$: $(B, H, T, d_k)$\n",
    "\n",
    "### 步骤 3: 拼接所有头\n",
    "将 $H$ 个头的输出沿特征维拼接：\n",
    "$$\n",
    "\\mathrm{Concat}(\\mathrm{head}_1,\\dots,\\mathrm{head}_H) \\;\\in\\; \\mathbb{R}^{B\\times T\\times (H\\cdot d_k)} = \\mathbb{R}^{B\\times T\\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "**操作：** \n",
    "- 转置：$(B, H, T, d_k) \\to (B, T, H, d_k)$\n",
    "- Reshape：$(B, T, H, d_k) \\to (B, T, H \\times d_k)$\n",
    "\n",
    "### 步骤 4: 输出投影\n",
    "通过线性层映射回原维度：\n",
    "$$\n",
    "\\mathrm{MHA}(X) = \\mathrm{Concat}(\\mathrm{head}_1,\\dots,\\mathrm{head}_H)\\, W_O\n",
    "$$\n",
    "\n",
    "**权重矩阵：**\n",
    "- $W_O\\in\\mathbb{R}^{(H\\cdot d_k)\\times d_{\\text{model}}} = \\mathbb{R}^{d_{\\text{model}}\\times d_{\\text{model}}}$\n",
    "\n",
    "**最终输出：** $(B, T, d_{\\text{model}})$\n",
    "\n",
    "## 自注意力 vs 交叉注意力\n",
    "- **自注意力（Self-Attention）：** $Q, K, V$ 都来自同一输入 $X$\n",
    "- **交叉注意力（Cross-Attention）：** $Q$ 来自一个输入，$K, V$ 来自另一个输入（如 Decoder 中 $Q$ 来自 Decoder，$K,V$ 来自 Encoder）\n",
    "\n",
    "## 参数量分析\n",
    "每个 MHA 模块的参数：\n",
    "- $W_Q, W_K, W_V$: $3 \\times d_{\\text{model}} \\times d_{\\text{model}}$\n",
    "- $W_O$: $d_{\\text{model}} \\times d_{\\text{model}}$\n",
    "- **总计：** $4 d_{\\text{model}}^2$ 参数（不含偏置）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f78ad",
   "metadata": {},
   "source": [
    "**可视化结构：**\n",
    "\n",
    "![Multi-Head Attention](Multi-Head_Attention.png)\n",
    "\n",
    "上图展示了多头注意力的完整流程：输入经过线性投影分成多个头，每个头独立进行注意力计算，最后将所有头的输出拼接并通过线性层映射回原始维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8652b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 32]) torch.Size([2, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _split_heads(self, x: Tensor) -> Tensor:\n",
    "        # x: (B,T,d_model) -> (B,H,T,d_k)\n",
    "        B, T, _ = x.shape\n",
    "        x = x.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def _combine_heads(self, x: Tensor) -> Tensor:\n",
    "        # x: (B,H,T,d_k) -> (B,T,d_model)\n",
    "        B, H, T, d_k = x.shape\n",
    "        x = x.transpose(1, 2).contiguous().view(B, T, H * d_k)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_q: Tensor, x_kv: Tensor, mask: Optional[Tensor] = None) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        x_q: (B,T_q,d_model)\n",
    "        x_kv: (B,T_k,d_model)\n",
    "        mask: (B,1,T_q,T_k) 或 (B,H,T_q,T_k)\n",
    "        返回: (out, attn)\n",
    "        \"\"\"\n",
    "        Q = self._split_heads(self.W_q(x_q))  # (B,H,T_q,d_k)\n",
    "        K = self._split_heads(self.W_k(x_kv)) # (B,H,T_k,d_k)\n",
    "        V = self._split_heads(self.W_v(x_kv)) # (B,H,T_k,d_k)\n",
    "\n",
    "        out, attn = self.attn(Q, K, V, mask)   # out: (B,H,T_q,d_k)\n",
    "        out = self._combine_heads(out)         # (B,T_q,d_model)\n",
    "        out = self.W_o(out)                    # (B,T_q,d_model)\n",
    "        out = self.dropout(out)\n",
    "        return out, attn\n",
    "\n",
    "# quick shape test\n",
    "if 'torch' in globals():\n",
    "    B, T_q, T_k, d_model, H = 2, 5, 6, 32, 4\n",
    "    x_q = torch.randn(B, T_q, d_model)\n",
    "    x_kv = torch.randn(B, T_k, d_model)\n",
    "    mask = torch.ones(B, 1, T_q, T_k)\n",
    "    mha = MultiHeadAttention(d_model, H)\n",
    "    y, a = mha(x_q, x_kv, mask)\n",
    "    print(y.shape, a.shape)  # expect: (2,5,32) (2,4,5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64b400",
   "metadata": {},
   "source": [
    "# Positional Encoding（位置编码）\n",
    "\n",
    "## 为什么需要位置编码？\n",
    "注意力机制本身是**置换不变**的（permutation-invariant）：交换序列顺序，输出也会相应交换，但注意力权重不变。为了让模型感知位置信息（如\"猫吃鱼\"和\"鱼吃猫\"的区别），需要显式注入位置信息。\n",
    "\n",
    "## 两种常见实现方式\n",
    "1. **固定位置编码（Sinusoidal）**：使用正弦/余弦函数，无需学习参数（本教程采用）\n",
    "2. **可学习位置编码**：`nn.Embedding(max_len, d_model)`，需要训练\n",
    "\n",
    "## 正弦/余弦位置编码公式\n",
    "对于位置 $\\mathrm{pos}\\in\\{0,1,\\dots,T-1\\}$ 和维度索引 $i\\in\\{0,1,\\dots,\\lfloor\\tfrac{d_{\\text{model}}}{2}\\rfloor-1\\}$：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{PE}[\\mathrm{pos},\\,2i] &\\;=\\; \\sin\\!\\left(\\frac{\\mathrm{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), \\\\[0.5em]\n",
    "\\mathrm{PE}[\\mathrm{pos},\\,2i+1] &\\;=\\; \\cos\\!\\left(\\frac{\\mathrm{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 公式解析\n",
    "- **偶数维度**（$2i$）：使用正弦函数\n",
    "- **奇数维度**（$2i+1$）：使用余弦函数\n",
    "- **频率**：$\\omega_i = \\frac{1}{10000^{2i/d_{\\text{model}}}}$\n",
    "  - 低维度（$i$ 小）：高频振荡，捕捉局部位置差异\n",
    "  - 高维度（$i$ 大）：低频振荡，捕捉远距离位置关系\n",
    "\n",
    "### 维度示例\n",
    "假设 $d_{\\text{model}}=512$，$T=100$（序列长度）：\n",
    "- $\\mathrm{PE}$ 形状：$(T, d_{\\text{model}}) = (100, 512)$\n",
    "- `PE[0, :]`：位置 0 的编码向量（512 维）\n",
    "- `PE[:, 0]`：所有位置在第 0 维的编码值\n",
    "\n",
    "## 使用方式\n",
    "将位置编码**直接加**到词嵌入上：\n",
    "$$\n",
    "X_{\\text{pos}} = X_{\\text{embed}} + \\mathrm{PE}\n",
    "$$\n",
    "\n",
    "**维度匹配：**\n",
    "- $X_{\\text{embed}}$: $(B, T, d_{\\text{model}})$ ← 词嵌入\n",
    "- $\\mathrm{PE}$: $(1, T, d_{\\text{model}})$ ← 位置编码（广播到批次维）\n",
    "- $X_{\\text{pos}}$: $(B, T, d_{\\text{model}})$ ← 最终输入\n",
    "\n",
    "## 为什么使用正弦/余弦？\n",
    "1. **相对位置关系：** $\\mathrm{PE}_{\\mathrm{pos}+k}$ 可以表示为 $\\mathrm{PE}_{\\mathrm{pos}}$ 的线性函数\n",
    "2. **外推能力：** 理论上可以处理比训练时更长的序列\n",
    "3. **无需学习：** 减少参数量，避免过拟合\n",
    "\n",
    "## 实现细节\n",
    "```python\n",
    "# 生成位置索引\n",
    "position = torch.arange(0, max_len).unsqueeze(1)  # (T, 1)\n",
    "\n",
    "# 生成频率项\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "# 等价于: 10000^(-2i/d_model)\n",
    "\n",
    "# 填充 PE 矩阵\n",
    "pe[:, 0::2] = torch.sin(position * div_term)  # 偶数列\n",
    "pe[:, 1::2] = torch.cos(position * div_term)  # 奇数列\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2fb36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (T, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (T,1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1,T,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x: (B,T,d_model)\n",
    "        T = x.size(1)\n",
    "        x = x + self.pe[:, :T, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# quick test\n",
    "if 'torch' in globals():\n",
    "    pe = PositionalEncoding(32)\n",
    "    x = torch.zeros(2, 10, 32)\n",
    "    y = pe(x)\n",
    "    print(y.shape)  # (2,10,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904f4e1",
   "metadata": {},
   "source": [
    "# FFN + LayerNorm + 残差连接\n",
    "\n",
    "## Position-wise Feed-Forward Network (FFN)\n",
    "\n",
    "### 结构\n",
    "两层全连接网络，对每个位置**独立**处理（不跨位置交互）：\n",
    "$$\n",
    "\\mathrm{FFN}(x) = W_2\\,\\sigma(W_1 x + b_1) + b_2\n",
    "$$\n",
    "\n",
    "**参数：**\n",
    "- $W_1\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{\\mathrm{ff}}}$：第一层权重（扩张）\n",
    "- $W_2\\in\\mathbb{R}^{d_{\\mathrm{ff}}\\times d_{\\text{model}}}$：第二层权重（压缩）\n",
    "- $\\sigma$：激活函数（ReLU 或 GELU）\n",
    "\n",
    "### 维度变化\n",
    "$$\n",
    "(B, T, d_{\\text{model}}) \\xrightarrow{W_1} (B, T, d_{\\mathrm{ff}}) \\xrightarrow{\\sigma} (B, T, d_{\\mathrm{ff}}) \\xrightarrow{W_2} (B, T, d_{\\text{model}})\n",
    "$$\n",
    "\n",
    "**典型值：** $d_{\\mathrm{ff}} = 4 \\times d_{\\text{model}}$（如 512 → 2048 → 512）\n",
    "\n",
    "### 作用\n",
    "- **非线性变换**：引入非线性，增强表达能力\n",
    "- **特征混合**：每个位置独立地在高维空间中进行特征变换\n",
    "- **位置独立**：与注意力的\"跨位置交互\"形成互补\n",
    "\n",
    "---\n",
    "\n",
    "## 残差连接（Residual Connection）\n",
    "$$\n",
    "\\text{output} = x + \\text{Sublayer}(x)\n",
    "$$\n",
    "\n",
    "**作用：**\n",
    "1. **缓解梯度消失**：梯度可直接通过恒等映射反向传播\n",
    "2. **简化学习**：子层只需学习\"残差\"（变化量），而非完整映射\n",
    "3. **稳定训练**：允许堆叠更深的网络\n",
    "\n",
    "---\n",
    "\n",
    "## Layer Normalization（层归一化）\n",
    "对每个样本的特征维度做归一化：\n",
    "$$\n",
    "\\mathrm{LN}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "**计算：**\n",
    "- $\\mu = \\frac{1}{d_{\\text{model}}}\\sum_{i=1}^{d_{\\text{model}}} x_i$：均值\n",
    "- $\\sigma^2 = \\frac{1}{d_{\\text{model}}}\\sum_{i=1}^{d_{\\text{model}}} (x_i - \\mu)^2$：方差\n",
    "- $\\gamma, \\beta\\in\\mathbb{R}^{d_{\\text{model}}}$：可学习的缩放和偏移参数\n",
    "- $\\epsilon$：数值稳定项（如 $10^{-5}$）\n",
    "\n",
    "**作用：** 稳定训练，加速收敛，减少对初始化的敏感度\n",
    "\n",
    "---\n",
    "\n",
    "## 两种组合顺序\n",
    "\n",
    "### Post-LN（本教程采用，原论文）\n",
    "$$\n",
    "y = \\mathrm{LN}\\big(x + \\mathrm{Sublayer}(x)\\big)\n",
    "$$\n",
    "\n",
    "**流程：** 子层输出 → 残差连接 → LayerNorm\n",
    "\n",
    "**特点：**\n",
    "- 梯度直接流经子层，训练初期可能不稳定\n",
    "- 需要 warm-up 学习率策略\n",
    "\n",
    "### Pre-LN（现代常用）\n",
    "$$\n",
    "y = x + \\mathrm{Sublayer}(\\mathrm{LN}(x))\n",
    "$$\n",
    "\n",
    "**流程：** LayerNorm → 子层 → 残差连接\n",
    "\n",
    "**特点：**\n",
    "- 更稳定，易于训练深层网络\n",
    "- 无需 warm-up，对学习率不敏感\n",
    "- GPT-2/3、BERT 等现代模型多采用此方式\n",
    "\n",
    "---\n",
    "\n",
    "## 完整子层结构（Post-LN）\n",
    "```\n",
    "输入 x (B, T, d_model)\n",
    "    ↓\n",
    "子层(MHA/FFN) → sublayer_out\n",
    "    ↓\n",
    "x + sublayer_out  (残差)\n",
    "    ↓\n",
    "LayerNorm\n",
    "    ↓\n",
    "输出 y (B, T, d_model)\n",
    "```\n",
    "\n",
    "**参数量：**\n",
    "- FFN: $2 \\times d_{\\text{model}} \\times d_{\\mathrm{ff}}$ （约 $8 d_{\\text{model}}^2$）\n",
    "- LayerNorm: $2 \\times d_{\\text{model}}$（$\\gamma$ 和 $\\beta$）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1369921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 32])\n",
      "torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0, activation: str = 'relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.act = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError('activation must be relu or gelu')\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.fc2(self.dropout(self.act(self.fc1(x))))\n",
    "\n",
    "class ResidualLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model, eps=eps)\n",
    "\n",
    "    def forward(self, x: Tensor, sublayer_out: Tensor) -> Tensor:\n",
    "        # x + sublayer(x) 再做 LN\n",
    "        return self.ln(x + sublayer_out)\n",
    "\n",
    "# quick test\n",
    "if 'torch' in globals():\n",
    "    ff = FeedForward(32, 64)\n",
    "    x = torch.randn(2, 10, 32)\n",
    "    y = ff(x)\n",
    "    print(y.shape)  # (2,10,32)\n",
    "    ln = ResidualLayerNorm(32)\n",
    "    z = ln(x, y)\n",
    "    print(z.shape)  # (2,10,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250c70e",
   "metadata": {},
   "source": [
    "# EncoderLayer / DecoderLayer\n",
    "\n",
    "## 符号约定\n",
    "设自注意力函数：\n",
    "$$\n",
    "\\mathrm{Att}(Q,K,V,M)=\\mathrm{softmax}\\!\\left(\\tfrac{QK^\\top}{\\sqrt{d_k}}+\\tilde{M}\\right)V\n",
    "$$\n",
    "\n",
    "其中 $M$ 是可见性掩码（mask）。\n",
    "\n",
    "---\n",
    "\n",
    "## EncoderLayer（编码器层）\n",
    "\n",
    "### 结构\n",
    "由两个子层组成：\n",
    "1. **多头自注意力（Multi-Head Self-Attention）**\n",
    "2. **前馈网络（FFN）**\n",
    "\n",
    "每个子层后都有**残差连接 + LayerNorm**。\n",
    "\n",
    "### 数学表达（Post-LN）\n",
    "输入 $X\\in\\mathbb{R}^{B\\times T_s\\times d_{\\text{model}}}$（源序列）：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{x}_1 &= \\mathrm{MHA}(X, X, X, M_{\\text{src}}), \\quad &\\text{← 自注意力}\\\\\n",
    "X' &= \\mathrm{LN}\\big(X + \\tilde{x}_1\\big), \\quad &\\text{← 残差+归一化}\\\\\n",
    "\\tilde{x}_2 &= \\mathrm{FFN}(X'), \\quad &\\text{← 前馈网络}\\\\\n",
    "Y &= \\mathrm{LN}\\big(X' + \\tilde{x}_2\\big). \\quad &\\text{← 残差+归一化}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 维度追踪\n",
    "- 输入 $X$: $(B, T_s, d_{\\text{model}})$\n",
    "- 自注意力输出 $\\tilde{x}_1$: $(B, T_s, d_{\\text{model}})$\n",
    "- 第一次 LN 后 $X'$: $(B, T_s, d_{\\text{model}})$\n",
    "- FFN 输出 $\\tilde{x}_2$: $(B, T_s, d_{\\text{model}})$\n",
    "- 最终输出 $Y$: $(B, T_s, d_{\\text{model}})$\n",
    "\n",
    "**关键点：** 自注意力中 $Q=K=V$，都来自同一输入 $X$，无需外部记忆。\n",
    "\n",
    "---\n",
    "\n",
    "## DecoderLayer（解码器层）\n",
    "\n",
    "### 结构\n",
    "由三个子层组成：\n",
    "1. **掩码多头自注意力（Masked Multi-Head Self-Attention）**\n",
    "2. **编码器-解码器注意力（Cross-Attention）**\n",
    "3. **前馈网络（FFN）**\n",
    "\n",
    "每个子层后都有**残差连接 + LayerNorm**。\n",
    "\n",
    "### 数学表达（Post-LN）\n",
    "输入：\n",
    "- $Y\\in\\mathbb{R}^{B\\times T_t\\times d_{\\text{model}}}$：目标序列（已解码部分）\n",
    "- $\\mathrm{Mem}\\in\\mathbb{R}^{B\\times T_s\\times d_{\\text{model}}}$：编码器输出（源序列的编码表示）\n",
    "\n",
    "计算流程：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{y}_1 &= \\mathrm{MHA}(Y, Y, Y, M_{\\text{causal}}), \\quad &\\text{← 掩码自注意力}\\\\\n",
    "Y' &= \\mathrm{LN}\\big(Y + \\tilde{y}_1\\big), \\quad &\\text{← 残差+归一化}\\\\\n",
    "\\tilde{y}_2 &= \\mathrm{MHA}(Y', \\mathrm{Mem}, \\mathrm{Mem}, M_{\\text{cross}}), \\quad &\\text{← 交叉注意力}\\\\\n",
    "Y'' &= \\mathrm{LN}\\big(Y' + \\tilde{y}_2\\big), \\quad &\\text{← 残差+归一化}\\\\\n",
    "\\tilde{y}_3 &= \\mathrm{FFN}(Y''), \\quad &\\text{← 前馈网络}\\\\\n",
    "Z &= \\mathrm{LN}\\big(Y'' + \\tilde{y}_3\\big). \\quad &\\text{← 残差+归一化}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 维度追踪\n",
    "- 输入 $Y$: $(B, T_t, d_{\\text{model}})$\n",
    "- 编码器记忆 $\\mathrm{Mem}$: $(B, T_s, d_{\\text{model}})$\n",
    "- 掩码自注意力输出 $\\tilde{y}_1$: $(B, T_t, d_{\\text{model}})$\n",
    "- 第一次 LN 后 $Y'$: $(B, T_t, d_{\\text{model}})$\n",
    "- 交叉注意力输出 $\\tilde{y}_2$: $(B, T_t, d_{\\text{model}})$ ← $Q$ 来自 $Y'$，$K,V$ 来自 $\\mathrm{Mem}$\n",
    "- 第二次 LN 后 $Y''$: $(B, T_t, d_{\\text{model}})$\n",
    "- FFN 输出 $\\tilde{y}_3$: $(B, T_t, d_{\\text{model}})$\n",
    "- 最终输出 $Z$: $(B, T_t, d_{\\text{model}})$\n",
    "\n",
    "---\n",
    "\n",
    "## 关键掩码机制\n",
    "\n",
    "### 1. Encoder Padding Mask ($M_{\\text{src}}$)\n",
    "**作用：** 屏蔽填充位置（PAD token），防止注意力关注无效位置\n",
    "\n",
    "**形状：** $(B, 1, T_s, T_s)$\n",
    "\n",
    "**示例：** 若序列 `[3, 5, 7, PAD, PAD]`，则 mask 为：\n",
    "```\n",
    "[[1, 1, 1, 0, 0],\n",
    " [1, 1, 1, 0, 0],\n",
    " [1, 1, 1, 0, 0],\n",
    " [1, 1, 1, 0, 0],\n",
    " [1, 1, 1, 0, 0]]\n",
    "```\n",
    "\n",
    "### 2. Decoder Causal Mask ($M_{\\text{causal}}$，下三角掩码）\n",
    "**作用：** 在自回归解码时，**防止未来信息泄露**，位置 $i$ 只能看到位置 $\\leq i$ 的信息\n",
    "\n",
    "**形状：** $(1, 1, T_t, T_t)$（所有样本共享）\n",
    "\n",
    "**示例：** $T_t=5$ 时的掩码：\n",
    "```\n",
    "[[1, 0, 0, 0, 0],    ← 位置0只能看自己\n",
    " [1, 1, 0, 0, 0],    ← 位置1可以看0,1\n",
    " [1, 1, 1, 0, 0],    ← 位置2可以看0,1,2\n",
    " [1, 1, 1, 1, 0],\n",
    " [1, 1, 1, 1, 1]]    ← 位置4可以看全部\n",
    "```\n",
    "\n",
    "### 3. Cross-Attention Mask ($M_{\\text{cross}}$)\n",
    "**作用：** 屏蔽编码器的填充位置，防止解码器关注源序列的无效位置\n",
    "\n",
    "**形状：** $(B, 1, T_t, T_s)$\n",
    "\n",
    "**注意：** 查询长度为 $T_t$（目标），键长度为 $T_s$（源）\n",
    "\n",
    "---\n",
    "\n",
    "## Encoder vs Decoder 对比\n",
    "\n",
    "| 特性 | EncoderLayer | DecoderLayer |\n",
    "|------|--------------|--------------|\n",
    "| 自注意力类型 | 双向（可见全部位置） | 单向（仅可见历史） |\n",
    "| 注意力层数 | 1（自注意力） | 2（自注意力 + 交叉注意力） |\n",
    "| 掩码类型 | Padding mask | Causal + Padding mask |\n",
    "| 输入依赖 | 仅源序列 | 源序列 + 目标序列 |\n",
    "| 并行性 | 完全并行 | 训练并行，推理串行 |\n",
    "\n",
    "---\n",
    "\n",
    "## 参数量分析（单层）\n",
    "- **EncoderLayer:**\n",
    "  - MHA: $4d_{\\text{model}}^2$\n",
    "  - FFN: $8d_{\\text{model}}^2$\n",
    "  - LayerNorm: $4d_{\\text{model}}$\n",
    "  - **总计:** ≈ $12d_{\\text{model}}^2$\n",
    "\n",
    "- **DecoderLayer:**\n",
    "  - Masked MHA: $4d_{\\text{model}}^2$\n",
    "  - Cross MHA: $4d_{\\text{model}}^2$\n",
    "  - FFN: $8d_{\\text{model}}^2$\n",
    "  - LayerNorm: $6d_{\\text{model}}$\n",
    "  - **总计:** ≈ $16d_{\\text{model}}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pad_mask(q_len: int, k_len: int, q_pad: Tensor | None, k_pad: Tensor | None) -> Tensor:\n",
    "    \"\"\"\n",
    "    构造 padding mask（1 可见, 0 屏蔽），形状 (B,1,q_len,k_len)\n",
    "    q_pad/k_pad: (B,T) 中 1 表示 pad 位置\n",
    "    \"\"\"\n",
    "    if q_pad is None and k_pad is None:\n",
    "        return None\n",
    "    if q_pad is None:\n",
    "        q_mask = torch.zeros_like(k_pad)\n",
    "    else:\n",
    "        q_mask = q_pad\n",
    "    if k_pad is None:\n",
    "        k_mask = torch.zeros_like(q_mask)\n",
    "    else:\n",
    "        k_mask = k_pad\n",
    "    # 可见位置=1，即非pad\n",
    "    q_visible = (q_mask == 0).unsqueeze(2)  # (B,T_q,1)\n",
    "    k_visible = (k_mask == 0).unsqueeze(1)  # (B,1,T_k)\n",
    "    mask = q_visible & k_visible            # (B,T_q,T_k)\n",
    "    return mask.unsqueeze(1)                # (B,1,T_q,T_k)\n",
    "\n",
    "\n",
    "def make_subsequent_mask(T: int) -> Tensor:\n",
    "    \"\"\"Decoder 自注意力的下三角可见性掩码（1 可见, 0 屏蔽），形状 (1,1,T,T)\"\"\"\n",
    "    return torch.tril(torch.ones(T, T, dtype=torch.bool)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = ResidualLayerNorm(d_model)\n",
    "        self.norm2 = ResidualLayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, src_mask: Optional[Tensor] = None) -> tuple[Tensor, Tensor]:\n",
    "        # Self-Attention\n",
    "        sa_out, sa_w = self.self_attn(x, x, src_mask)\n",
    "        x = self.norm1(x, self.dropout(sa_out))\n",
    "        # FFN\n",
    "        ff_out = self.ffn(x)\n",
    "        x = self.norm2(x, self.dropout(ff_out))\n",
    "        return x, sa_w\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = ResidualLayerNorm(d_model)\n",
    "        self.norm2 = ResidualLayerNorm(d_model)\n",
    "        self.norm3 = ResidualLayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y: Tensor, memory: Tensor, tgt_mask: Optional[Tensor], memory_mask: Optional[Tensor]) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n",
    "        # Masked Self-Attention (decoder)\n",
    "        sa_out, sa_w = self.self_attn(y, y, tgt_mask)\n",
    "        y = self.norm1(y, self.dropout(sa_out))\n",
    "        # Cross-Attention: Q=decoder, K/V=encoder memory\n",
    "        ca_out, ca_w = self.cross_attn(y, memory, memory_mask)\n",
    "        y = self.norm2(y, self.dropout(ca_out))\n",
    "        # FFN\n",
    "        ff_out = self.ffn(y)\n",
    "        y = self.norm3(y, self.dropout(ff_out))\n",
    "        return y, (sa_w, ca_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b6ff6",
   "metadata": {},
   "source": [
    "# 总装：Transformer Encoder-Decoder\n",
    "- 词嵌入 + 位置编码\n",
    "- N 层 EncoderLayer / DecoderLayer 堆叠\n",
    "- 输出线性层映射到词表大小\n",
    "- 解码时使用贪心或 beam search（本教程实现贪心）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30769848",
   "metadata": {},
   "source": [
    "**Transformer 整体架构：**\n",
    "\n",
    "![Transformer Architecture](attention_architerture.png)\n",
    "\n",
    "上图展示了完整的 Transformer Encoder-Decoder 架构：\n",
    "- **左侧 Encoder**：输入嵌入 + 位置编码 → N×(多头自注意力 + FFN)\n",
    "- **右侧 Decoder**：输出嵌入 + 位置编码 → N×(掩码多头自注意力 + 编码器-解码器注意力 + FFN)\n",
    "- **输出层**：线性映射 + Softmax 生成目标词表概率分布\n",
    "\n",
    "注意每个子层后都有残差连接和 LayerNorm。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab: int, tgt_vocab: int, d_model: int = 256, num_heads: int = 8,\n",
    "                 d_ff: int = 512, num_layers: int = 4, dropout: float = 0.1, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        # src_vocab: 源端（输入）词表大小，例如包含 PAD/BOS/EOS 等特殊 token\n",
    "        # tgt_vocab: 目标端（输出）词表大小，用于最后的线性投影到词表概率\n",
    "        self.src_embed = nn.Embedding(src_vocab, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len, dropout)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(d_model, tgt_vocab)\n",
    "\n",
    "    def encode(self, src: Tensor, src_pad: Optional[Tensor] = None) -> tuple[Tensor, list[Tensor]]:\n",
    "        # src: (B,T_s), src_pad: (B,T_s) 1=pad\n",
    "        x = self.pos_enc(self.src_embed(src))  # (B,T_s,d_model)\n",
    "        attn_weights = []\n",
    "        src_len = src.size(1)\n",
    "        src_mask = make_pad_mask(src_len, src_len, src_pad, src_pad)  # (B,1,T,T)\n",
    "        for layer in self.encoder_layers:\n",
    "            x, sa_w = layer(x, src_mask)\n",
    "            attn_weights.append(sa_w)\n",
    "        return x, attn_weights\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, src_pad: Optional[Tensor] = None, tgt_pad: Optional[Tensor] = None) -> tuple[Tensor, list[tuple[Tensor, Tensor]]]:\n",
    "        # tgt: (B,T_t)\n",
    "        y = self.pos_enc(self.tgt_embed(tgt))\n",
    "        T_t = tgt.size(1)\n",
    "        B, T_s = memory.size(0), memory.size(1)\n",
    "        # masks\n",
    "        pad_mask = make_pad_mask(T_t, T_t, tgt_pad, tgt_pad)            # (B,1,T_t,T_t)\n",
    "        subs_mask = make_subsequent_mask(T_t).to(y.device)              # (1,1,T_t,T_t)\n",
    "        tgt_mask = pad_mask & subs_mask if pad_mask is not None else subs_mask\n",
    "        mem_mask = make_pad_mask(T_t, T_s, tgt_pad, src_pad)            # (B,1,T_t,T_s)\n",
    "\n",
    "        attn_pairs = []\n",
    "        for layer in self.decoder_layers:\n",
    "            y, (sa_w, ca_w) = layer(y, memory, tgt_mask, mem_mask)\n",
    "            attn_pairs.append((sa_w, ca_w))\n",
    "        return y, attn_pairs\n",
    "\n",
    "    def forward(self, src: Tensor, tgt_inp: Tensor, src_pad: Optional[Tensor] = None, tgt_pad: Optional[Tensor] = None) -> Tensor:\n",
    "        memory, _ = self.encode(src, src_pad)\n",
    "        y, _ = self.decode(tgt_inp, memory, src_pad, tgt_pad)\n",
    "        logits = self.out_proj(y)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src: Tensor, bos_id: int, eos_id: int, max_new_tokens: int,\n",
    "                      src_pad: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Greedy decode using the Transformer (no beam search).\n",
    "\n",
    "        用途:\n",
    "        - 在推理阶段从编码器记忆中逐步生成目标序列。\n",
    "        - 每步选择当前模型概率最高的下一个 token（argmax），直到生成 EOS 或达到最大长度。\n",
    "\n",
    "        参数说明:\n",
    "        - src (Tensor): 源序列输入，形状 (B, T_s)，元素为 token id。\n",
    "        - bos_id (int): 解码起始符 (BOS) 的 token id，用于初始化生成序列。\n",
    "        - eos_id (int): 结束符 (EOS) 的 token id，遇到后可停止生成（对所有样本均为 EOS 时提前终止）。\n",
    "        - max_new_tokens (int): 最多生成的新 token 数量（不包括初始 BOS）。\n",
    "        - src_pad (Optional[Tensor]): 可选的源端 padding 标志，形状 (B, T_s)，1 表示 PAD，用于构造 encoder/decoder 的 mask（若为 None 则不使用 pad 屏蔽）。\n",
    "\n",
    "        返回:\n",
    "        - Tensor: 生成的 token id 序列，形状 (B, T_out)，通常包含初始 BOS 和随后生成的 token（可能包含 EOS）。\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        memory, _ = self.encode(src, src_pad)\n",
    "        B = src.size(0)\n",
    "        ys = torch.full((B, 1), bos_id, dtype=torch.long, device=src.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            y, _ = self.decode(ys, memory, src_pad, tgt_pad=None)\n",
    "            logits = self.out_proj(y)  # (B,T,d_vocab)\n",
    "            next_token = logits[:, -1].argmax(dim=-1, keepdim=True)  # (B,1)\n",
    "            ys = torch.cat([ys, next_token], dim=1)\n",
    "            if (next_token == eos_id).all():\n",
    "                break\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73af9f1",
   "metadata": {},
   "source": [
    "# 极简玩具任务：Copy Task（验证前向/反向是否正确）\n",
    "任务：输入序列 [a b c]，输出也为 [a b c]。\n",
    "- 词表：{PAD=0, BOS=1, EOS=2, 其他 3..V-1}\n",
    "- 损失：交叉熵（忽略 PAD）\n",
    "- 只训练少量步数，演示损失可下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df4ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss=4.4004\n",
      "step 20: loss=4.2882\n",
      "step 30: loss=4.2305\n",
      "step 40: loss=4.0967\n",
      "step 30: loss=4.2305\n",
      "step 40: loss=4.0967\n",
      "step 50: loss=4.0603\n",
      "src: tensor([[46, 37, 72, 78, 88],\n",
      "        [33, 99, 70, 35,  9]])\n",
      "pred: tensor([[ 1, 46, 46, 46,  2],\n",
      "        [ 1, 46, 46,  2,  2]])\n",
      "step 50: loss=4.0603\n",
      "src: tensor([[46, 37, 72, 78, 88],\n",
      "        [33, 99, 70, 35,  9]])\n",
      "pred: tensor([[ 1, 46, 46, 46,  2],\n",
      "        [ 1, 46, 46,  2,  2]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def make_copy_batch(batch_size: int, seq_len: int, vocab_size: int, pad_id: int = 0, bos_id: int = 1, eos_id: int = 2):\n",
    "    \"\"\"构造一批 copy 样本。返回 src,tgt_inp,tgt_out, 以及 pad mask。\"\"\"\n",
    "    src = []\n",
    "    tgt_inp = []\n",
    "    tgt_out = []\n",
    "    for _ in range(batch_size):\n",
    "        toks = [random.randint(3, vocab_size - 1) for _ in range(seq_len)]\n",
    "        src.append(toks)\n",
    "        # tgt: 以 BOS 开始，后接相同序列，最后 EOS\n",
    "        tgt_inp.append([bos_id] + toks)\n",
    "        tgt_out.append(toks + [eos_id])\n",
    "    src = torch.tensor(src, dtype=torch.long)\n",
    "    tgt_inp = torch.tensor(tgt_inp, dtype=torch.long)\n",
    "    tgt_out = torch.tensor(tgt_out, dtype=torch.long)\n",
    "    # 无 pad，这里简单起见\n",
    "    src_pad = torch.zeros_like(src)\n",
    "    tgt_pad = torch.zeros_like(tgt_inp)\n",
    "    return src, tgt_inp, tgt_out, src_pad, tgt_pad\n",
    "\n",
    "# 训练演示（可选）\n",
    "if 'torch' in globals():\n",
    "    torch.manual_seed(0)\n",
    "    V = 100\n",
    "    model = Transformer(src_vocab=V, tgt_vocab=V, d_model=128, num_heads=4, d_ff=256, num_layers=2, dropout=0.1)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    for step in range(50):  # 小步数演示\n",
    "        model.train()\n",
    "        src, tgt_inp, tgt_out, src_pad, tgt_pad = make_copy_batch(batch_size=16, seq_len=5, vocab_size=V)\n",
    "        logits = model(src, tgt_inp, src_pad, tgt_pad)     # (B,T+1,V)\n",
    "        loss = criterion(logits.reshape(-1, V), tgt_out.reshape(-1))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"step {step+1}: loss={loss.item():.4f}\")\n",
    "\n",
    "    # 贪心解码测试\n",
    "    model.eval()\n",
    "    src, tgt_inp, tgt_out, src_pad, tgt_pad = make_copy_batch(batch_size=2, seq_len=5, vocab_size=V)\n",
    "    pred = model.greedy_decode(src, bos_id=1, eos_id=2, max_new_tokens=6)\n",
    "    print(\"src:\", src)\n",
    "    print(\"pred:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ac8a2",
   "metadata": {},
   "source": [
    "# 复杂度、易错点与面试答题要点\n",
    "\n",
    "## 时间复杂度分析\n",
    "\n",
    "### 注意力机制的主要复杂度\n",
    "$$\n",
    "\\mathcal{O}\\big(B\\,\\cdot\\,H\\,\\cdot\\,T_q\\,\\cdot\\,T_k\\,\\cdot\\,d_k\\big)\n",
    "$$\n",
    "\n",
    "**分解：**\n",
    "- $B$：批次大小\n",
    "- $H$：头数\n",
    "- $T_q \\times T_k$：计算 $QK^{\\top}$ 的矩阵乘法\n",
    "- $d_k$：每个头的维度\n",
    "\n",
    "**自注意力情况**（$T_q = T_k = T$，$d_k = d_{\\text{model}}/H$）：\n",
    "$$\n",
    "\\mathcal{O}\\big(B\\,\\cdot\\,H\\,\\cdot\\,T^2\\,\\cdot\\,\\frac{d_{\\text{model}}}{H}\\big) = \\mathcal{O}\\big(B\\,\\cdot\\,T^2\\,\\cdot\\,d_{\\text{model}}\\big)\n",
    "$$\n",
    "\n",
    "**瓶颈：** $T^2$ 项导致序列长度的**二次复杂度**，这是标准 Transformer 的主要限制。\n",
    "\n",
    "### 其他操作的复杂度\n",
    "| 操作 | 复杂度 | 说明 |\n",
    "|------|--------|------|\n",
    "| 线性投影（$XW$） | $\\mathcal{O}(B \\cdot T \\cdot d^2)$ | $d=d_{\\text{model}}$ |\n",
    "| FFN | $\\mathcal{O}(B \\cdot T \\cdot d \\cdot d_{\\mathrm{ff}})$ | 通常 $d_{\\mathrm{ff}}=4d$ |\n",
    "| LayerNorm | $\\mathcal{O}(B \\cdot T \\cdot d)$ | 轻量级操作 |\n",
    "\n",
    "**结论：** 当 $T$ 较大时，注意力的 $T^2$ 项占主导地位。\n",
    "\n",
    "---\n",
    "\n",
    "## 空间复杂度（显存占用）\n",
    "\n",
    "### 注意力权重矩阵\n",
    "$$\n",
    "\\mathcal{O}(B\\,\\cdot\\,H\\,\\cdot\\,T_q\\,\\cdot\\,T_k)\n",
    "$$\n",
    "\n",
    "**影响：**\n",
    "- 存储所有注意力权重用于反向传播\n",
    "- 自注意力时为 $\\mathcal{O}(B \\cdot H \\cdot T^2)$\n",
    "- 长序列（$T>1000$）时显存消耗显著\n",
    "\n",
    "### 优化方向\n",
    "- **FlashAttention**：融合操作，减少中间激活存储\n",
    "- **Sparse Attention**：仅计算部分注意力权重\n",
    "- **Gradient Checkpointing**：重新计算代替存储\n",
    "\n",
    "---\n",
    "\n",
    "## 易错点清单（面试高频）\n",
    "\n",
    "### 1. MHA 头部分割/合并时的维度变换\n",
    "**错误示例：**\n",
    "```python\n",
    "# 错误：直接 view 可能导致内存不连续\n",
    "x = x.view(B, T, H, d_k).transpose(1, 2)\n",
    "out = out.transpose(1, 2).view(B, T, d_model)  # 可能报错\n",
    "```\n",
    "\n",
    "**正确做法：**\n",
    "```python\n",
    "# 分头：先 view 再 transpose\n",
    "x = x.view(B, T, H, d_k).transpose(1, 2)  # (B,H,T,d_k)\n",
    "\n",
    "# 合头：transpose 后必须 contiguous()\n",
    "out = out.transpose(1, 2).contiguous().view(B, T, d_model)\n",
    "```\n",
    "\n",
    "**原因：** `transpose` 改变步长（stride），需要 `contiguous()` 使内存连续后才能 `view`。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Mask 的取值约定（1=可见 vs 1=遮挡）\n",
    "**本教程约定：** `mask[i,j]=1` 表示位置 $j$ **可见**，`0` 表示**遮挡**\n",
    "\n",
    "**实现：**\n",
    "```python\n",
    "scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "```\n",
    "\n",
    "**注意：** 不同框架/论文可能约定相反，务必统一！\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Decoder 自注意力的下三角 Mask\n",
    "**目的：** 防止未来信息泄露（如生成第3个词时不能看到第4、5个词）\n",
    "\n",
    "**生成方式：**\n",
    "```python\n",
    "mask = torch.tril(torch.ones(T, T))  # 下三角全1\n",
    "```\n",
    "\n",
    "**形状：** $(1, 1, T, T)$ 或 $(T, T)$（广播）\n",
    "\n",
    "**易错：** 忘记在推理时也需要此 mask！\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Cross-Attention 的 Q/K/V 来源\n",
    "**正确理解：**\n",
    "- $Q$：来自 **Decoder 当前层的输出**（\"我想查询什么\"）\n",
    "- $K, V$：来自 **Encoder 的输出记忆**（\"从源序列中提取信息\"）\n",
    "\n",
    "**代码：**\n",
    "```python\n",
    "cross_out = MultiHeadAttention(\n",
    "    x_q=decoder_hidden,    # Query from decoder\n",
    "    x_kv=encoder_memory    # Key/Value from encoder\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 残差连接 + LayerNorm 的顺序\n",
    "**Post-LN（原论文）：**\n",
    "```python\n",
    "x = LayerNorm(x + Sublayer(x))\n",
    "```\n",
    "\n",
    "**Pre-LN（现代常用）：**\n",
    "```python\n",
    "x = x + Sublayer(LayerNorm(x))\n",
    "```\n",
    "\n",
    "**面试要点：** 能说明两者差异和适用场景。\n",
    "\n",
    "---\n",
    "\n",
    "### 6. 位置编码长度要足够\n",
    "**问题：** 若 `max_len=512` 但输入序列长度为 600，会越界！\n",
    "\n",
    "**解决：**\n",
    "```python\n",
    "# 动态裁剪\n",
    "T = x.size(1)\n",
    "x = x + self.pe[:, :T, :]  # 只取前 T 个位置\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 面试快速讲解结构（建议话术）\n",
    "\n",
    "### 1. 总体架构（30秒）\n",
    "> \"Transformer 由 Encoder-Decoder 组成。Encoder 用多头自注意力捕获源序列的全局依赖，Decoder 在生成时通过掩码自注意力保证自回归特性，并用交叉注意力融合源序列信息。每个子层后都有残差连接和 LayerNorm。\"\n",
    "\n",
    "### 2. 核心公式（1分钟）\n",
    "> \"注意力的核心是缩放点积：$\\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}(\\frac{QK^{\\top}}{\\sqrt{d_k}})V$。多头注意力通过 $H$ 个并行头捕获不同子空间的特征，拼接后再投影。位置编码用正弦余弦函数注入位置信息。\"\n",
    "\n",
    "### 3. 形状追踪（关键！）\n",
    "> \"输入 $(B,T,d_{\\text{model}})$ → 投影并分头为 $(B,H,T,d_k)$ → 注意力计算 $(B,H,T,T)$ 的权重矩阵 → 加权求和得 $(B,H,T,d_v)$ → 合头回 $(B,T,d_{\\text{model}})$。\"\n",
    "\n",
    "### 4. Mask 机制（必考）\n",
    "> \"Encoder 用 padding mask 屏蔽 PAD。Decoder 有两种 mask：自注意力用下三角 causal mask 防止看到未来，交叉注意力用 padding mask 屏蔽源序列的 PAD。\"\n",
    "\n",
    "### 5. 复杂度（加分项）\n",
    "> \"自注意力的复杂度是 $\\mathcal{O}(T^2 \\cdot d)$，$T^2$ 是瓶颈。长序列场景可用 Sparse Attention、Linformer、Performer 等优化。\"\n",
    "\n",
    "---\n",
    "\n",
    "## 可扩展点（展示深度理解）\n",
    "\n",
    "1. **相对位置编码（RPE）**：如 T5、XLNet 的相对位置偏置\n",
    "2. **Pre-LN vs Post-LN**：训练稳定性差异\n",
    "3. **RoPE（旋转位置编码）**：LLaMA 等模型采用，外推能力强\n",
    "4. **FlashAttention**：IO 优化，加速 2-4 倍\n",
    "5. **Efficient Transformer**：Linformer、Performer、Reformer 等 $\\mathcal{O}(T)$ 变体\n",
    "6. **参数共享**：ALBERT 跨层共享参数降低模型大小\n",
    "\n",
    "---\n",
    "\n",
    "## 维度速查表（面试快速核对）\n",
    "\n",
    "| 符号 | 含义 | 典型值 |\n",
    "|------|------|--------|\n",
    "| $B$ | Batch size | 32, 64 |\n",
    "| $T$ | 序列长度 | 128, 512 |\n",
    "| $d_{\\text{model}}$ | 模型维度 | 512, 768 |\n",
    "| $H$ | 注意力头数 | 8, 12 |\n",
    "| $d_k = d_{\\text{model}}/H$ | 每头维度 | 64 |\n",
    "| $d_{\\mathrm{ff}}$ | FFN 隐藏层 | $4d_{\\text{model}}$ |\n",
    "| $V$ | 词表大小 | 30k-50k |\n",
    "\n",
    "**记忆技巧：** \"BTD-HK\" → Batch-Time-Dmodel-Heads-dK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
