{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d30a5da",
   "metadata": {},
   "source": [
    "# 手撕 Transformer：从零实现面试速通教程\n",
    "\n",
    "本教程面向 AI/算法工程师面试，目标是“手撕 Transformer”时能在白板/编辑器中快速、正确、可讲解地实现关键模块与完整骨架。\n",
    "\n",
    "你将学习并实现：\n",
    "- Scaled Dot-Product Attention（带 mask）\n",
    "- Multi-Head Attention（MHA）\n",
    "- Position-wise Feed Forward（FFN）\n",
    "- 残差连接 + LayerNorm\n",
    "- 位置编码（Positional Encoding）\n",
    "- EncoderLayer / DecoderLayer\n",
    "- Transformer Encoder-Decoder 总装\n",
    "- 贪心解码（Greedy Decode）与一个极简玩具任务\n",
    "\n",
    "建议：面试中优先保证“正确 + 清晰 + 注释完善 + 形状无误”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc581d4",
   "metadata": {},
   "source": [
    "# 环境与依赖\n",
    "\n",
    "- Python ≥ 3.8\n",
    "- 推荐使用 PyTorch（面试常用）\n",
    "- 若无 torch，可按需安装或在纸上仅写伪代码/接口签名\n",
    "\n",
    "下面代码会尝试导入 torch 并给出缺失提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da1d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Import and quick check\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch import Tensor\n",
    "    print(torch.__version__)\n",
    "except Exception as e:\n",
    "    print(\"[Warn] torch not available. You can still read/understand the code.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b61bf",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention（带 Mask）\n",
    "核心公式：\n",
    "\n",
    ".\n",
    "令 $Q\\in\\mathbb{R}^{B\\times H\\times T_q\\times d_k},\\ K\\in\\mathbb{R}^{B\\times H\\times T_k\\times d_k},\\ V\\in\\mathbb{R}^{B\\times H\\times T_k\\times d_v}$。\n",
    "\n",
    "- 分数矩阵（缩放点积）：\n",
    "$$\n",
    "\\mathrm{scores} \\,=\\, \\frac{QK^{\\top}}{\\sqrt{d_k}}\\;\\;\\in\\;\\mathbb{R}^{B\\times H\\times T_q\\times T_k}\n",
    "$$\n",
    "\n",
    "- 加掩码（可见为1，不可见为0）：\n",
    "令 $M\\in\\{0,1\\}^{B\\times 1\\times T_q\\times T_k}$ 为可见性掩码，定义加性掩码\n",
    "$$\n",
    "\\tilde{M} \\,=\\, (1-M)\\cdot (-\\infty),\n",
    "$$\n",
    "则\n",
    "$$\n",
    "\\mathrm{attn} \\,=\\, \\mathrm{softmax}(\\mathrm{scores} + \\tilde{M})\\;\\;\\in\\;\\mathbb{R}^{B\\times H\\times T_q\\times T_k}.\n",
    "$$\n",
    "\n",
    "- 加权求和输出：\n",
    "$$\n",
    "\\mathrm{out} \\,=\\, \\mathrm{attn}\\,V\\;\\;\\in\\;\\mathbb{R}^{B\\times H\\times T_q\\times d_v}.\n",
    "$$\n",
    "\n",
    "数值稳定性：使用足够大的负数（实现中以 $-\\infty$ 近似）使被遮挡位置在 softmax 后概率趋近 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c57c4",
   "metadata": {},
   "source": [
    "**可视化结构：**\n",
    "\n",
    "![Scaled Dot-Product Attention](Scaled_dot-product_attention.png)\n",
    "\n",
    "上图展示了缩放点积注意力的计算流程：输入 Q、K、V 经过矩阵乘法、缩放、Mask、Softmax，最后加权求和得到输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2464279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5, 8]) torch.Size([2, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Q: Tensor, K: Tensor, V: Tensor, mask: Optional[Tensor] = None) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Q: (B, H, T_q, d_k)\n",
    "        K: (B, H, T_k, d_k)\n",
    "        V: (B, H, T_k, d_v)\n",
    "        mask: (B, 1, T_q, T_k) 或 (B, H, T_q, T_k)，1 表示可见，0 表示遮挡\n",
    "        返回: (out, attn)\n",
    "          out: (B, H, T_q, d_v)\n",
    "          attn: (B, H, T_q, T_k)\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)  # (B,H,T_q,T_k)\n",
    "        if mask is not None:\n",
    "            # 将不可见位置置为 -inf\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = attn @ V  # (B,H,T_q,d_v)\n",
    "        return out, attn\n",
    "\n",
    "# quick shape test (no torch run here if not installed)\n",
    "if 'torch' in globals():\n",
    "    B, H, T_q, T_k, d_k, d_v = 2, 4, 5, 6, 8, 8\n",
    "    Q = torch.randn(B, H, T_q, d_k)\n",
    "    K = torch.randn(B, H, T_k, d_k)\n",
    "    V = torch.randn(B, H, T_k, d_v)\n",
    "    mask = torch.ones(B, 1, T_q, T_k)\n",
    "    attn = ScaledDotProductAttention()\n",
    "    out, w = attn(Q, K, V, mask)\n",
    "    print(out.shape, w.shape)  # expect: (2,4,5,8) (2,4,5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572cfa0",
   "metadata": {},
   "source": [
    "# Multi-Head Attention（MHA）\n",
    "令头数为 $H$，模型维度 $d_{\\text{model}}$，每头维度 $d_k = d_{\\text{model}}/H$。对输入 $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$：\n",
    "\n",
    "- 线性映射（对每个头分别使用参数）：\n",
    "$$\n",
    "Q_i = X W_Q^{(i)},\\quad K_i = X W_K^{(i)},\\quad V_i = X W_V^{(i)},\n",
    "$$\n",
    "其中 $W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_k}$。\n",
    "\n",
    "- 头内注意力：\n",
    "$$\n",
    "\\mathrm{head}_i = \\mathrm{Attention}(Q_i, K_i, V_i) = \\mathrm{softmax}\\!\\left(\\frac{Q_i K_i^{\\top}}{\\sqrt{d_k}} + \\tilde{M}\\right)V_i.\n",
    "$$\n",
    "\n",
    "- 头拼接与输出映射：\n",
    "$$\n",
    "\\mathrm{MHA}(X) = \\mathrm{Concat}(\\mathrm{head}_1,\\dots,\\mathrm{head}_H)\\, W_O,\n",
    "$$\n",
    "其中 $W_O\\in\\mathbb{R}^{(H\\cdot d_k)\\times d_{\\text{model}}}$，最终输出形状为 $\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f78ad",
   "metadata": {},
   "source": [
    "**可视化结构：**\n",
    "\n",
    "![Multi-Head Attention](Multi-Head_Attention.png)\n",
    "\n",
    "上图展示了多头注意力的完整流程：输入经过线性投影分成多个头，每个头独立进行注意力计算，最后将所有头的输出拼接并通过线性层映射回原始维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8652b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 32]) torch.Size([2, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _split_heads(self, x: Tensor) -> Tensor:\n",
    "        # x: (B,T,d_model) -> (B,H,T,d_k)\n",
    "        B, T, _ = x.shape\n",
    "        x = x.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def _combine_heads(self, x: Tensor) -> Tensor:\n",
    "        # x: (B,H,T,d_k) -> (B,T,d_model)\n",
    "        B, H, T, d_k = x.shape\n",
    "        x = x.transpose(1, 2).contiguous().view(B, T, H * d_k)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_q: Tensor, x_kv: Tensor, mask: Optional[Tensor] = None) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        x_q: (B,T_q,d_model)\n",
    "        x_kv: (B,T_k,d_model)\n",
    "        mask: (B,1,T_q,T_k) 或 (B,H,T_q,T_k)\n",
    "        返回: (out, attn)\n",
    "        \"\"\"\n",
    "        Q = self._split_heads(self.W_q(x_q))  # (B,H,T_q,d_k)\n",
    "        K = self._split_heads(self.W_k(x_kv)) # (B,H,T_k,d_k)\n",
    "        V = self._split_heads(self.W_v(x_kv)) # (B,H,T_k,d_k)\n",
    "\n",
    "        out, attn = self.attn(Q, K, V, mask)   # out: (B,H,T_q,d_k)\n",
    "        out = self._combine_heads(out)         # (B,T_q,d_model)\n",
    "        out = self.W_o(out)                    # (B,T_q,d_model)\n",
    "        out = self.dropout(out)\n",
    "        return out, attn\n",
    "\n",
    "# quick shape test\n",
    "if 'torch' in globals():\n",
    "    B, T_q, T_k, d_model, H = 2, 5, 6, 32, 4\n",
    "    x_q = torch.randn(B, T_q, d_model)\n",
    "    x_kv = torch.randn(B, T_k, d_model)\n",
    "    mask = torch.ones(B, 1, T_q, T_k)\n",
    "    mha = MultiHeadAttention(d_model, H)\n",
    "    y, a = mha(x_q, x_kv, mask)\n",
    "    print(y.shape, a.shape)  # expect: (2,5,32) (2,4,5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64b400",
   "metadata": {},
   "source": [
    "# Positional Encoding（位置编码）\n",
    "采用正弦/余弦固定位置编码。对位置 $\\mathrm{pos}\\in\\{0,\\dots,T-1\\}$、维度索引 $i\\in\\{0,\\dots,\\lfloor\\tfrac{d_{\\text{model}}}{2}\\rfloor-1\\}$：\n",
    "\n",
    "$$\n",
    "\\mathrm{PE}[\\mathrm{pos},\\,2i] \\;=\\; \\sin\\!\\left(\\frac{\\mathrm{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right),\\quad\n",
    "\\mathrm{PE}[\\mathrm{pos},\\,2i+1] \\;=\\; \\cos\\!\\left(\\frac{\\mathrm{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right).\n",
    "$$\n",
    "\n",
    "将其加到词嵌入上得到：\n",
    "$$\n",
    "X_{\\text{pos}} = X + \\mathrm{PE}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2fb36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (T, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (T,1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1,T,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x: (B,T,d_model)\n",
    "        T = x.size(1)\n",
    "        x = x + self.pe[:, :T, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# quick test\n",
    "if 'torch' in globals():\n",
    "    pe = PositionalEncoding(32)\n",
    "    x = torch.zeros(2, 10, 32)\n",
    "    y = pe(x)\n",
    "    print(y.shape)  # (2,10,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904f4e1",
   "metadata": {},
   "source": [
    "# FFN + LayerNorm + 残差\n",
    "Position-wise FFN 通常是两层 MLP：$d_{\\text{model}} \\to d_{\\mathrm{ff}} \\to d_{\\text{model}}$。\n",
    "\n",
    "- FFN：\n",
    "$$\n",
    "\\mathrm{FFN}(x) = W_2\\,\\sigma(W_1 x + b_1) + b_2,\\quad W_1\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{\\mathrm{ff}}},\\; W_2\\in\\mathbb{R}^{d_{\\mathrm{ff}}\\times d_{\\text{model}}}.\n",
    "$$\n",
    "\n",
    "- 残差 + LayerNorm（Post-LN，本教程采用）：\n",
    "$$\n",
    "y = \\mathrm{LN}\\big(x + \\mathrm{Sublayer}(x)\\big).\n",
    "$$\n",
    "\n",
    "（对照）Pre-LN 变体：$y = x + \\mathrm{Sublayer}(\\mathrm{LN}(x))$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1369921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 32])\n",
      "torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0, activation: str = 'relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.act = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError('activation must be relu or gelu')\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.fc2(self.dropout(self.act(self.fc1(x))))\n",
    "\n",
    "class ResidualLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model, eps=eps)\n",
    "\n",
    "    def forward(self, x: Tensor, sublayer_out: Tensor) -> Tensor:\n",
    "        # x + sublayer(x) 再做 LN\n",
    "        return self.ln(x + sublayer_out)\n",
    "\n",
    "# quick test\n",
    "if 'torch' in globals():\n",
    "    ff = FeedForward(32, 64)\n",
    "    x = torch.randn(2, 10, 32)\n",
    "    y = ff(x)\n",
    "    print(y.shape)  # (2,10,32)\n",
    "    ln = ResidualLayerNorm(32)\n",
    "    z = ln(x, y)\n",
    "    print(z.shape)  # (2,10,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250c70e",
   "metadata": {},
   "source": [
    "# EncoderLayer / DecoderLayer\n",
    "- EncoderLayer：Self-Attention + FFN（每个子层后 Residual + LayerNorm）\n",
    "- DecoderLayer：Masked Self-Attention + Cross-Attention + FFN\n",
    "\n",
    "设自注意力 $\\mathrm{Att}(Q,K,V)=\\mathrm{softmax}\\!\\left(\\tfrac{QK^\\top}{\\sqrt{d_k}}+\\tilde{M}\\right)V$。\n",
    "\n",
    "EncoderLayer：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{x}_1 &= \\mathrm{Att}(X, X, X),\\\\\n",
    "X' &= \\mathrm{LN}\\big(X + \\tilde{x}_1\\big),\\\\\n",
    "\\tilde{x}_2 &= \\mathrm{FFN}(X'),\\\\\n",
    "Y &= \\mathrm{LN}\\big(X' + \\tilde{x}_2\\big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "DecoderLayer（含两次注意力）：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{y}_1 &= \\mathrm{Att}_{\\text{masked}}(Y, Y, Y),\\\\\n",
    "Y' &= \\mathrm{LN}\\big(Y + \\tilde{y}_1\\big),\\\\\n",
    "\\tilde{y}_2 &= \\mathrm{Att}(Y',\\,\\mathrm{Mem},\\,\\mathrm{Mem}),\\\\\n",
    "Y'' &= \\mathrm{LN}\\big(Y' + \\tilde{y}_2\\big),\\\\\n",
    "Z &= \\mathrm{LN}\\big(Y'' + \\mathrm{FFN}(Y'')\\big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中 $\\mathrm{Mem}$ 为 Encoder 的输出记忆，$\\mathrm{Att}_{\\text{masked}}$ 在自回归解码时使用下三角掩码（仅允许关注历史位）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d4ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pad_mask(q_len: int, k_len: int, q_pad: Tensor | None, k_pad: Tensor | None) -> Tensor:\n",
    "    \"\"\"\n",
    "    构造 padding mask（1 可见, 0 屏蔽），形状 (B,1,q_len,k_len)\n",
    "    q_pad/k_pad: (B,T) 中 1 表示 pad 位置\n",
    "    \"\"\"\n",
    "    if q_pad is None and k_pad is None:\n",
    "        return None\n",
    "    if q_pad is None:\n",
    "        q_mask = torch.zeros_like(k_pad)\n",
    "    else:\n",
    "        q_mask = q_pad\n",
    "    if k_pad is None:\n",
    "        k_mask = torch.zeros_like(q_mask)\n",
    "    else:\n",
    "        k_mask = k_pad\n",
    "    # 可见位置=1，即非pad\n",
    "    q_visible = (q_mask == 0).unsqueeze(2)  # (B,T_q,1)\n",
    "    k_visible = (k_mask == 0).unsqueeze(1)  # (B,1,T_k)\n",
    "    mask = q_visible & k_visible            # (B,T_q,T_k)\n",
    "    return mask.unsqueeze(1)                # (B,1,T_q,T_k)\n",
    "\n",
    "\n",
    "def make_subsequent_mask(T: int) -> Tensor:\n",
    "    \"\"\"Decoder 自注意力的下三角可见性掩码（1 可见, 0 屏蔽），形状 (1,1,T,T)\"\"\"\n",
    "    return torch.tril(torch.ones(T, T, dtype=torch.bool)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = ResidualLayerNorm(d_model)\n",
    "        self.norm2 = ResidualLayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, src_mask: Optional[Tensor] = None) -> tuple[Tensor, Tensor]:\n",
    "        # Self-Attention\n",
    "        sa_out, sa_w = self.self_attn(x, x, src_mask)\n",
    "        x = self.norm1(x, self.dropout(sa_out))\n",
    "        # FFN\n",
    "        ff_out = self.ffn(x)\n",
    "        x = self.norm2(x, self.dropout(ff_out))\n",
    "        return x, sa_w\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = ResidualLayerNorm(d_model)\n",
    "        self.norm2 = ResidualLayerNorm(d_model)\n",
    "        self.norm3 = ResidualLayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y: Tensor, memory: Tensor, tgt_mask: Optional[Tensor], memory_mask: Optional[Tensor]) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n",
    "        # Masked Self-Attention (decoder)\n",
    "        sa_out, sa_w = self.self_attn(y, y, tgt_mask)\n",
    "        y = self.norm1(y, self.dropout(sa_out))\n",
    "        # Cross-Attention: Q=decoder, K/V=encoder memory\n",
    "        ca_out, ca_w = self.cross_attn(y, memory, memory_mask)\n",
    "        y = self.norm2(y, self.dropout(ca_out))\n",
    "        # FFN\n",
    "        ff_out = self.ffn(y)\n",
    "        y = self.norm3(y, self.dropout(ff_out))\n",
    "        return y, (sa_w, ca_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b6ff6",
   "metadata": {},
   "source": [
    "# 总装：Transformer Encoder-Decoder\n",
    "- 词嵌入 + 位置编码\n",
    "- N 层 EncoderLayer / DecoderLayer 堆叠\n",
    "- 输出线性层映射到词表大小\n",
    "- 解码时使用贪心或 beam search（本教程实现贪心）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30769848",
   "metadata": {},
   "source": [
    "**Transformer 整体架构：**\n",
    "\n",
    "![Transformer Architecture](attention_architerture.png)\n",
    "\n",
    "上图展示了完整的 Transformer Encoder-Decoder 架构：\n",
    "- **左侧 Encoder**：输入嵌入 + 位置编码 → N×(多头自注意力 + FFN)\n",
    "- **右侧 Decoder**：输出嵌入 + 位置编码 → N×(掩码多头自注意力 + 编码器-解码器注意力 + FFN)\n",
    "- **输出层**：线性映射 + Softmax 生成目标词表概率分布\n",
    "\n",
    "注意每个子层后都有残差连接和 LayerNorm。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eef361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab: int, tgt_vocab: int, d_model: int = 256, num_heads: int = 8,\n",
    "                 d_ff: int = 512, num_layers: int = 4, dropout: float = 0.1, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(src_vocab, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len, dropout)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(d_model, tgt_vocab)\n",
    "\n",
    "    def encode(self, src: Tensor, src_pad: Optional[Tensor] = None) -> tuple[Tensor, list[Tensor]]:\n",
    "        # src: (B,T_s), src_pad: (B,T_s) 1=pad\n",
    "        x = self.pos_enc(self.src_embed(src))  # (B,T_s,d_model)\n",
    "        attn_weights = []\n",
    "        src_len = src.size(1)\n",
    "        src_mask = make_pad_mask(src_len, src_len, src_pad, src_pad)  # (B,1,T,T)\n",
    "        for layer in self.encoder_layers:\n",
    "            x, sa_w = layer(x, src_mask)\n",
    "            attn_weights.append(sa_w)\n",
    "        return x, attn_weights\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, src_pad: Optional[Tensor] = None, tgt_pad: Optional[Tensor] = None) -> tuple[Tensor, list[tuple[Tensor, Tensor]]]:\n",
    "        # tgt: (B,T_t)\n",
    "        y = self.pos_enc(self.tgt_embed(tgt))\n",
    "        T_t = tgt.size(1)\n",
    "        B, T_s = memory.size(0), memory.size(1)\n",
    "        # masks\n",
    "        pad_mask = make_pad_mask(T_t, T_t, tgt_pad, tgt_pad)            # (B,1,T_t,T_t)\n",
    "        subs_mask = make_subsequent_mask(T_t).to(y.device)              # (1,1,T_t,T_t)\n",
    "        tgt_mask = pad_mask & subs_mask if pad_mask is not None else subs_mask\n",
    "        mem_mask = make_pad_mask(T_t, T_s, tgt_pad, src_pad)            # (B,1,T_t,T_s)\n",
    "\n",
    "        attn_pairs = []\n",
    "        for layer in self.decoder_layers:\n",
    "            y, (sa_w, ca_w) = layer(y, memory, tgt_mask, mem_mask)\n",
    "            attn_pairs.append((sa_w, ca_w))\n",
    "        return y, attn_pairs\n",
    "\n",
    "    def forward(self, src: Tensor, tgt_inp: Tensor, src_pad: Optional[Tensor] = None, tgt_pad: Optional[Tensor] = None) -> Tensor:\n",
    "        memory, _ = self.encode(src, src_pad)\n",
    "        y, _ = self.decode(tgt_inp, memory, src_pad, tgt_pad)\n",
    "        logits = self.out_proj(y)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src: Tensor, bos_id: int, eos_id: int, max_new_tokens: int,\n",
    "                      src_pad: Optional[Tensor] = None) -> Tensor:\n",
    "        self.eval()\n",
    "        memory, _ = self.encode(src, src_pad)\n",
    "        B = src.size(0)\n",
    "        ys = torch.full((B, 1), bos_id, dtype=torch.long, device=src.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            y, _ = self.decode(ys, memory, src_pad, tgt_pad=None)\n",
    "            logits = self.out_proj(y)  # (B,T,d_vocab)\n",
    "            next_token = logits[:, -1].argmax(dim=-1, keepdim=True)  # (B,1)\n",
    "            ys = torch.cat([ys, next_token], dim=1)\n",
    "            if (next_token == eos_id).all():\n",
    "                break\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73af9f1",
   "metadata": {},
   "source": [
    "# 极简玩具任务：Copy Task（验证前向/反向是否正确）\n",
    "任务：输入序列 [a b c]，输出也为 [a b c]。\n",
    "- 词表：{PAD=0, BOS=1, EOS=2, 其他 3..V-1}\n",
    "- 损失：交叉熵（忽略 PAD）\n",
    "- 只训练少量步数，演示损失可下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df4ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss=4.4004\n",
      "step 20: loss=4.2882\n",
      "step 30: loss=4.2305\n",
      "step 40: loss=4.0967\n",
      "step 30: loss=4.2305\n",
      "step 40: loss=4.0967\n",
      "step 50: loss=4.0603\n",
      "src: tensor([[46, 37, 72, 78, 88],\n",
      "        [33, 99, 70, 35,  9]])\n",
      "pred: tensor([[ 1, 46, 46, 46,  2],\n",
      "        [ 1, 46, 46,  2,  2]])\n",
      "step 50: loss=4.0603\n",
      "src: tensor([[46, 37, 72, 78, 88],\n",
      "        [33, 99, 70, 35,  9]])\n",
      "pred: tensor([[ 1, 46, 46, 46,  2],\n",
      "        [ 1, 46, 46,  2,  2]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def make_copy_batch(batch_size: int, seq_len: int, vocab_size: int, pad_id: int = 0, bos_id: int = 1, eos_id: int = 2):\n",
    "    \"\"\"构造一批 copy 样本。返回 src,tgt_inp,tgt_out, 以及 pad mask。\"\"\"\n",
    "    src = []\n",
    "    tgt_inp = []\n",
    "    tgt_out = []\n",
    "    for _ in range(batch_size):\n",
    "        toks = [random.randint(3, vocab_size - 1) for _ in range(seq_len)]\n",
    "        src.append(toks)\n",
    "        # tgt: 以 BOS 开始，后接相同序列，最后 EOS\n",
    "        tgt_inp.append([bos_id] + toks)\n",
    "        tgt_out.append(toks + [eos_id])\n",
    "    src = torch.tensor(src, dtype=torch.long)\n",
    "    tgt_inp = torch.tensor(tgt_inp, dtype=torch.long)\n",
    "    tgt_out = torch.tensor(tgt_out, dtype=torch.long)\n",
    "    # 无 pad，这里简单起见\n",
    "    src_pad = torch.zeros_like(src)\n",
    "    tgt_pad = torch.zeros_like(tgt_inp)\n",
    "    return src, tgt_inp, tgt_out, src_pad, tgt_pad\n",
    "\n",
    "# 训练演示（可选）\n",
    "if 'torch' in globals():\n",
    "    torch.manual_seed(0)\n",
    "    V = 100\n",
    "    model = Transformer(src_vocab=V, tgt_vocab=V, d_model=128, num_heads=4, d_ff=256, num_layers=2, dropout=0.1)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    for step in range(50):  # 小步数演示\n",
    "        model.train()\n",
    "        src, tgt_inp, tgt_out, src_pad, tgt_pad = make_copy_batch(batch_size=16, seq_len=5, vocab_size=V)\n",
    "        logits = model(src, tgt_inp, src_pad, tgt_pad)     # (B,T+1,V)\n",
    "        loss = criterion(logits.reshape(-1, V), tgt_out.reshape(-1))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"step {step+1}: loss={loss.item():.4f}\")\n",
    "\n",
    "    # 贪心解码测试\n",
    "    model.eval()\n",
    "    src, tgt_inp, tgt_out, src_pad, tgt_pad = make_copy_batch(batch_size=2, seq_len=5, vocab_size=V)\n",
    "    pred = model.greedy_decode(src, bos_id=1, eos_id=2, max_new_tokens=6)\n",
    "    print(\"src:\", src)\n",
    "    print(\"pred:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ac8a2",
   "metadata": {},
   "source": [
    "# 复杂度、易错点与面试答题要点\n",
    "\n",
    "## 复杂度\n",
    "注意力计算主复杂度：\n",
    "$$\n",
    "\\mathcal{O}\\big(B\\,\\cdot\\,H\\,\\cdot\\,T_q\\,\\cdot\\,T_k\\,\\cdot\\,d_k\\big).\n",
    "$$\n",
    "当 $T_q\\approx T_k\\approx T$ 且 $d_k\\approx d_{\\text{model}}/H$：\n",
    "$$\n",
    "\\mathcal{O}\\big(B\\,\\cdot\\,T^2\\,\\cdot\\,d_{\\text{model}}\\big),\n",
    "$$\n",
    "这是标准 Transformer 的瓶颈（$T^2$）。\n",
    "\n",
    "（可选）注意力权重显存占用：$\\mathcal{O}(B\\,\\cdot\\,H\\,\\cdot\\,T_q\\,\\cdot\\,T_k)$。\n",
    "\n",
    "## 易错点清单\n",
    "1. MHA 头部分割/合并时的维度变换（view/transpose/contiguous）\n",
    "2. Mask 的取值方向（1=可见还是 1=遮挡）要统一（本教程采用 1=可见）\n",
    "3. Decoder 自注意力需要“下三角”mask（Subsequent Mask）\n",
    "4. Cross-Attention 的 $K/V$ 来自 Encoder 的 memory\n",
    "5. 残差 + LayerNorm 的顺序（本教程采用 Post-LN：sublayer 后残差再 LN）\n",
    "6. 位置编码长度要覆盖输入最大长度\n",
    "\n",
    "## 面试快速讲解结构（建议）\n",
    "- 大纲：Embedding+PE → MHA（QKV、分头、缩放点积）→ FFN → 残差+LN → 编解码器堆叠 → 输出层\n",
    "- 强调形状：清晰写出 $(B,T,d_{\\text{model}})$ / $(B,H,T,d_k)$\n",
    "- 强调 Mask：pad mask 与 subsequent mask 作用位置\n",
    "- 强调解码：贪心/BeamSearch 差异\n",
    "- 可扩展点：相对位置编码、Pre-LN、RoPE、FlashAttention、Efficient Transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
