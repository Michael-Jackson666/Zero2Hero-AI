{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d30a5da",
   "metadata": {},
   "source": [
    "# 手撕 Transformer：从零实现面试速通教程\n",
    "\n",
    "本教程面向 AI/算法工程师面试，目标是“手撕 Transformer”时能在白板/编辑器中快速、正确、可讲解地实现关键模块与完整骨架。\n",
    "\n",
    "你将学习并实现：\n",
    "- Scaled Dot-Product Attention（带 mask）\n",
    "- Multi-Head Attention（MHA）\n",
    "- Position-wise Feed Forward（FFN）\n",
    "- 残差连接 + LayerNorm\n",
    "- 位置编码（Positional Encoding）\n",
    "- EncoderLayer / DecoderLayer\n",
    "- Transformer Encoder-Decoder 总装\n",
    "- 贪心解码（Greedy Decode）与一个极简玩具任务\n",
    "\n",
    "建议：面试中优先保证“正确 + 清晰 + 注释完善 + 形状无误”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc581d4",
   "metadata": {},
   "source": [
    "# 环境与依赖\n",
    "\n",
    "- Python ≥ 3.8\n",
    "- 推荐使用 PyTorch（面试常用）\n",
    "- 若无 torch，可按需安装或在纸上仅写伪代码/接口签名\n",
    "\n",
    "下面代码会尝试导入 torch 并给出缺失提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da1d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Import and quick check\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch import Tensor\n",
    "    print(torch.__version__)\n",
    "except Exception as e:\n",
    "    print(\"[Warn] torch not available. You can still read/understand the code.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b61bf",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention（带 Mask）\n",
    "\n",
    "## 核心思想\n",
    "注意力机制的本质是**加权求和**：对于每个查询位置，计算它与所有键位置的相似度，然后用这些相似度作为权重对值进行加权平均。\n",
    "\n",
    "## 输入张量及其含义\n",
    "令：\n",
    "- $Q\\in\\mathbb{R}^{B\\times H\\times T_q\\times d_k}$：**查询（Query）张量**\n",
    "  - $B$：批次大小（Batch size）\n",
    "  - $H$：注意力头数（num Heads）\n",
    "  - $T_q$：查询序列长度（Query sequence length）\n",
    "  - $d_k$：每个头的键/查询维度（Key/Query dimension per head）\n",
    "  \n",
    "- $K\\in\\mathbb{R}^{B\\times H\\times T_k\\times d_k}$：**键（Key）张量**\n",
    "  - $T_k$：键序列长度（Key sequence length，通常等于值序列长度）\n",
    "  \n",
    "- $V\\in\\mathbb{R}^{B\\times H\\times T_k\\times d_v}$：**值（Value）张量**\n",
    "  - $d_v$：每个头的值维度（Value dimension per head，通常 $d_v=d_k$）\n",
    "\n",
    "## 计算步骤\n",
    "\n",
    "### 步骤 1: 计算注意力分数（缩放点积）\n",
    "$$\n",
    "\\mathrm{scores} \\,=\\, \\frac{QK^{\\top}}{\\sqrt{d_k}} \\in \\mathbb{R}^{B\\times H\\times T_q\\times T_k}\n",
    "$$\n",
    "\n",
    "**维度分析：**\n",
    "- $Q$: $(B, H, T_q, d_k)$\n",
    "- $K^{\\top}$: $(B, H, d_k, T_k)$ ← 转置最后两维\n",
    "- $QK^{\\top}$: $(B, H, T_q, T_k)$ ← 批量矩阵乘法\n",
    "- 除以 $\\sqrt{d_k}$ 进行缩放，防止点积过大导致梯度消失\n",
    "\n",
    "**物理意义：** `scores[b,h,i,j]` 表示第 $b$ 个样本、第 $h$ 个头中，查询位置 $i$ 对键位置 $j$ 的**相似度得分**。\n",
    "\n",
    "### 步骤 2: 应用掩码（Mask）\n",
    "令 $M\\in\\{0,1\\}^{B\\times 1\\times T_q\\times T_k}$ 为可见性掩码（1=可见，0=不可见）。定义加性掩码：\n",
    "$$\n",
    "\\tilde{M} \\,=\\, (1-M)\\cdot (-\\infty)\n",
    "$$\n",
    "\n",
    "将掩码加到分数上：\n",
    "$$\n",
    "\\mathrm{scores}_{\\text{masked}} = \\mathrm{scores} + \\tilde{M}\n",
    "$$\n",
    "\n",
    "**作用：** 被遮挡位置（$M=0$）的分数变为 $-\\infty$，经过 softmax 后概率趋近 0，实现\"屏蔽\"效果。\n",
    "\n",
    "### 步骤 3: Softmax 归一化\n",
    "$$\n",
    "\\mathrm{attn} \\,=\\, \\mathrm{softmax}(\\mathrm{scores}_{\\text{masked}})\\in \\mathbb{R}^{B\\times H\\times T_q\\times T_k}\n",
    "$$\n",
    "\n",
    "**操作：** 对最后一维（$T_k$ 维）做 softmax，使得对每个查询位置 $i$，所有键位置的权重和为 1：\n",
    "$$\n",
    "\\sum_{j=1}^{T_k} \\mathrm{attn}[b,h,i,j] = 1\n",
    "$$\n",
    "\n",
    "### 步骤 4: 加权求和输出\n",
    "$$\n",
    "\\mathrm{out} \\,=\\, \\mathrm{attn}* V\\in \\mathbb{R}^{B\\times H\\times T_q\\times d_v}\n",
    "$$\n",
    "\n",
    "**维度分析：**\n",
    "- $\\mathrm{attn}$: $(B, H, T_q, T_k)$\n",
    "- $V$: $(B, H, T_k, d_v)$\n",
    "- $\\mathrm{attn} \\cdot V$: $(B, H, T_q, d_v)$ ← 批量矩阵乘法\n",
    "\n",
    "**物理意义：** 输出的每个位置 $i$ 是所有值位置的**加权平均**，权重由注意力分数决定。\n",
    "\n",
    "## 数值稳定性技巧\n",
    "- 使用 `float('-inf')` 近似 $-\\infty$，使被遮挡位置在 softmax 后概率趋近 0\n",
    "- 缩放因子 $\\sqrt{d_k}$ 防止点积值过大，避免 softmax 饱和导致梯度消失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c57c4",
   "metadata": {},
   "source": [
    "**可视化结构：**\n",
    "\n",
    "![Scaled Dot-Product Attention](Scaled_dot-product_attention.png)\n",
    "\n",
    "上图展示了缩放点积注意力的计算流程：输入 Q、K、V 经过矩阵乘法、缩放、Mask、Softmax，最后加权求和得到输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2464279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5, 8]) torch.Size([2, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Q: Tensor, K: Tensor, V: Tensor, mask: Optional[Tensor] = None) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Q: (B, H, T_q, d_k)\n",
    "        K: (B, H, T_k, d_k)\n",
    "        V: (B, H, T_k, d_v)\n",
    "        mask: (B, 1, T_q, T_k) 或 (B, H, T_q, T_k), 1表示可见, 0表示遮挡\n",
    "        返回: (out, attn)\n",
    "          out: (B, H, T_q, d_v)\n",
    "          attn: (B, H, T_q, T_k)\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)  # (B,H,T_q,T_k)\n",
    "        if mask is not None:\n",
    "            # 将不可见位置置为 -inf\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = attn @ V  # (B,H,T_q,d_v)\n",
    "        return out, attn\n",
    "\n",
    "# quick shape test (no torch run here if not installed)\n",
    "if 'torch' in globals():\n",
    "    B, H, T_q, T_k, d_k, d_v = 2, 4, 5, 6, 8, 8\n",
    "    Q = torch.randn(B, H, T_q, d_k)\n",
    "    K = torch.randn(B, H, T_k, d_k)\n",
    "    V = torch.randn(B, H, T_k, d_v)\n",
    "    mask = torch.ones(B, 1, T_q, T_k)\n",
    "    attn = ScaledDotProductAttention()\n",
    "    out, w = attn(Q, K, V, mask)\n",
    "    print(out.shape, w.shape)  # expect: (2,4,5,8) (2,4,5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572cfa0",
   "metadata": {},
   "source": [
    "# Multi-Head Attention（MHA）\n",
    "\n",
    "## 核心思想\n",
    "单头注意力只能学习一种模式，多头注意力通过**并行运行多个注意力头**，让模型同时关注不同的表示子空间，捕获更丰富的特征关系。\n",
    "\n",
    "## 参数设定\n",
    "- $H$：头数（num_heads）\n",
    "- $d_{\\text{model}}$：模型总维度（embedding dimension）\n",
    "- $d_k = d_{\\text{model}}/H$：每个头的维度（dimension per head）\n",
    "\n",
    "**设计原则：** 保持总参数量不变，$H \\times d_k = d_{\\text{model}}$\n",
    "\n",
    "## 输入输出\n",
    "- **输入：** $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$\n",
    "  - $B$：批次大小\n",
    "  - $T$：序列长度\n",
    "  - $d_{\\text{model}}$：特征维度（如 512）\n",
    "  \n",
    "- **输出：** $\\mathrm{MHA}(X)\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$（形状不变）\n",
    "\n",
    "## 计算流程\n",
    "\n",
    "### 步骤 1: 线性投影（生成 Q, K, V）\n",
    "对每个头 $i\\in\\{1,\\dots,H\\}$，分别投影：\n",
    "$$\n",
    "Q_i = X W_Q^{(i)},\\quad K_i = X W_K^{(i)},\\quad V_i = X W_V^{(i)}\n",
    "$$\n",
    "\n",
    "**权重矩阵：**\n",
    "- $W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_k}$\n",
    "\n",
    "**维度变化：**\n",
    "- $X$: $(B, T, d_{\\text{model}})$\n",
    "- $W_Q^{(i)}$: $(d_{\\text{model}}, d_k)$\n",
    "- $Q_i = X W_Q^{(i)}$: $(B, T, d_k)$\n",
    "\n",
    "**实现技巧（本教程）：** 实际代码中使用 $W_Q\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{\\text{model}}}$ 一次性投影，再 reshape 分头：\n",
    "$$\n",
    "Q_{\\text{all}} = X W_Q \\;\\in\\; \\mathbb{R}^{B\\times T\\times d_{\\text{model}}} \\;\\xrightarrow{\\text{reshape}}\\; \\mathbb{R}^{B\\times T\\times H\\times d_k} \\;\\xrightarrow{\\text{transpose}}\\; \\mathbb{R}^{B\\times H\\times T\\times d_k}\n",
    "$$\n",
    "\n",
    "### 步骤 2: 头内注意力（并行计算）\n",
    "对每个头独立计算缩放点积注意力：\n",
    "$$\n",
    "\\mathrm{head}_i = \\mathrm{Attention}(Q_i, K_i, V_i) = \\mathrm{softmax}\\!\\left(\\frac{Q_i K_i^{\\top}}{\\sqrt{d_k}} + \\tilde{M}\\right)V_i\n",
    "$$\n",
    "\n",
    "**维度：**\n",
    "- 输入 $Q_i, K_i, V_i$: $(B, H, T, d_k)$（已包含所有头）\n",
    "- 输出 $\\mathrm{head}_i$: $(B, H, T, d_k)$\n",
    "\n",
    "### 步骤 3: 拼接所有头\n",
    "将 $H$ 个头的输出沿特征维拼接：\n",
    "$$\n",
    "\\mathrm{Concat}(\\mathrm{head}_1,\\dots,\\mathrm{head}_H) \\;\\in\\; \\mathbb{R}^{B\\times T\\times (H\\cdot d_k)} = \\mathbb{R}^{B\\times T\\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "**操作：** \n",
    "- 转置：$(B, H, T, d_k) \\to (B, T, H, d_k)$\n",
    "- Reshape：$(B, T, H, d_k) \\to (B, T, H \\times d_k)$\n",
    "\n",
    "### 步骤 4: 输出投影\n",
    "通过线性层映射回原维度：\n",
    "$$\n",
    "\\mathrm{MHA}(X) = \\mathrm{Concat}(\\mathrm{head}_1,\\dots,\\mathrm{head}_H)\\, W_O\n",
    "$$\n",
    "\n",
    "**权重矩阵：**\n",
    "- $W_O\\in\\mathbb{R}^{(H\\cdot d_k)\\times d_{\\text{model}}} = \\mathbb{R}^{d_{\\text{model}}\\times d_{\\text{model}}}$\n",
    "\n",
    "**最终输出：** $(B, T, d_{\\text{model}})$\n",
    "\n",
    "## 自注意力 vs 交叉注意力\n",
    "- **自注意力（Self-Attention）：** $Q, K, V$ 都来自同一输入 $X$\n",
    "- **交叉注意力（Cross-Attention）：** $Q$ 来自一个输入，$K, V$ 来自另一个输入（如 Decoder 中 $Q$ 来自 Decoder，$K,V$ 来自 Encoder）\n",
    "\n",
    "## 参数量分析\n",
    "每个 MHA 模块的参数：\n",
    "- $W_Q, W_K, W_V$: $3 \\times d_{\\text{model}} \\times d_{\\text{model}}$\n",
    "- $W_O$: $d_{\\text{model}} \\times d_{\\text{model}}$\n",
    "- **总计：** $4 d_{\\text{model}}^2$ 参数（不含偏置）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f78ad",
   "metadata": {},
   "source": [
    "**可视化结构：**\n",
    "\n",
    "![Multi-Head Attention](Multi-Head_Attention.png)\n",
    "\n",
    "上图展示了多头注意力的完整流程：输入经过线性投影分成多个头，每个头独立进行注意力计算，最后将所有头的输出拼接并通过线性层映射回原始维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8652b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 32]) torch.Size([2, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _split_heads(self, x: Tensor) -> Tensor:\n",
    "        # x: (B,T,d_model) -> (B,H,T,d_k)\n",
    "        B, T, _ = x.shape\n",
    "        x = x.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def _combine_heads(self, x: Tensor) -> Tensor:\n",
    "        # x: (B,H,T,d_k) -> (B,T,d_model)\n",
    "        B, H, T, d_k = x.shape\n",
    "        x = x.transpose(1, 2).contiguous().view(B, T, H * d_k)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_q: Tensor, x_kv: Tensor, mask: Optional[Tensor] = None) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        x_q: (B,T_q,d_model)\n",
    "        x_kv: (B,T_k,d_model)\n",
    "        mask: (B,1,T_q,T_k) 或 (B,H,T_q,T_k)\n",
    "        返回: (out, attn)\n",
    "        \"\"\"\n",
    "        Q = self._split_heads(self.W_q(x_q))  # (B,H,T_q,d_k)\n",
    "        K = self._split_heads(self.W_k(x_kv)) # (B,H,T_k,d_k)\n",
    "        V = self._split_heads(self.W_v(x_kv)) # (B,H,T_k,d_k)\n",
    "\n",
    "        out, attn = self.attn(Q, K, V, mask)   # out: (B,H,T_q,d_k)\n",
    "        out = self._combine_heads(out)         # (B,T_q,d_model)\n",
    "        out = self.W_o(out)                    # (B,T_q,d_model)\n",
    "        out = self.dropout(out)\n",
    "        return out, attn\n",
    "\n",
    "# quick shape test\n",
    "if 'torch' in globals():\n",
    "    B, T_q, T_k, d_model, H = 2, 5, 6, 32, 4\n",
    "    x_q = torch.randn(B, T_q, d_model)\n",
    "    x_kv = torch.randn(B, T_k, d_model)\n",
    "    mask = torch.ones(B, 1, T_q, T_k)\n",
    "    mha = MultiHeadAttention(d_model, H)\n",
    "    y, a = mha(x_q, x_kv, mask)\n",
    "    print(y.shape, a.shape)  # expect: (2,5,32) (2,4,5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64b400",
   "metadata": {},
   "source": [
    "# Positional Encoding（位置编码）\n",
    "\n",
    "## 为什么需要位置编码？\n",
    "注意力机制本身是**置换不变**的（permutation-invariant）：交换序列顺序，输出也会相应交换，但注意力权重不变。为了让模型感知位置信息（如\"猫吃鱼\"和\"鱼吃猫\"的区别），需要显式注入位置信息。\n",
    "\n",
    "## 两种常见实现方式\n",
    "1. **固定位置编码（Sinusoidal）**：使用正弦/余弦函数，无需学习参数（本教程采用）\n",
    "2. **可学习位置编码**：`nn.Embedding(max_len, d_model)`，需要训练\n",
    "\n",
    "## 正弦/余弦位置编码公式\n",
    "对于位置 $\\mathrm{pos}\\in\\{0,1,\\dots,T-1\\}$ 和维度索引 $i\\in\\{0,1,\\dots,\\lfloor\\tfrac{d_{\\text{model}}}{2}\\rfloor-1\\}$：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{PE}[\\mathrm{pos},\\,2i] &\\;=\\; \\sin\\!\\left(\\frac{\\mathrm{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), \\\\[0.5em]\n",
    "\\mathrm{PE}[\\mathrm{pos},\\,2i+1] &\\;=\\; \\cos\\!\\left(\\frac{\\mathrm{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 公式解析\n",
    "- **偶数维度**（$2i$）：使用正弦函数\n",
    "- **奇数维度**（$2i+1$）：使用余弦函数\n",
    "- **频率**：$\\omega_i = \\frac{1}{10000^{2i/d_{\\text{model}}}}$\n",
    "  - 低维度（$i$ 小）：高频振荡，捕捉局部位置差异\n",
    "  - 高维度（$i$ 大）：低频振荡，捕捉远距离位置关系\n",
    "\n",
    "### 维度示例\n",
    "假设 $d_{\\text{model}}=512$，$T=100$（序列长度）：\n",
    "- $\\mathrm{PE}$ 形状：$(T, d_{\\text{model}}) = (100, 512)$\n",
    "- `PE[0, :]`：位置 0 的编码向量（512 维）\n",
    "- `PE[:, 0]`：所有位置在第 0 维的编码值\n",
    "\n",
    "## 使用方式\n",
    "将位置编码**直接加**到词嵌入上：\n",
    "$$\n",
    "X_{\\text{pos}} = X_{\\text{embed}} + \\mathrm{PE}\n",
    "$$\n",
    "\n",
    "**维度匹配：**\n",
    "- $X_{\\text{embed}}$: $(B, T, d_{\\text{model}})$ ← 词嵌入\n",
    "- $\\mathrm{PE}$: $(1, T, d_{\\text{model}})$ ← 位置编码（广播到批次维）\n",
    "- $X_{\\text{pos}}$: $(B, T, d_{\\text{model}})$ ← 最终输入\n",
    "\n",
    "## 为什么使用正弦/余弦？\n",
    "1. **相对位置关系：** $\\mathrm{PE}_{\\mathrm{pos}+k}$ 可以表示为 $\\mathrm{PE}_{\\mathrm{pos}}$ 的线性函数\n",
    "2. **外推能力：** 理论上可以处理比训练时更长的序列\n",
    "3. **无需学习：** 减少参数量，避免过拟合\n",
    "\n",
    "## 实现细节\n",
    "```python\n",
    "# 生成位置索引\n",
    "position = torch.arange(0, max_len).unsqueeze(1)  # (T, 1)\n",
    "\n",
    "# 生成频率项\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "# 等价于: 10000^(-2i/d_model)\n",
    "\n",
    "# 填充 PE 矩阵\n",
    "pe[:, 0::2] = torch.sin(position * div_term)  # 偶数列\n",
    "pe[:, 1::2] = torch.cos(position * div_term)  # 奇数列\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2fb36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (T, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (T,1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1,T,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x: (B,T,d_model)\n",
    "        T = x.size(1)\n",
    "        x = x + self.pe[:, :T, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# quick test\n",
    "if 'torch' in globals():\n",
    "    pe = PositionalEncoding(32)\n",
    "    x = torch.zeros(2, 10, 32)\n",
    "    y = pe(x)\n",
    "    print(y.shape)  # (2,10,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904f4e1",
   "metadata": {},
   "source": [
    "# FFN + LayerNorm + 残差连接\n",
    "\n",
    "## Position-wise Feed-Forward Network (FFN)\n",
    "\n",
    "### 结构\n",
    "两层全连接网络，对每个位置**独立**处理（不跨位置交互）：\n",
    "$$\n",
    "\\mathrm{FFN}(x) = W_2\\,\\sigma(W_1 x + b_1) + b_2\n",
    "$$\n",
    "\n",
    "**参数：**\n",
    "- $W_1\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{\\mathrm{ff}}}$：第一层权重（扩张）\n",
    "- $W_2\\in\\mathbb{R}^{d_{\\mathrm{ff}}\\times d_{\\text{model}}}$：第二层权重（压缩）\n",
    "- $\\sigma$：激活函数（ReLU 或 GELU）\n",
    "\n",
    "### 维度变化\n",
    "$$\n",
    "(B, T, d_{\\text{model}}) \\xrightarrow{W_1} (B, T, d_{\\mathrm{ff}}) \\xrightarrow{\\sigma} (B, T, d_{\\mathrm{ff}}) \\xrightarrow{W_2} (B, T, d_{\\text{model}})\n",
    "$$\n",
    "\n",
    "**典型值：** $d_{\\mathrm{ff}} = 4 \\times d_{\\text{model}}$（如 512 → 2048 → 512）\n",
    "\n",
    "### 作用\n",
    "- **非线性变换**：引入非线性，增强表达能力\n",
    "- **特征混合**：每个位置独立地在高维空间中进行特征变换\n",
    "- **位置独立**：与注意力的\"跨位置交互\"形成互补\n",
    "\n",
    "---\n",
    "\n",
    "## 残差连接（Residual Connection）\n",
    "$$\n",
    "\\text{output} = x + \\text{Sublayer}(x)\n",
    "$$\n",
    "\n",
    "**作用：**\n",
    "1. **缓解梯度消失**：梯度可直接通过恒等映射反向传播\n",
    "2. **简化学习**：子层只需学习\"残差\"（变化量），而非完整映射\n",
    "3. **稳定训练**：允许堆叠更深的网络\n",
    "\n",
    "---\n",
    "\n",
    "## Layer Normalization（层归一化）\n",
    "对每个样本的特征维度做归一化：\n",
    "$$\n",
    "\\mathrm{LN}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "**计算：**\n",
    "- $\\mu = \\frac{1}{d_{\\text{model}}}\\sum_{i=1}^{d_{\\text{model}}} x_i$：均值\n",
    "- $\\sigma^2 = \\frac{1}{d_{\\text{model}}}\\sum_{i=1}^{d_{\\text{model}}} (x_i - \\mu)^2$：方差\n",
    "- $\\gamma, \\beta\\in\\mathbb{R}^{d_{\\text{model}}}$：可学习的缩放和偏移参数\n",
    "- $\\epsilon$：数值稳定项（如 $10^{-5}$）\n",
    "\n",
    "**作用：** 稳定训练，加速收敛，减少对初始化的敏感度\n",
    "\n",
    "---\n",
    "\n",
    "## 两种组合顺序\n",
    "\n",
    "### Post-LN（本教程采用，原论文）\n",
    "$$\n",
    "y = \\mathrm{LN}\\big(x + \\mathrm{Sublayer}(x)\\big)\n",
    "$$\n",
    "\n",
    "**流程：** 子层输出 → 残差连接 → LayerNorm\n",
    "\n",
    "**特点：**\n",
    "- 梯度直接流经子层，训练初期可能不稳定\n",
    "- 需要 warm-up 学习率策略\n",
    "\n",
    "### Pre-LN（现代常用）\n",
    "$$\n",
    "y = x + \\mathrm{Sublayer}(\\mathrm{LN}(x))\n",
    "$$\n",
    "\n",
    "**流程：** LayerNorm → 子层 → 残差连接\n",
    "\n",
    "**特点：**\n",
    "- 更稳定，易于训练深层网络\n",
    "- 无需 warm-up，对学习率不敏感\n",
    "- GPT-2/3、BERT 等现代模型多采用此方式\n",
    "\n",
    "---\n",
    "\n",
    "## 完整子层结构（Post-LN）\n",
    "```\n",
    "输入 x (B, T, d_model)\n",
    "    ↓\n",
    "子层(MHA/FFN) → sublayer_out\n",
    "    ↓\n",
    "x + sublayer_out  (残差)\n",
    "    ↓\n",
    "LayerNorm\n",
    "    ↓\n",
    "输出 y (B, T, d_model)\n",
    "```\n",
    "\n",
    "**参数量：**\n",
    "- FFN: $2 \\times d_{\\text{model}} \\times d_{\\mathrm{ff}}$ （约 $8 d_{\\text{model}}^2$）\n",
    "- LayerNorm: $2 \\times d_{\\text{model}}$（$\\gamma$ 和 $\\beta$）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1369921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 32])\n",
      "torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0, activation: str = 'relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.act = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError('activation must be relu or gelu')\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.fc2(self.dropout(self.act(self.fc1(x))))\n",
    "\n",
    "class ResidualLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model, eps=eps)\n",
    "\n",
    "    def forward(self, x: Tensor, sublayer_out: Tensor) -> Tensor:\n",
    "        # x + sublayer(x) 再做 LN\n",
    "        return self.ln(x + sublayer_out)\n",
    "\n",
    "# quick test\n",
    "if 'torch' in globals():\n",
    "    ff = FeedForward(32, 64)\n",
    "    x = torch.randn(2, 10, 32)\n",
    "    y = ff(x)\n",
    "    print(y.shape)  # (2,10,32)\n",
    "    ln = ResidualLayerNorm(32)\n",
    "    z = ln(x, y)\n",
    "    print(z.shape)  # (2,10,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250c70e",
   "metadata": {},
   "source": [
    "# EncoderLayer / DecoderLayer\n",
    "\n",
    "## 符号约定\n",
    "设自注意力函数：\n",
    "$$\n",
    "\\mathrm{Att}(Q,K,V,M)=\\mathrm{softmax}\\!\\left(\\tfrac{QK^\\top}{\\sqrt{d_k}}+\\tilde{M}\\right)V\n",
    "$$\n",
    "\n",
    "其中 $M$ 是可见性掩码（mask）。\n",
    "\n",
    "---\n",
    "\n",
    "## EncoderLayer（编码器层）\n",
    "\n",
    "### 结构\n",
    "由两个子层组成：\n",
    "1. **多头自注意力（Multi-Head Self-Attention）**\n",
    "2. **前馈网络（FFN）**\n",
    "\n",
    "每个子层后都有**残差连接 + LayerNorm**。\n",
    "\n",
    "### 数学表达（Post-LN）\n",
    "输入 $X\\in\\mathbb{R}^{B\\times T_s\\times d_{\\text{model}}}$（源序列）：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{x}_1 &= \\mathrm{MHA}(X, X, X, M_{\\text{src}}), \\quad &\\text{← 自注意力}\\\\\n",
    "X' &= \\mathrm{LN}\\big(X + \\tilde{x}_1\\big), \\quad &\\text{← 残差+归一化}\\\\\n",
    "\\tilde{x}_2 &= \\mathrm{FFN}(X'), \\quad &\\text{← 前馈网络}\\\\\n",
    "Y &= \\mathrm{LN}\\big(X' + \\tilde{x}_2\\big). \\quad &\\text{← 残差+归一化}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 维度追踪\n",
    "- 输入 $X$: $(B, T_s, d_{\\text{model}})$\n",
    "- 自注意力输出 $\\tilde{x}_1$: $(B, T_s, d_{\\text{model}})$\n",
    "- 第一次 LN 后 $X'$: $(B, T_s, d_{\\text{model}})$\n",
    "- FFN 输出 $\\tilde{x}_2$: $(B, T_s, d_{\\text{model}})$\n",
    "- 最终输出 $Y$: $(B, T_s, d_{\\text{model}})$\n",
    "\n",
    "**关键点：** 自注意力中 $Q=K=V$，都来自同一输入 $X$，无需外部记忆。\n",
    "\n",
    "---\n",
    "\n",
    "## DecoderLayer（解码器层）\n",
    "\n",
    "### 结构\n",
    "由三个子层组成：\n",
    "1. **掩码多头自注意力（Masked Multi-Head Self-Attention）**\n",
    "2. **编码器-解码器注意力（Cross-Attention）**\n",
    "3. **前馈网络（FFN）**\n",
    "\n",
    "每个子层后都有**残差连接 + LayerNorm**。\n",
    "\n",
    "### 数学表达（Post-LN）\n",
    "输入：\n",
    "- $Y\\in\\mathbb{R}^{B\\times T_t\\times d_{\\text{model}}}$：目标序列（已解码部分）\n",
    "- $\\mathrm{Mem}\\in\\mathbb{R}^{B\\times T_s\\times d_{\\text{model}}}$：编码器输出（源序列的编码表示）\n",
    "\n",
    "计算流程：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{y}_1 &= \\mathrm{MHA}(Y, Y, Y, M_{\\text{causal}}), \\quad &\\text{← 掩码自注意力}\\\\\n",
    "Y' &= \\mathrm{LN}\\big(Y + \\tilde{y}_1\\big), \\quad &\\text{← 残差+归一化}\\\\\n",
    "\\tilde{y}_2 &= \\mathrm{MHA}(Y', \\mathrm{Mem}, \\mathrm{Mem}, M_{\\text{cross}}), \\quad &\\text{← 交叉注意力}\\\\\n",
    "Y'' &= \\mathrm{LN}\\big(Y' + \\tilde{y}_2\\big), \\quad &\\text{← 残差+归一化}\\\\\n",
    "\\tilde{y}_3 &= \\mathrm{FFN}(Y''), \\quad &\\text{← 前馈网络}\\\\\n",
    "Z &= \\mathrm{LN}\\big(Y'' + \\tilde{y}_3\\big). \\quad &\\text{← 残差+归一化}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 维度追踪\n",
    "- 输入 $Y$: $(B, T_t, d_{\\text{model}})$\n",
    "- 编码器记忆 $\\mathrm{Mem}$: $(B, T_s, d_{\\text{model}})$\n",
    "- 掩码自注意力输出 $\\tilde{y}_1$: $(B, T_t, d_{\\text{model}})$\n",
    "- 第一次 LN 后 $Y'$: $(B, T_t, d_{\\text{model}})$\n",
    "- 交叉注意力输出 $\\tilde{y}_2$: $(B, T_t, d_{\\text{model}})$ ← $Q$ 来自 $Y'$，$K,V$ 来自 $\\mathrm{Mem}$\n",
    "- 第二次 LN 后 $Y''$: $(B, T_t, d_{\\text{model}})$\n",
    "- FFN 输出 $\\tilde{y}_3$: $(B, T_t, d_{\\text{model}})$\n",
    "- 最终输出 $Z$: $(B, T_t, d_{\\text{model}})$\n",
    "\n",
    "---\n",
    "\n",
    "## 关键掩码机制\n",
    "\n",
    "### 1. Encoder Padding Mask ($M_{\\text{src}}$)\n",
    "**作用：** 屏蔽填充位置（PAD token），防止注意力关注无效位置\n",
    "\n",
    "**形状：** $(B, 1, T_s, T_s)$\n",
    "\n",
    "**示例：** 若序列 `[3, 5, 7, PAD, PAD]`，则 mask 为：\n",
    "```\n",
    "[[1, 1, 1, 0, 0],\n",
    " [1, 1, 1, 0, 0],\n",
    " [1, 1, 1, 0, 0],\n",
    " [1, 1, 1, 0, 0],\n",
    " [1, 1, 1, 0, 0]]\n",
    "```\n",
    "\n",
    "### 2. Decoder Causal Mask ($M_{\\text{causal}}$，下三角掩码）\n",
    "**作用：** 在自回归解码时，**防止未来信息泄露**，位置 $i$ 只能看到位置 $\\leq i$ 的信息\n",
    "\n",
    "**形状：** $(1, 1, T_t, T_t)$（所有样本共享）\n",
    "\n",
    "**示例：** $T_t=5$ 时的掩码：\n",
    "```\n",
    "[[1, 0, 0, 0, 0],    ← 位置0只能看自己\n",
    " [1, 1, 0, 0, 0],    ← 位置1可以看0,1\n",
    " [1, 1, 1, 0, 0],    ← 位置2可以看0,1,2\n",
    " [1, 1, 1, 1, 0],\n",
    " [1, 1, 1, 1, 1]]    ← 位置4可以看全部\n",
    "```\n",
    "\n",
    "### 3. Cross-Attention Mask ($M_{\\text{cross}}$)\n",
    "**作用：** 屏蔽编码器的填充位置，防止解码器关注源序列的无效位置\n",
    "\n",
    "**形状：** $(B, 1, T_t, T_s)$\n",
    "\n",
    "**注意：** 查询长度为 $T_t$（目标），键长度为 $T_s$（源）\n",
    "\n",
    "---\n",
    "\n",
    "## Encoder vs Decoder 对比\n",
    "\n",
    "| 特性 | EncoderLayer | DecoderLayer |\n",
    "|------|--------------|--------------|\n",
    "| 自注意力类型 | 双向（可见全部位置） | 单向（仅可见历史） |\n",
    "| 注意力层数 | 1（自注意力） | 2（自注意力 + 交叉注意力） |\n",
    "| 掩码类型 | Padding mask | Causal + Padding mask |\n",
    "| 输入依赖 | 仅源序列 | 源序列 + 目标序列 |\n",
    "| 并行性 | 完全并行 | 训练并行，推理串行 |\n",
    "\n",
    "---\n",
    "\n",
    "## 参数量分析（单层）\n",
    "- **EncoderLayer:**\n",
    "  - MHA: $4d_{\\text{model}}^2$\n",
    "  - FFN: $8d_{\\text{model}}^2$\n",
    "  - LayerNorm: $4d_{\\text{model}}$\n",
    "  - **总计:** ≈ $12d_{\\text{model}}^2$\n",
    "\n",
    "- **DecoderLayer:**\n",
    "  - Masked MHA: $4d_{\\text{model}}^2$\n",
    "  - Cross MHA: $4d_{\\text{model}}^2$\n",
    "  - FFN: $8d_{\\text{model}}^2$\n",
    "  - LayerNorm: $6d_{\\text{model}}$\n",
    "  - **总计:** ≈ $16d_{\\text{model}}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pad_mask(q_len: int, k_len: int, q_pad: Tensor | None, k_pad: Tensor | None) -> Tensor:\n",
    "    \"\"\"\n",
    "    构造 padding mask（1 可见, 0 屏蔽），形状 (B,1,q_len,k_len)\n",
    "    q_pad/k_pad: (B,T) 中 1 表示 pad 位置\n",
    "    \"\"\"\n",
    "    if q_pad is None and k_pad is None:\n",
    "        return None\n",
    "    if q_pad is None:\n",
    "        q_mask = torch.zeros_like(k_pad)\n",
    "    else:\n",
    "        q_mask = q_pad\n",
    "    if k_pad is None:\n",
    "        k_mask = torch.zeros_like(q_mask)\n",
    "    else:\n",
    "        k_mask = k_pad\n",
    "    # 可见位置=1，即非pad\n",
    "    q_visible = (q_mask == 0).unsqueeze(2)  # (B,T_q,1)\n",
    "    k_visible = (k_mask == 0).unsqueeze(1)  # (B,1,T_k)\n",
    "    mask = q_visible & k_visible            # (B,T_q,T_k)\n",
    "    return mask.unsqueeze(1)                # (B,1,T_q,T_k)\n",
    "\n",
    "\n",
    "def make_subsequent_mask(T: int) -> Tensor:\n",
    "    \"\"\"Decoder 自注意力的下三角可见性掩码（1 可见, 0 屏蔽），形状 (1,1,T,T)\"\"\"\n",
    "    return torch.tril(torch.ones(T, T, dtype=torch.bool)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = ResidualLayerNorm(d_model)\n",
    "        self.norm2 = ResidualLayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, src_mask: Optional[Tensor] = None) -> tuple[Tensor, Tensor]:\n",
    "        # Self-Attention\n",
    "        sa_out, sa_w = self.self_attn(x, x, src_mask)\n",
    "        x = self.norm1(x, self.dropout(sa_out))\n",
    "        # FFN\n",
    "        ff_out = self.ffn(x)\n",
    "        x = self.norm2(x, self.dropout(ff_out))\n",
    "        return x, sa_w\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = ResidualLayerNorm(d_model)\n",
    "        self.norm2 = ResidualLayerNorm(d_model)\n",
    "        self.norm3 = ResidualLayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y: Tensor, memory: Tensor, tgt_mask: Optional[Tensor], memory_mask: Optional[Tensor]) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n",
    "        # Masked Self-Attention (decoder)\n",
    "        sa_out, sa_w = self.self_attn(y, y, tgt_mask)\n",
    "        y = self.norm1(y, self.dropout(sa_out))\n",
    "        # Cross-Attention: Q=decoder, K/V=encoder memory\n",
    "        ca_out, ca_w = self.cross_attn(y, memory, memory_mask)\n",
    "        y = self.norm2(y, self.dropout(ca_out))\n",
    "        # FFN\n",
    "        ff_out = self.ffn(y)\n",
    "        y = self.norm3(y, self.dropout(ff_out))\n",
    "        return y, (sa_w, ca_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b6ff6",
   "metadata": {},
   "source": [
    "# 总装：Transformer Encoder-Decoder\n",
    "- 词嵌入 + 位置编码\n",
    "- N 层 EncoderLayer / DecoderLayer 堆叠\n",
    "- 输出线性层映射到词表大小\n",
    "- 解码时使用贪心或 beam search（本教程实现贪心）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30769848",
   "metadata": {},
   "source": [
    "**Transformer 整体架构：**\n",
    "\n",
    "![Transformer Architecture](attention_architerture.png)\n",
    "\n",
    "上图展示了完整的 Transformer Encoder-Decoder 架构：\n",
    "- **左侧 Encoder**：输入嵌入 + 位置编码 → N×(多头自注意力 + FFN)\n",
    "- **右侧 Decoder**：输出嵌入 + 位置编码 → N×(掩码多头自注意力 + 编码器-解码器注意力 + FFN)\n",
    "- **输出层**：线性映射 + Softmax 生成目标词表概率分布\n",
    "\n",
    "注意每个子层后都有残差连接和 LayerNorm。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab: int, tgt_vocab: int, d_model: int = 256, num_heads: int = 8,\n",
    "                 d_ff: int = 512, num_layers: int = 4, dropout: float = 0.1, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        # src_vocab: 源端（输入）词表大小，例如包含 PAD/BOS/EOS 等特殊 token\n",
    "        # tgt_vocab: 目标端（输出）词表大小，用于最后的线性投影到词表概率\n",
    "        self.src_embed = nn.Embedding(src_vocab, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len, dropout)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(d_model, tgt_vocab)\n",
    "\n",
    "    def encode(self, src: Tensor, src_pad: Optional[Tensor] = None) -> tuple[Tensor, list[Tensor]]:\n",
    "        # src: (B,T_s), src_pad: (B,T_s) 1=pad\n",
    "        x = self.pos_enc(self.src_embed(src))  # (B,T_s,d_model)\n",
    "        attn_weights = []\n",
    "        src_len = src.size(1)\n",
    "        src_mask = make_pad_mask(src_len, src_len, src_pad, src_pad)  # (B,1,T,T)\n",
    "        for layer in self.encoder_layers:\n",
    "            x, sa_w = layer(x, src_mask)\n",
    "            attn_weights.append(sa_w)\n",
    "        return x, attn_weights\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, src_pad: Optional[Tensor] = None, tgt_pad: Optional[Tensor] = None) -> tuple[Tensor, list[tuple[Tensor, Tensor]]]:\n",
    "        # tgt: (B,T_t)\n",
    "        y = self.pos_enc(self.tgt_embed(tgt))\n",
    "        T_t = tgt.size(1)\n",
    "        B, T_s = memory.size(0), memory.size(1)\n",
    "        # masks\n",
    "        pad_mask = make_pad_mask(T_t, T_t, tgt_pad, tgt_pad)            # (B,1,T_t,T_t)\n",
    "        subs_mask = make_subsequent_mask(T_t).to(y.device)              # (1,1,T_t,T_t)\n",
    "        tgt_mask = pad_mask & subs_mask if pad_mask is not None else subs_mask\n",
    "        mem_mask = make_pad_mask(T_t, T_s, tgt_pad, src_pad)            # (B,1,T_t,T_s)\n",
    "\n",
    "        attn_pairs = []\n",
    "        for layer in self.decoder_layers:\n",
    "            y, (sa_w, ca_w) = layer(y, memory, tgt_mask, mem_mask)\n",
    "            attn_pairs.append((sa_w, ca_w))\n",
    "        return y, attn_pairs\n",
    "\n",
    "    def forward(self, src: Tensor, tgt_inp: Tensor, src_pad: Optional[Tensor] = None, tgt_pad: Optional[Tensor] = None) -> Tensor:\n",
    "        memory, _ = self.encode(src, src_pad)\n",
    "        y, _ = self.decode(tgt_inp, memory, src_pad, tgt_pad)\n",
    "        logits = self.out_proj(y)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src: Tensor, bos_id: int, eos_id: int, max_new_tokens: int,\n",
    "                      src_pad: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Greedy decode using the Transformer (no beam search).\n",
    "\n",
    "        用途:\n",
    "        - 在推理阶段从编码器记忆中逐步生成目标序列。\n",
    "        - 每步选择当前模型概率最高的下一个 token（argmax），直到生成 EOS 或达到最大长度。\n",
    "\n",
    "        参数说明:\n",
    "        - src (Tensor): 源序列输入，形状 (B, T_s)，元素为 token id。\n",
    "        - bos_id (int): 解码起始符 (BOS) 的 token id，用于初始化生成序列。\n",
    "        - eos_id (int): 结束符 (EOS) 的 token id，遇到后可停止生成（对所有样本均为 EOS 时提前终止）。\n",
    "        - max_new_tokens (int): 最多生成的新 token 数量（不包括初始 BOS）。\n",
    "        - src_pad (Optional[Tensor]): 可选的源端 padding 标志，形状 (B, T_s)，1 表示 PAD，用于构造 encoder/decoder 的 mask（若为 None 则不使用 pad 屏蔽）。\n",
    "\n",
    "        返回:\n",
    "        - Tensor: 生成的 token id 序列，形状 (B, T_out)，通常包含初始 BOS 和随后生成的 token（可能包含 EOS）。\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        memory, _ = self.encode(src, src_pad)\n",
    "        B = src.size(0)\n",
    "        ys = torch.full((B, 1), bos_id, dtype=torch.long, device=src.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            y, _ = self.decode(ys, memory, src_pad, tgt_pad=None)\n",
    "            logits = self.out_proj(y)  # (B,T,d_vocab)\n",
    "            next_token = logits[:, -1].argmax(dim=-1, keepdim=True)  # (B,1)\n",
    "            ys = torch.cat([ys, next_token], dim=1)\n",
    "            if (next_token == eos_id).all():\n",
    "                break\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73af9f1",
   "metadata": {},
   "source": [
    "# 极简玩具任务：Copy Task（验证前向/反向是否正确）\n",
    "任务：输入序列 [a b c]，输出也为 [a b c]。\n",
    "- 词表：{PAD=0, BOS=1, EOS=2, 其他 3..V-1}\n",
    "- 损失：交叉熵（忽略 PAD）\n",
    "- 只训练少量步数，演示损失可下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df4ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss=4.4004\n",
      "step 20: loss=4.2882\n",
      "step 30: loss=4.2305\n",
      "step 40: loss=4.0967\n",
      "step 30: loss=4.2305\n",
      "step 40: loss=4.0967\n",
      "step 50: loss=4.0603\n",
      "src: tensor([[46, 37, 72, 78, 88],\n",
      "        [33, 99, 70, 35,  9]])\n",
      "pred: tensor([[ 1, 46, 46, 46,  2],\n",
      "        [ 1, 46, 46,  2,  2]])\n",
      "step 50: loss=4.0603\n",
      "src: tensor([[46, 37, 72, 78, 88],\n",
      "        [33, 99, 70, 35,  9]])\n",
      "pred: tensor([[ 1, 46, 46, 46,  2],\n",
      "        [ 1, 46, 46,  2,  2]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def make_copy_batch(batch_size: int, seq_len: int, vocab_size: int, pad_id: int = 0, bos_id: int = 1, eos_id: int = 2):\n",
    "    \"\"\"构造一批 copy 样本。返回 src,tgt_inp,tgt_out, 以及 pad mask。\"\"\"\n",
    "    src = []\n",
    "    tgt_inp = []\n",
    "    tgt_out = []\n",
    "    for _ in range(batch_size):\n",
    "        toks = [random.randint(3, vocab_size - 1) for _ in range(seq_len)]\n",
    "        src.append(toks)\n",
    "        # tgt: 以 BOS 开始，后接相同序列，最后 EOS\n",
    "        tgt_inp.append([bos_id] + toks)\n",
    "        tgt_out.append(toks + [eos_id])\n",
    "    src = torch.tensor(src, dtype=torch.long)\n",
    "    tgt_inp = torch.tensor(tgt_inp, dtype=torch.long)\n",
    "    tgt_out = torch.tensor(tgt_out, dtype=torch.long)\n",
    "    # 无 pad，这里简单起见\n",
    "    src_pad = torch.zeros_like(src)\n",
    "    tgt_pad = torch.zeros_like(tgt_inp)\n",
    "    return src, tgt_inp, tgt_out, src_pad, tgt_pad\n",
    "\n",
    "# 训练演示（可选）\n",
    "if 'torch' in globals():\n",
    "    torch.manual_seed(0)\n",
    "    V = 100\n",
    "    model = Transformer(src_vocab=V, tgt_vocab=V, d_model=128, num_heads=4, d_ff=256, num_layers=2, dropout=0.1)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    for step in range(50):  # 小步数演示\n",
    "        model.train()\n",
    "        src, tgt_inp, tgt_out, src_pad, tgt_pad = make_copy_batch(batch_size=16, seq_len=5, vocab_size=V)\n",
    "        logits = model(src, tgt_inp, src_pad, tgt_pad)     # (B,T+1,V)\n",
    "        loss = criterion(logits.reshape(-1, V), tgt_out.reshape(-1))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"step {step+1}: loss={loss.item():.4f}\")\n",
    "\n",
    "    # 贪心解码测试\n",
    "    model.eval()\n",
    "    src, tgt_inp, tgt_out, src_pad, tgt_pad = make_copy_batch(batch_size=2, seq_len=5, vocab_size=V)\n",
    "    pred = model.greedy_decode(src, bos_id=1, eos_id=2, max_new_tokens=6)\n",
    "    print(\"src:\", src)\n",
    "    print(\"pred:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ac8a2",
   "metadata": {},
   "source": [
    "# 复杂度、易错点与面试答题要点\n",
    "\n",
    "## 时间复杂度分析\n",
    "\n",
    "### 注意力机制的主要复杂度\n",
    "$$\n",
    "\\mathcal{O}\\big(B\\,\\cdot\\,H\\,\\cdot\\,T_q\\,\\cdot\\,T_k\\,\\cdot\\,d_k\\big)\n",
    "$$\n",
    "\n",
    "**分解：**\n",
    "- $B$：批次大小\n",
    "- $H$：头数\n",
    "- $T_q \\times T_k$：计算 $QK^{\\top}$ 的矩阵乘法\n",
    "- $d_k$：每个头的维度\n",
    "\n",
    "**自注意力情况**（$T_q = T_k = T$，$d_k = d_{\\text{model}}/H$）：\n",
    "$$\n",
    "\\mathcal{O}\\big(B\\,\\cdot\\,H\\,\\cdot\\,T^2\\,\\cdot\\,\\frac{d_{\\text{model}}}{H}\\big) = \\mathcal{O}\\big(B\\,\\cdot\\,T^2\\,\\cdot\\,d_{\\text{model}}\\big)\n",
    "$$\n",
    "\n",
    "**瓶颈：** $T^2$ 项导致序列长度的**二次复杂度**，这是标准 Transformer 的主要限制。\n",
    "\n",
    "### 其他操作的复杂度\n",
    "| 操作 | 复杂度 | 说明 |\n",
    "|------|--------|------|\n",
    "| 线性投影（$XW$） | $\\mathcal{O}(B \\cdot T \\cdot d^2)$ | $d=d_{\\text{model}}$ |\n",
    "| FFN | $\\mathcal{O}(B \\cdot T \\cdot d \\cdot d_{\\mathrm{ff}})$ | 通常 $d_{\\mathrm{ff}}=4d$ |\n",
    "| LayerNorm | $\\mathcal{O}(B \\cdot T \\cdot d)$ | 轻量级操作 |\n",
    "\n",
    "**结论：** 当 $T$ 较大时，注意力的 $T^2$ 项占主导地位。\n",
    "\n",
    "---\n",
    "\n",
    "## 空间复杂度（显存占用）\n",
    "\n",
    "### 注意力权重矩阵\n",
    "$$\n",
    "\\mathcal{O}(B\\,\\cdot\\,H\\,\\cdot\\,T_q\\,\\cdot\\,T_k)\n",
    "$$\n",
    "\n",
    "**影响：**\n",
    "- 存储所有注意力权重用于反向传播\n",
    "- 自注意力时为 $\\mathcal{O}(B \\cdot H \\cdot T^2)$\n",
    "- 长序列（$T>1000$）时显存消耗显著\n",
    "\n",
    "### 优化方向\n",
    "- **FlashAttention**：融合操作，减少中间激活存储\n",
    "- **Sparse Attention**：仅计算部分注意力权重\n",
    "- **Gradient Checkpointing**：重新计算代替存储\n",
    "\n",
    "---\n",
    "\n",
    "## 易错点清单（面试高频）\n",
    "\n",
    "### 1. MHA 头部分割/合并时的维度变换\n",
    "**错误示例：**\n",
    "```python\n",
    "# 错误：直接 view 可能导致内存不连续\n",
    "x = x.view(B, T, H, d_k).transpose(1, 2)\n",
    "out = out.transpose(1, 2).view(B, T, d_model)  # 可能报错\n",
    "```\n",
    "\n",
    "**正确做法：**\n",
    "```python\n",
    "# 分头：先 view 再 transpose\n",
    "x = x.view(B, T, H, d_k).transpose(1, 2)  # (B,H,T,d_k)\n",
    "\n",
    "# 合头：transpose 后必须 contiguous()\n",
    "out = out.transpose(1, 2).contiguous().view(B, T, d_model)\n",
    "```\n",
    "\n",
    "**原因：** `transpose` 改变步长（stride），需要 `contiguous()` 使内存连续后才能 `view`。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Mask 的取值约定（1=可见 vs 1=遮挡）\n",
    "**本教程约定：** `mask[i,j]=1` 表示位置 $j$ **可见**，`0` 表示**遮挡**\n",
    "\n",
    "**实现：**\n",
    "```python\n",
    "scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "```\n",
    "\n",
    "**注意：** 不同框架/论文可能约定相反，务必统一！\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Decoder 自注意力的下三角 Mask\n",
    "**目的：** 防止未来信息泄露（如生成第3个词时不能看到第4、5个词）\n",
    "\n",
    "**生成方式：**\n",
    "```python\n",
    "mask = torch.tril(torch.ones(T, T))  # 下三角全1\n",
    "```\n",
    "\n",
    "**形状：** $(1, 1, T, T)$ 或 $(T, T)$（广播）\n",
    "\n",
    "**易错：** 忘记在推理时也需要此 mask！\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Cross-Attention 的 Q/K/V 来源\n",
    "**正确理解：**\n",
    "- $Q$：来自 **Decoder 当前层的输出**（\"我想查询什么\"）\n",
    "- $K, V$：来自 **Encoder 的输出记忆**（\"从源序列中提取信息\"）\n",
    "\n",
    "**代码：**\n",
    "```python\n",
    "cross_out = MultiHeadAttention(\n",
    "    x_q=decoder_hidden,    # Query from decoder\n",
    "    x_kv=encoder_memory    # Key/Value from encoder\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 残差连接 + LayerNorm 的顺序\n",
    "**Post-LN（原论文）：**\n",
    "```python\n",
    "x = LayerNorm(x + Sublayer(x))\n",
    "```\n",
    "\n",
    "**Pre-LN（现代常用）：**\n",
    "```python\n",
    "x = x + Sublayer(LayerNorm(x))\n",
    "```\n",
    "\n",
    "**面试要点：** 能说明两者差异和适用场景。\n",
    "\n",
    "---\n",
    "\n",
    "### 6. 位置编码长度要足够\n",
    "**问题：** 若 `max_len=512` 但输入序列长度为 600，会越界！\n",
    "\n",
    "**解决：**\n",
    "```python\n",
    "# 动态裁剪\n",
    "T = x.size(1)\n",
    "x = x + self.pe[:, :T, :]  # 只取前 T 个位置\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 面试快速讲解结构（建议话术）\n",
    "\n",
    "### 1. 总体架构（30秒）\n",
    "> \"Transformer 由 Encoder-Decoder 组成。Encoder 用多头自注意力捕获源序列的全局依赖，Decoder 在生成时通过掩码自注意力保证自回归特性，并用交叉注意力融合源序列信息。每个子层后都有残差连接和 LayerNorm。\"\n",
    "\n",
    "### 2. 核心公式（1分钟）\n",
    "> \"注意力的核心是缩放点积：$\\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}(\\frac{QK^{\\top}}{\\sqrt{d_k}})V$。多头注意力通过 $H$ 个并行头捕获不同子空间的特征，拼接后再投影。位置编码用正弦余弦函数注入位置信息。\"\n",
    "\n",
    "### 3. 形状追踪（关键！）\n",
    "> \"输入 $(B,T,d_{\\text{model}})$ → 投影并分头为 $(B,H,T,d_k)$ → 注意力计算 $(B,H,T,T)$ 的权重矩阵 → 加权求和得 $(B,H,T,d_v)$ → 合头回 $(B,T,d_{\\text{model}})$。\"\n",
    "\n",
    "### 4. Mask 机制（必考）\n",
    "> \"Encoder 用 padding mask 屏蔽 PAD。Decoder 有两种 mask：自注意力用下三角 causal mask 防止看到未来，交叉注意力用 padding mask 屏蔽源序列的 PAD。\"\n",
    "\n",
    "### 5. 复杂度（加分项）\n",
    "> \"自注意力的复杂度是 $\\mathcal{O}(T^2 \\cdot d)$，$T^2$ 是瓶颈。长序列场景可用 Sparse Attention、Linformer、Performer 等优化。\"\n",
    "\n",
    "---\n",
    "\n",
    "## 可扩展点（展示深度理解）\n",
    "\n",
    "1. **相对位置编码（RPE）**：如 T5、XLNet 的相对位置偏置\n",
    "2. **Pre-LN vs Post-LN**：训练稳定性差异\n",
    "3. **RoPE（旋转位置编码）**：LLaMA 等模型采用，外推能力强\n",
    "4. **FlashAttention**：IO 优化，加速 2-4 倍\n",
    "5. **Efficient Transformer**：Linformer、Performer、Reformer 等 $\\mathcal{O}(T)$ 变体\n",
    "6. **参数共享**：ALBERT 跨层共享参数降低模型大小\n",
    "\n",
    "---\n",
    "\n",
    "## 维度速查表（面试快速核对）\n",
    "\n",
    "| 符号 | 含义 | 典型值 |\n",
    "|------|------|--------|\n",
    "| $B$ | Batch size | 32, 64 |\n",
    "| $T$ | 序列长度 | 128, 512 |\n",
    "| $d_{\\text{model}}$ | 模型维度 | 512, 768 |\n",
    "| $H$ | 注意力头数 | 8, 12 |\n",
    "| $d_k = d_{\\text{model}}/H$ | 每头维度 | 64 |\n",
    "| $d_{\\mathrm{ff}}$ | FFN 隐藏层 | $4d_{\\text{model}}$ |\n",
    "| $V$ | 词表大小 | 30k-50k |\n",
    "\n",
    "**记忆技巧：** \"BTD-HK\" → Batch-Time-Dmodel-Heads-dK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d7dc3",
   "metadata": {},
   "source": [
    "# 面试高频问题详解\n",
    "\n",
    "本节针对面试中常被问到但容易答不好的核心问题，提供清晰、完整、可直接背诵的答案。\n",
    "\n",
    "---\n",
    "\n",
    "## 问题 1：影响 Transformer 主要复杂度的因素有哪些？\n",
    "\n",
    "### 标准回答框架（推荐直接背诵）\n",
    "> \"Transformer 的计算复杂度主要受**五个因素**影响：\n",
    "> 1. **序列长度 $T$**：影响最大，呈**二次关系** $\\mathcal{O}(T^2)$\n",
    "> 2. **模型维度 $d$**：线性或平方关系，取决于操作类型\n",
    "> 3. **层数 $N$**：线性叠加 $\\mathcal{O}(N)$\n",
    "> 4. **批次大小 $B$**：线性影响 $\\mathcal{O}(B)$\n",
    "> 5. **注意力头数 $H$**：对计算复杂度影响较小，主要影响显存\n",
    "> \n",
    "> 其中，**序列长度 $T$ 是唯一导致二次增长的因素**，是长文本处理的主要瓶颈。\"\n",
    "\n",
    "---\n",
    "\n",
    "### 核心组件复杂度\n",
    "\n",
    "对于单层 Transformer，长度为 $T$、模型维度为 $d$ 的序列：\n",
    "- **自注意力：** $\\mathcal{O}(T^2 \\cdot d)$ ← $T^2$ 瓶颈\n",
    "- **前馈网络：** $\\mathcal{O}(T \\cdot d^2)$ ← $d^2$ 瓶颈\n",
    "\n",
    "---\n",
    "\n",
    "### 详细推导\n",
    "\n",
    "#### 1. 自注意力复杂度：$\\mathcal{O}(T^2 \\cdot d)$\n",
    "\n",
    "**计算步骤拆解：**\n",
    "\n",
    "| 步骤 | 操作 | 输入形状 | 输出形状 | 复杂度 | 说明 |\n",
    "|------|------|----------|----------|--------|------|\n",
    "| 1️⃣ | 生成 $Q, K, V$ | $(B,T,d)$ | $(B,T,d)$ | $\\mathcal{O}(T \\cdot d^2)$ | 3 次线性投影：$XW_Q, XW_K, XW_V$ |\n",
    "| 2️⃣ | 计算 $QK^{\\top}$ | $(B,H,T,d_k)$ | $(B,H,T,T)$ | $\\mathcal{O}(T^2 \\cdot d)$ | **关键瓶颈：** 矩阵乘法 |\n",
    "| 3️⃣ | Softmax | $(B,H,T,T)$ | $(B,H,T,T)$ | $\\mathcal{O}(T^2)$ | 对每行归一化 |\n",
    "| 4️⃣ | 乘以 $V$ | $(B,H,T,T) \\times (B,H,T,d_k)$ | $(B,H,T,d_k)$ | $\\mathcal{O}(T^2 \\cdot d)$ | 加权求和 |\n",
    "| 5️⃣ | 输出投影 | $(B,T,d)$ | $(B,T,d)$ | $\\mathcal{O}(T \\cdot d^2)$ | $W_O$ 线性变换 |\n",
    "\n",
    "**总复杂度分析：**\n",
    "- **步骤 2️⃣ 和 4️⃣ 占主导**：$\\mathcal{O}(T^2 \\cdot d)$\n",
    "- 步骤 1️⃣ 和 5️⃣：$\\mathcal{O}(T \\cdot d^2)$（当 $T>d$ 时可忽略）\n",
    "\n",
    "**结论：** 自注意力 = $\\mathcal{O}(T^2 \\cdot d)$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 前馈网络（FFN）复杂度：$\\mathcal{O}(T \\cdot d^2)$\n",
    "\n",
    "**结构：** 两层全连接\n",
    "$$\n",
    "\\mathrm{FFN}(x) = W_2 \\,\\sigma(W_1 x + b_1) + b_2\n",
    "$$\n",
    "\n",
    "**权重矩阵维度：**\n",
    "- $W_1$: $(d, d_{\\mathrm{ff}})$，通常 $d_{\\mathrm{ff}} = 4d$\n",
    "- $W_2$: $(d_{\\mathrm{ff}}, d)$\n",
    "\n",
    "**计算量（忽略常数）：**\n",
    "$$\n",
    "\\mathcal{O}\\big(T \\cdot (d \\cdot d_{\\mathrm{ff}} + d_{\\mathrm{ff}} \\cdot d)\\big) = \\mathcal{O}(T \\cdot d \\cdot d_{\\mathrm{ff}}) = \\mathcal{O}(T \\cdot d^2)\n",
    "$$\n",
    "\n",
    "（假设 $d_{\\mathrm{ff}} = 4d$）\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 单层 Transformer 总复杂度\n",
    "\n",
    "**Encoder/Decoder 单层包含：**\n",
    "- 多头自注意力：$\\mathcal{O}(T^2 \\cdot d)$\n",
    "- FFN：$\\mathcal{O}(T \\cdot d^2)$\n",
    "\n",
    "**总复杂度：**\n",
    "$$\n",
    "\\mathcal{O}(T^2 \\cdot d + T \\cdot d^2) = \\mathcal{O}(T^2 \\cdot d + T \\cdot d^2)\n",
    "$$\n",
    "\n",
    "**不同场景下的主导项：**\n",
    "- **长序列场景**（$T > d$，如 $T=1024, d=512$）：$\\boxed{\\mathcal{O}(T^2 \\cdot d)}$ 主导\n",
    "- **短序列场景**（$T < d$，少见）：$\\mathcal{O}(T \\cdot d^2)$ 可能更大\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 完整模型（$N$ 层）复杂度\n",
    "\n",
    "**总层数：** $N_{\\text{enc}} + N_{\\text{dec}}$（通常各 6 层）\n",
    "\n",
    "**总复杂度：**\n",
    "$$\n",
    "\\mathcal{O}\\big(N \\cdot (T^2 \\cdot d + T \\cdot d^2)\\big)\n",
    "$$\n",
    "\n",
    "**Decoder 额外开销：**\n",
    "- Cross-Attention：$\\mathcal{O}(T_{\\text{tgt}} \\cdot T_{\\text{src}} \\cdot d)$\n",
    "- 若 $T_{\\text{tgt}} \\approx T_{\\text{src}}$，仍为 $\\mathcal{O}(T^2 \\cdot d)$\n",
    "\n",
    "---\n",
    "\n",
    "### 实际影响与优化方向\n",
    "\n",
    "#### 为什么 $T^2$ 是问题？\n",
    "| 序列长度 $T$ | 计算量（相对于 $T=128$） | 显存占用（注意力矩阵） |\n",
    "|--------------|-------------------------|------------------------|\n",
    "| 128 | 1× | 128² = 16K |\n",
    "| 512 | 16× | 512² = 262K |\n",
    "| 1024 | 64× | 1024² = 1M |\n",
    "| 4096 | 1024× ⚠️ | 4096² = 16M |\n",
    "\n",
    "**瓶颈：** 长文本、高分辨率图像（ViT）、视频（每帧作为 token）\n",
    "\n",
    "---\n",
    "\n",
    "#### 优化方法对比\n",
    "\n",
    "| 方法 | 复杂度 | 核心思想 | 代表模型 |\n",
    "|------|--------|----------|----------|\n",
    "| **Sparse Attention** | $\\mathcal{O}(T \\sqrt{T} \\cdot d)$ | 仅计算部分位置对 | Sparse Transformer, Longformer |\n",
    "| **Linformer** | $\\mathcal{O}(T \\cdot d)$ | 低秩近似 $K,V$ | Linformer |\n",
    "| **Performer** | $\\mathcal{O}(T \\cdot d)$ | 核方法近似 Softmax | Performer |\n",
    "| **FlashAttention** | $\\mathcal{O}(T^2 \\cdot d)$ | IO 优化，无复杂度降低 | GPT-3, LLaMA |\n",
    "| **Linear Attention** | $\\mathcal{O}(T \\cdot d^2)$ | 改变运算顺序 | TransNormer |\n",
    "\n",
    "---\n",
    "\n",
    "### 五大影响因素详解\n",
    "\n",
    "#### 因素对比总览\n",
    "\n",
    "| 因素 | 符号 | 对计算复杂度的影响 | 对显存的影响 | 典型值 | 调整难度 |\n",
    "|------|------|-------------------|-------------|--------|---------|\n",
    "| **序列长度** | $T$ | $\\mathcal{O}(T^2)$ 🔥🔥🔥 | $\\mathcal{O}(T^2)$ 🔥🔥🔥 | 128-2048 | 高（受任务限制） |\n",
    "| **模型维度** | $d$ | $\\mathcal{O}(d)$～$\\mathcal{O}(d^2)$ | $\\mathcal{O}(d)$ | 512-1024 | 中（可调整） |\n",
    "| **层数** | $N$ | $\\mathcal{O}(N)$ | $\\mathcal{O}(N)$ | 6-24 | 低（易扩展） |\n",
    "| **批次大小** | $B$ | $\\mathcal{O}(B)$ | $\\mathcal{O}(B)$ | 8-256 | 易（直接控制） |\n",
    "| **注意力头数** | $H$ | $\\mathcal{O}(1)$ | $\\mathcal{O}(H)$ | 8-16 | 低（影响小） |\n",
    "\n",
    "**关键结论：** 序列长度 $T$ 是**唯一导致二次增长**的因素，影响最大！\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 序列长度 $T$：二次增长瓶颈 🔥\n",
    "\n",
    "**影响机制：**\n",
    "- **自注意力**：计算 $T \\times T$ 的注意力矩阵\n",
    "  - $QK^{\\top}$：$\\mathcal{O}(T^2 \\cdot d_k)$\n",
    "  - Softmax：$\\mathcal{O}(T^2)$\n",
    "  - 注意力矩阵存储：$\\mathcal{O}(B \\cdot H \\cdot T^2)$\n",
    "\n",
    "**增长曲线：**\n",
    "| $T$ | 相对计算量 | 相对显存 | 实际影响 |\n",
    "|-----|-----------|---------|---------|\n",
    "| 128 | 1× | 1× | ✅ 基线 |\n",
    "| 256 | 4× | 4× | ⚠️ 可接受 |\n",
    "| 512 | 16× | 16× | ⚠️ 需优化 |\n",
    "| 1024 | 64× | 64× | 🔥 显存瓶颈 |\n",
    "| 2048 | 256× | 256× | ❌ 通常 OOM |\n",
    "\n",
    "**为什么影响最大？**\n",
    "- 唯一的 $\\mathcal{O}(T^2)$ 项\n",
    "- 长度翻倍 → 计算/显存增长 **4 倍**\n",
    "- 应用场景（长文本、视频）受限\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 模型维度 $d$：双重影响\n",
    "\n",
    "**影响机制：**\n",
    "- **自注意力**：$\\mathcal{O}(T^2 \\cdot d)$ ← 线性影响\n",
    "- **线性投影**（$Q,K,V,O$）：$\\mathcal{O}(T \\cdot d^2)$ ← 平方影响\n",
    "- **FFN**：$\\mathcal{O}(T \\cdot d \\cdot d_{\\text{ff}}) = \\mathcal{O}(T \\cdot d^2)$ ← 平方影响（$d_{\\text{ff}}=4d$）\n",
    "\n",
    "**不同 $T$ 下的主导项：**\n",
    "```\n",
    "当 T > d：自注意力 O(T²·d) 主导\n",
    "当 T ≈ d：两者相当\n",
    "当 T < d：FFN O(T·d²) 主导（罕见）\n",
    "```\n",
    "\n",
    "**典型配置：**\n",
    "- **BERT-Base**：$d=768$\n",
    "- **BERT-Large**：$d=1024$\n",
    "- **GPT-3**：$d=12288$\n",
    "- **扩大策略**：维度翻倍 → 参数量增长 **4 倍**\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 层数 $N$：线性叠加\n",
    "\n",
    "**影响机制：**\n",
    "- **计算量**：$N$ 层串行 → $\\mathcal{O}(N \\cdot \\text{单层复杂度})$\n",
    "- **显存**：每层激活值存储（训练时）\n",
    "\n",
    "**优势：** 相比序列长度，层数增长是**线性**的！\n",
    "\n",
    "**实际配置：**\n",
    "| 模型 | 层数 | 说明 |\n",
    "|------|------|------|\n",
    "| Transformer-Base | 6 | 原论文 |\n",
    "| BERT-Base | 12 | 2× 层数 |\n",
    "| GPT-2 | 48 | 8× 层数 |\n",
    "| GPT-3 | 96 | 16× 层数 |\n",
    "\n",
    "**扩展策略：**\n",
    "- ✅ **深度优先**：增加层数比增加宽度（$d$）更有效\n",
    "- ⚠️ **梯度问题**：需 Pre-LN、残差连接、精心初始化\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 批次大小 $B$：显存与效率的权衡\n",
    "\n",
    "**影响机制：**\n",
    "- **计算量**：线性增长 $\\mathcal{O}(B)$\n",
    "- **显存**：线性增长 $\\mathcal{O}(B)$（存储所有样本的激活值）\n",
    "- **训练效率**：更大 $B$ → 更稳定的梯度估计\n",
    "\n",
    "**权衡：**\n",
    "```\n",
    "小 B (8-16)：\n",
    "  ✅ 显存占用低\n",
    "  ❌ 梯度噪声大，训练不稳定\n",
    "\n",
    "大 B (128-512)：\n",
    "  ✅ 训练稳定，收敛快\n",
    "  ❌ 显存占用高\n",
    "  ❌ 泛化能力可能下降（需调整学习率）\n",
    "```\n",
    "\n",
    "**实践技巧：**\n",
    "- **梯度累积**：模拟大 batch size\n",
    "  ```python\n",
    "  accumulation_steps = 4\n",
    "  for i, batch in enumerate(dataloader):\n",
    "      loss = model(batch) / accumulation_steps\n",
    "      loss.backward()\n",
    "      if (i + 1) % accumulation_steps == 0:\n",
    "          optimizer.step()\n",
    "          optimizer.zero_grad()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 注意力头数 $H$：影响最小\n",
    "\n",
    "**影响机制：**\n",
    "- **计算复杂度**：$\\mathcal{O}(1)$（总维度不变，$H \\times d_k = d$）\n",
    "- **显存**：$\\mathcal{O}(H)$（需存储每个头的注意力权重）\n",
    "- **表达能力**：更多头 → 捕获多种模式\n",
    "\n",
    "**为什么影响小？**\n",
    "```\n",
    "单头：d_k = 512,  1 个注意力矩阵\n",
    "8 头：d_k = 64,   8 个注意力矩阵\n",
    "总计算量相同！（512 vs 8×64）\n",
    "```\n",
    "\n",
    "**典型配置：**\n",
    "- $d=512$：$H=8$（$d_k=64$）\n",
    "- $d=1024$：$H=16$（$d_k=64$）\n",
    "- 通常保持 $d_k \\in [64, 128]$\n",
    "\n",
    "---\n",
    "\n",
    "### 综合复杂度公式\n",
    "\n",
    "**完整 Transformer（$N$ 层）的总复杂度：**\n",
    "$$\n",
    "\\boxed{\\mathcal{O}\\big(B \\cdot N \\cdot (T^2 \\cdot d + T \\cdot d^2)\\big)}\n",
    "$$\n",
    "\n",
    "**各因素贡献：**\n",
    "```\n",
    "序列长度 T：二次增长 T²    🔥🔥🔥 主要瓶颈\n",
    "模型维度 d：平方增长 d²    🔥🔥   次要瓶颈\n",
    "层数 N：    线性增长 N     🔥     可控\n",
    "批次大小 B：线性增长 B     ✅     易调整\n",
    "头数 H：    几乎无影响      ✅     可忽略\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 面试加分回答模板\n",
    "> \"Transformer 的复杂度主要受五个因素影响：\n",
    "> \n",
    "> **1. 序列长度 $T$（最重要）：** 自注意力的 $\\mathcal{O}(T^2 \\cdot d)$ 复杂度导致长度翻倍时计算量增长 4 倍，这是处理长文本的主要瓶颈。\n",
    "> \n",
    "> **2. 模型维度 $d$：** 对自注意力是线性 $\\mathcal{O}(T^2 \\cdot d)$，对 FFN 是平方 $\\mathcal{O}(T \\cdot d^2)$，当 $T>d$ 时前者主导。\n",
    "> \n",
    "> **3. 层数 $N$：** 线性叠加，增加层数是扩展模型最有效的方式（如 GPT-3 有 96 层）。\n",
    "> \n",
    "> **4. 批次大小 $B$：** 线性影响计算和显存，实践中通过梯度累积模拟大 batch。\n",
    "> \n",
    "> **5. 注意力头数 $H$：** 对计算复杂度几乎无影响（总维度不变），主要增加少量显存开销。\n",
    "> \n",
    "> 业界优化主要针对 $T^2$ 瓶颈：FlashAttention（IO 优化）、Sparse Attention（$T \\sqrt{T}$）、Linformer/Performer（线性复杂度 $T$）等。\"\n",
    "\n",
    "---\n",
    "\n",
    "## 问题 2：Embedding 层是什么？有什么作用？\n",
    "\n",
    "### 标准回答框架（推荐直接背诵）\n",
    "> \"Embedding 层的作用是将**离散的 token ID**（整数）映射为**连续的向量表示**（浮点数），使模型能够处理和学习 token 之间的语义关系。它本质上是一个**可学习的查找表**，每个 token 对应一个固定维度的向量。\"\n",
    "\n",
    "---\n",
    "\n",
    "### 详细解析\n",
    "\n",
    "#### 1. 什么是 Embedding？\n",
    "\n",
    "**定义：**\n",
    "- **输入：** Token ID（整数），如 `[5, 12, 3]`（词表索引）\n",
    "- **输出：** Dense Vector（浮点向量），如 `[[0.2, -0.5, ...], [0.1, 0.3, ...], ...]`\n",
    "- **维度：** $(B, T) \\xrightarrow{\\text{Embedding}} (B, T, d_{\\text{model}})$\n",
    "\n",
    "**可视化：**\n",
    "```\n",
    "词表 (Vocab Size = 30,000):\n",
    "┌────────────┬─────────────────────────────┐\n",
    "│ Token ID   │ Embedding Vector (d=512)   │\n",
    "├────────────┼─────────────────────────────┤\n",
    "│ 0 (PAD)    │ [0.00, 0.00, ..., 0.00]    │\n",
    "│ 1 (BOS)    │ [0.12, -0.34, ..., 0.56]   │\n",
    "│ 2 (EOS)    │ [-0.23, 0.45, ..., -0.12]  │\n",
    "│ 3 (\"hello\")│ [0.45, 0.12, ..., -0.78]   │\n",
    "│ 4 (\"world\")│ [-0.12, 0.89, ..., 0.34]   │\n",
    "│ ...        │ ...                         │\n",
    "└────────────┴─────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 为什么需要 Embedding？\n",
    "\n",
    "**问题：** 为什么不能直接用 one-hot 编码？\n",
    "\n",
    "| 方法 | 表示形式 | 维度 | 语义关系 | 参数量 |\n",
    "|------|----------|------|----------|--------|\n",
    "| **One-Hot** | `[0,0,1,0,...]` | $(T, V)$ | ❌ 无语义 | 无参数（固定） |\n",
    "| **Embedding** | `[0.2, -0.5, ...]` | $(T, d)$ | ✅ 可学习 | $V \\times d$ |\n",
    "\n",
    "**One-Hot 的问题：**\n",
    "1. **维度爆炸：** 若词表 $V=50,000$，每个 token 需 50K 维向量\n",
    "2. **无语义信息：** `\"dog\"` 和 `\"cat\"` 的 one-hot 向量点积为 0（正交），无法表达\"都是动物\"\n",
    "3. **稀疏表示：** 99.99% 的元素是 0，计算低效\n",
    "\n",
    "**Embedding 的优势：**\n",
    "1. **低维稠密：** $d=512 \\ll V=50,000$，降维 100 倍\n",
    "2. **语义编码：** 相似词（如 \"king\" 和 \"queen\"）的向量接近，可通过余弦相似度度量\n",
    "3. **可学习：** 通过反向传播自动学习最优表示\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Embedding 如何工作？\n",
    "\n",
    "**实现方式（PyTorch）：**\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建 Embedding 层\n",
    "embedding = nn.Embedding(num_embeddings=30000, embedding_dim=512)\n",
    "# 参数量：30000 × 512 = 15.36M\n",
    "\n",
    "# 使用\n",
    "token_ids = torch.tensor([[3, 12, 5, 2],    # 句子1：4个token\n",
    "                          [7, 9, 2, 0]])    # 句子2：3个token + 1个PAD\n",
    "# 形状：(B=2, T=4)\n",
    "\n",
    "embedded = embedding(token_ids)\n",
    "# 形状：(B=2, T=4, d=512)\n",
    "# embedded[0, 0] = 第1个句子第1个token的512维向量\n",
    "```\n",
    "\n",
    "**内部原理：**\n",
    "```python\n",
    "# 等价于查表操作\n",
    "weight = embedding.weight  # (30000, 512) 的可学习矩阵\n",
    "embedded = weight[token_ids]  # 索引查找\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Transformer 中的 Embedding 使用\n",
    "\n",
    "**完整输入流程：**\n",
    "```\n",
    "Token IDs (B, T)\n",
    "    ↓\n",
    "Embedding 查表\n",
    "    ↓\n",
    "Word Embeddings (B, T, d_model)\n",
    "    ↓ +\n",
    "Positional Encoding (1, T, d_model)  ← 位置信息\n",
    "    ↓\n",
    "输入表示 (B, T, d_model)\n",
    "    ↓\n",
    "进入 Transformer\n",
    "```\n",
    "\n",
    "**代码示例：**\n",
    "```python\n",
    "class TransformerInputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = PositionalEncoding(d_model, max_len, dropout)\n",
    "        self.scale = math.sqrt(d_model)  # 缩放因子（原论文使用）\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, T) token IDs\n",
    "        # 1. Token embedding\n",
    "        tok_emb = self.token_embed(x) * self.scale  # (B, T, d_model)\n",
    "        # 2. 加位置编码\n",
    "        output = self.pos_embed(tok_emb)  # (B, T, d_model)\n",
    "        return output\n",
    "```\n",
    "\n",
    "**为什么乘以 $\\sqrt{d_{\\text{model}}}$？**\n",
    "- **原因：** Embedding 初始化时方差较小，位置编码可能覆盖 token 信息\n",
    "- **作用：** 放大 token embedding 使其与位置编码在相同量级\n",
    "- **原论文公式：** $\\text{Input} = \\sqrt{d_{\\text{model}}} \\cdot E_{\\text{token}} + E_{\\text{pos}}$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Embedding 与权重共享\n",
    "\n",
    "**技巧：** Encoder/Decoder 共享 Embedding 权重 + 输出层共享（Weight Tying）\n",
    "\n",
    "**代码示例：**\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        # 共享的 embedding\n",
    "        self.shared_embed = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Encoder/Decoder 使用同一 embedding\n",
    "        self.encoder_embed = self.shared_embed\n",
    "        self.decoder_embed = self.shared_embed\n",
    "        \n",
    "        # 输出层共享 embedding 权重（转置使用）\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.output_proj.weight = self.shared_embed.weight  # 权重共享\n",
    "```\n",
    "\n",
    "**好处：**\n",
    "- **减少参数量：** 节省 $V \\times d$ 参数\n",
    "- **强制一致性：** 输入和输出使用相同的语义空间\n",
    "- **改善低资源任务：** 减少过拟合风险\n",
    "\n",
    "---\n",
    "\n",
    "### 面试加分回答模板\n",
    "> \"Embedding 层将离散的 token ID 映射为连续向量，使模型能够处理语义信息。它本质是一个 $(V, d)$ 的可学习查找表，相比 one-hot 编码降低了维度（如 50K → 512）并能表达语义相似性。\n",
    "> \n",
    "> 在 Transformer 中，token embedding 会乘以 $\\sqrt{d_{\\text{model}}}$ 后与位置编码相加，形成最终输入。实践中常使用权重共享（Weight Tying）：Encoder/Decoder 共享 embedding，输出层转置复用该权重，可减少约 1/3 参数量。\n",
    "> \n",
    "> 现代模型如 BERT 使用 WordPiece/BPE 子词切分 + 共享 embedding；GPT 系列使用 Byte-Pair Encoding；多模态模型（CLIP/DALL-E）则用独立的图像/文本 embedding 再投影到统一空间。\"\n",
    "\n",
    "---\n",
    "\n",
    "## 问题 3：序列长度会影响训练复杂度吗？如何影响？\n",
    "\n",
    "### 标准回答框架（推荐直接背诵）\n",
    "> \"序列长度对训练复杂度有**显著且非线性**的影响。在 Transformer 中，复杂度与序列长度呈**二次关系** $\\mathcal{O}(T^2)$，主要体现在：\n",
    "> 1. **计算量**：自注意力需计算 $T \\times T$ 的注意力矩阵\n",
    "> 2. **显存占用**：需存储 $(B, H, T, T)$ 的注意力权重用于反向传播\n",
    "> 3. **训练时间**：长序列导致单步迭代时间呈平方增长\n",
    "> \n",
    "> 当序列长度翻倍时，理论上计算量增长 **4 倍**，显存需求也增长 **4 倍**。\"\n",
    "\n",
    "---\n",
    "\n",
    "### 详细解析\n",
    "\n",
    "#### 1. 序列长度如何影响计算复杂度\n",
    "\n",
    "**自注意力的核心操作：** 计算 $QK^{\\top}$\n",
    "$$\n",
    "\\text{Scores} = \\frac{QK^{\\top}}{\\sqrt{d_k}} \\quad \\text{其中 } Q, K \\in \\mathbb{R}^{B \\times H \\times T \\times d_k}\n",
    "$$\n",
    "\n",
    "**复杂度分解：**\n",
    "- **矩阵乘法 $QK^{\\top}$：** $\\mathcal{O}(T \\cdot T \\cdot d_k) = \\mathcal{O}(T^2 \\cdot d_k)$\n",
    "- **Softmax + 乘以 $V$：** $\\mathcal{O}(T^2 \\cdot d_k)$\n",
    "- **总复杂度（单头）：** $\\mathcal{O}(T^2 \\cdot d_k)$\n",
    "- **多头（$H$ 个头）：** $\\mathcal{O}(H \\cdot T^2 \\cdot d_k) = \\mathcal{O}(T^2 \\cdot d)$\n",
    "\n",
    "**关键点：** $T^2$ 项是瓶颈！\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 实际增长曲线\n",
    "\n",
    "**对比不同序列长度的资源消耗：**\n",
    "\n",
    "| 序列长度 $T$ | 计算 FLOPs（相对） | 显存占用（相对） | 训练时间（相对） |\n",
    "|-------------|-------------------|-----------------|----------------|\n",
    "| 128 | **1×** | **1×** | **1×** |\n",
    "| 256 | **4×** ⚠️ | **4×** ⚠️ | **~4×** |\n",
    "| 512 | **16×** 🔥 | **16×** 🔥 | **~15×** |\n",
    "| 1024 | **64×** ❌ | **64×** ❌ | **~60×** |\n",
    "| 2048 | **256×** ❌❌ | **256×** ❌❌ | **OOM** |\n",
    "\n",
    "**说明：**\n",
    "- **计算 FLOPs：** 严格 $T^2$ 增长\n",
    "- **显存占用：** 注意力矩阵 $(B, H, T, T)$ 呈 $T^2$ 增长\n",
    "- **训练时间：** 实际略小于理论值（IO 开销相对减少）\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 显存占用的详细分析\n",
    "\n",
    "**Transformer 训练时的显存占用：**\n",
    "\n",
    "| 组件 | 形状 | 复杂度 | 说明 |\n",
    "|------|------|--------|------|\n",
    "| **输入/输出激活** | $(B, T, d)$ | $\\mathcal{O}(BTd)$ | 线性增长 |\n",
    "| **注意力分数** | $(B, H, T, T)$ | $\\mathcal{O}(BHT^2)$ | **二次增长** ⚠️ |\n",
    "| **注意力权重** | $(B, H, T, T)$ | $\\mathcal{O}(BHT^2)$ | 需保存用于反向传播 |\n",
    "| **梯度** | 同前向 | $\\mathcal{O}(BHT^2)$ | 反向传播时翻倍 |\n",
    "| **优化器状态** | 同参数 | $\\mathcal{O}(V \\cdot d)$ | 与序列长度无关 |\n",
    "\n",
    "**总显存（训练时）：**\n",
    "$$\n",
    "\\text{Memory} \\approx \\mathcal{O}\\big(BTd + N \\cdot BHT^2\\big)\n",
    "$$\n",
    "- $N$：层数\n",
    "- 当 $T$ 较大时，$BHT^2$ 项主导\n",
    "\n",
    "**实际数值示例：**\n",
    "- 配置：$B=32, H=8, T=1024, d=512$（单层）\n",
    "- 注意力矩阵：$32 \\times 8 \\times 1024 \\times 1024 \\times 4\\text{B} = 1\\text{GB}$\n",
    "- 梯度：再翻倍 = **2GB**（仅单层！）\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 对不同阶段的影响\n",
    "\n",
    "**训练（Training）：**\n",
    "- **瓶颈：** 显存（需存储所有中间激活）\n",
    "- **影响：** 长序列 → Batch size 被迫减小 → 训练不稳定\n",
    "- **解决：** Gradient Checkpointing（用时间换空间）\n",
    "\n",
    "**推理（Inference）：**\n",
    "- **瓶颈：** 计算量（尤其是自回归解码）\n",
    "- **影响：** 解码 $T$ 个 token 需 $\\mathcal{O}(T^2)$ 次前向传播\n",
    "- **解决：** KV Cache（缓存已计算的 Key/Value）\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 优化策略对比\n",
    "\n",
    "**针对长序列的主流优化方法：**\n",
    "\n",
    "| 方法 | 复杂度降低 | 显存节省 | 速度提升 | 准确度影响 | 适用场景 |\n",
    "|------|-----------|---------|---------|-----------|----------|\n",
    "| **Gradient Checkpointing** | ❌ 无 | ✅ 50% | ❌ -30% | ✅ 无损 | 显存受限 |\n",
    "| **FlashAttention** | ❌ 无 | ✅ 显著 | ✅ 2-4× | ✅ 无损 | 通用加速 |\n",
    "| **Sparse Attention** | ✅ $T^2 \\to T\\sqrt{T}$ | ✅ 对应降低 | ✅ 对应提升 | ⚠️ 略降 | 长文档 |\n",
    "| **Linformer** | ✅ $T^2 \\to T$ | ✅ 显著 | ✅ 显著 | ⚠️ 降低 | 固定长度 |\n",
    "| **KV Cache（推理）** | ✅ $T^2 \\to T$ | ⚠️ 增加 | ✅ 10-100× | ✅ 无损 | 自回归生成 |\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. 实际训练策略\n",
    "\n",
    "**长序列训练的工程实践：**\n",
    "\n",
    "1. **混合训练（Mixed Training）：**\n",
    "   ```python\n",
    "   # 预训练：短序列（512）高 batch size (64)\n",
    "   # 微调：长序列（2048）低 batch size (8)\n",
    "   ```\n",
    "\n",
    "2. **渐进式长度增长（Progressive Length Training）：**\n",
    "   ```\n",
    "   阶段1: T=128, BS=256, 10K steps\n",
    "   阶段2: T=256, BS=128, 5K steps\n",
    "   阶段3: T=512, BS=64,  5K steps\n",
    "   ```\n",
    "\n",
    "3. **动态 Padding（避免浪费）：**\n",
    "   ```python\n",
    "   # 坏做法：所有样本 pad 到最大长度 2048\n",
    "   # 好做法：每个 batch 内 pad 到最长样本\n",
    "   collate_fn = DataCollatorWithPadding(tokenizer, padding='longest')\n",
    "   ```\n",
    "\n",
    "4. **Attention 分段（Chunking）：**\n",
    "   ```python\n",
    "   # 将长序列切分为多个 chunk，分别计算注意力\n",
    "   for chunk in split_sequence(seq, chunk_size=512):\n",
    "       attn_output = attention(chunk)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### 面试加分回答模板\n",
    "> \"序列长度对 Transformer 训练复杂度影响巨大，主要体现在计算和显存的**二次增长** $\\mathcal{O}(T^2)$。例如序列长度从 512 增加到 1024，计算量和显存占用都会增长 **4 倍**。\n",
    "> \n",
    "> 这导致实际训练中长序列面临两大挑战：\n",
    "> 1. **显存不足：** 注意力矩阵 $(B, H, T, T)$ 占用大量显存，迫使 batch size 减小\n",
    "> 2. **训练慢：** $T^2$ 的计算量导致单步迭代时间快速增长\n",
    "> \n",
    "> 业界主流解决方案：\n",
    "> - **训练优化：** FlashAttention（IO 优化，2-4× 加速）、Gradient Checkpointing（节省 50% 显存但慢 30%）\n",
    "> - **架构优化：** Sparse Attention（Longformer 的滑动窗口 + 全局注意力）、Linear Attention（复杂度降至 $\\mathcal{O}(T)$）\n",
    "> - **工程技巧：** 渐进式长度训练（从短序列开始逐步增长）、动态 padding（避免不必要的 pad）、KV Cache（推理加速）\n",
    "> \n",
    "> 实际应用中，GPT-3 训练时使用 2048 的上下文长度，后期才扩展到 4096；Claude-2 通过稀疏注意力支持 100K tokens；LLaMA-2 使用 RoPE + FlashAttention 高效处理长序列。\"\n",
    "\n",
    "---\n",
    "\n",
    "### 快速对比：序列长度 vs 其他因素\n",
    "\n",
    "| 因素 | 对计算复杂度的影响 | 对显存的影响 | 调整难度 |\n",
    "|------|-------------------|-------------|---------|\n",
    "| **序列长度 $T$** | $\\mathcal{O}(T^2)$ 🔥 | $\\mathcal{O}(T^2)$ 🔥 | 高（受任务限制） |\n",
    "| **模型维度 $d$** | $\\mathcal{O}(d)$ | $\\mathcal{O}(d)$ | 中（可调整） |\n",
    "| **层数 $N$** | $\\mathcal{O}(N)$ | $\\mathcal{O}(N)$ | 低（易扩展） |\n",
    "| **Batch size $B$** | $\\mathcal{O}(B)$ | $\\mathcal{O}(B)$ | 易（直接控制） |\n",
    "| **头数 $H$** | $\\mathcal{O}(1)$ | $\\mathcal{O}(H)$ | 低（影响小） |\n",
    "\n",
    "**结论：** 序列长度是唯一导致**二次增长**的因素，影响最大！\n",
    "\n",
    "---\n",
    "\n",
    "### 代码示例：测量复杂度\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def measure_attention_cost(seq_len, d_model=512, num_heads=8, batch_size=32):\n",
    "    \"\"\"测量不同序列长度下的注意力计算开销\"\"\"\n",
    "    d_k = d_model // num_heads\n",
    "    Q = torch.randn(batch_size, num_heads, seq_len, d_k, device='cuda')\n",
    "    K = torch.randn(batch_size, num_heads, seq_len, d_k, device='cuda')\n",
    "    V = torch.randn(batch_size, num_heads, seq_len, d_k, device='cuda')\n",
    "    \n",
    "    # 预热\n",
    "    for _ in range(10):\n",
    "        _ = F.scaled_dot_product_attention(Q, K, V)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # 计时\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        out = F.scaled_dot_product_attention(Q, K, V)\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # 显存\n",
    "    mem_used = torch.cuda.max_memory_allocated() / 1024**3  # GB\n",
    "    \n",
    "    return elapsed / 100, mem_used\n",
    "\n",
    "# 测试不同序列长度\n",
    "for T in [128, 256, 512, 1024]:\n",
    "    time_per_step, mem = measure_attention_cost(T)\n",
    "    print(f\"T={T:4d} | Time: {time_per_step*1000:.2f}ms | Memory: {mem:.2f}GB\")\n",
    "```\n",
    "\n",
    "**预期输出（相对关系）：**\n",
    "```\n",
    "T= 128 | Time:  1.20ms | Memory: 0.5GB\n",
    "T= 256 | Time:  4.80ms | Memory: 2.0GB   (4× slower, 4× memory)\n",
    "T= 512 | Time: 19.20ms | Memory: 8.0GB   (16× slower, 16× memory)\n",
    "T=1024 | Time: 76.80ms | Memory: 32.0GB  (64× slower, 64× memory)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
