# Transformer ä»£ç å®ç°

æœ¬ç›®å½•åŒ…å«å®Œæ•´çš„ Transformer Encoder-Decoder å®ç°, ä»£ç å·²æ¨¡å—åŒ–åˆ†è§£ä¸ºå¤šä¸ªæ–‡ä»¶, ä¾¿äºç†è§£ã€å­¦ä¹ å’Œå¤ç”¨.

---

## ğŸ“ å®Œæ•´æ–‡ä»¶ç»“æ„

```
Code/
â”œâ”€â”€ æ ¸å¿ƒæ¨¡å— (æ¨èç‰ˆæœ¬ - æ¨¡å—åŒ–è®¾è®¡)
â”‚   â”œâ”€â”€ __init__.py                    # æ¨¡å—åˆå§‹åŒ–, ç»Ÿä¸€å¯¼å‡ºæ¥å£
â”‚   â”œâ”€â”€ attention.py                   # æ³¨æ„åŠ›æœºåˆ¶ (ScaledDotProductAttention, MultiHeadAttention)
â”‚   â”œâ”€â”€ embedding.py                   # ä½ç½®ç¼–ç  (PositionalEncoding)
â”‚   â”œâ”€â”€ feedforward.py                 # å‰é¦ˆç½‘ç»œå’Œå½’ä¸€åŒ– (FeedForward, ResidualLayerNorm)
â”‚   â”œâ”€â”€ mask.py                        # æ©ç å·¥å…·å‡½æ•° (make_pad_mask, make_subsequent_mask)
â”‚   â”œâ”€â”€ layers.py                      # Encoder/Decoder å±‚ (EncoderLayer, DecoderLayer)
â”‚   â””â”€â”€ transformer.py                 # å®Œæ•´ Transformer æ¨¡å‹ (Transformer)
â”‚
â”œâ”€â”€ å•ç‹¬æ¨¡å— (æ•™å­¦ç‰ˆæœ¬ - ä¾¿äºå•ç‹¬å­¦ä¹ )
â”‚   â”œâ”€â”€ ScaledDotProductAttention.py   # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (ç‹¬ç«‹æ–‡ä»¶)
â”‚   â””â”€â”€ MultiHeadAttention.py          # å¤šå¤´æ³¨æ„åŠ› (ç‹¬ç«‹æ–‡ä»¶)
â”‚
â”œâ”€â”€ æµ‹è¯•ä¸ç¤ºä¾‹
â”‚   â”œâ”€â”€ test_attention.py              # æ³¨æ„åŠ›æ¨¡å—æµ‹è¯• (5ä¸ªæµ‹è¯•åœºæ™¯)
â”‚   â””â”€â”€ ATTENTION_USAGE.md             # æ³¨æ„åŠ›æ¨¡å—ä½¿ç”¨æŒ‡å— (è¯¦ç»†ç¤ºä¾‹)
â”‚
â”œâ”€â”€ å‚è€ƒæ–‡æ¡£
â”‚   â”œâ”€â”€ README.md                      # æœ¬æ–‡ä»¶ (æ€»ä½“è¯´æ˜)
â”‚   â””â”€â”€ Combined.py                    # åŸå§‹åˆå¹¶ç‰ˆæœ¬ (å·²å¼ƒç”¨, ä»…ä¾›å‚è€ƒ)
```

---

## ğŸ¯ ä½¿ç”¨æŒ‡å—

### é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬

#### 1. **æ¨èç‰ˆæœ¬** (æ ¸å¿ƒæ¨¡å—)
é€‚ç”¨åœºæ™¯:
- âœ… æ„å»ºå®Œæ•´çš„ Transformer æ¨¡å‹
- âœ… é›†æˆåˆ°é¡¹ç›®ä¸­ä½¿ç”¨
- âœ… ç†è§£æ¨¡å—é—´çš„å…³ç³»
- âœ… é¢è¯•æ—¶å±•ç¤ºæ¶æ„è®¾è®¡èƒ½åŠ›

ä½¿ç”¨æ–¹å¼:
```python
from transformer import Transformer
# æˆ–
from attention import MultiHeadAttention
from layers import EncoderLayer, DecoderLayer
```

#### 2. **æ•™å­¦ç‰ˆæœ¬** (å•ç‹¬æ¨¡å—)
é€‚ç”¨åœºæ™¯:
- âœ… å­¦ä¹ å•ä¸ªç»„ä»¶çš„å®ç°
- âœ… é¢è¯•æ—¶æ‰‹æ’•ä»£ç  (é€ä¸ªå®ç°)
- âœ… æ·±å…¥ç†è§£æŸä¸ªæ¨¡å—
- âœ… å¿«é€Ÿæµ‹è¯•å’Œè°ƒè¯•

ä½¿ç”¨æ–¹å¼:
```python
from ScaledDotProductAttention import ScaledDotProductAttention
from MultiHeadAttention import MultiHeadAttention
```

---

## ğŸ“š æ¨¡å—è¯¦ç»†è¯´æ˜

### æ ¸å¿ƒæ¨¡å— (æ¨èç‰ˆæœ¬)

#### 1. `__init__.py` - æ¨¡å—åˆå§‹åŒ–

**åŠŸèƒ½**: ç»Ÿä¸€å¯¼å‡ºæ‰€æœ‰å…¬å…±æ¥å£

**å¯¼å‡ºå†…å®¹**:
```python
__all__ = [
    'ScaledDotProductAttention',    # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
    'MultiHeadAttention',            # å¤šå¤´æ³¨æ„åŠ›
    'PositionalEncoding',            # ä½ç½®ç¼–ç 
    'FeedForward',                   # å‰é¦ˆç½‘ç»œ
    'ResidualLayerNorm',             # æ®‹å·®è¿æ¥ + LayerNorm
    'make_pad_mask',                 # Padding æ©ç å·¥å…·
    'make_subsequent_mask',          # å› æœæ©ç å·¥å…·
    'EncoderLayer',                  # Encoder å±‚
    'DecoderLayer',                  # Decoder å±‚
    'Transformer',                   # å®Œæ•´æ¨¡å‹
]
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
# å¯¼å…¥æ‰€æœ‰ç»„ä»¶
from transformer_module import *

# æˆ–é€‰æ‹©æ€§å¯¼å…¥
from transformer_module import Transformer, MultiHeadAttention
```

---

#### 2. `attention.py` - æ³¨æ„åŠ›æœºåˆ¶ â­â­â­

**åŒ…å«ç±»**:
- `ScaledDotProductAttention`: ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (Attention çš„æ ¸å¿ƒ)
- `MultiHeadAttention`: å¤šå¤´æ³¨æ„åŠ› (Transformer çš„å…³é”®ç»„ä»¶)

**æ ¸å¿ƒå…¬å¼**:
$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**å…³é”®ç‰¹æ€§**:
- âœ… æ”¯æŒä»»æ„å½¢çŠ¶çš„ Q, K, V
- âœ… æ”¯æŒ Padding Mask å’Œ Causal Mask
- âœ… å¤šå¤´å¹¶è¡Œè®¡ç®—
- âœ… Dropout æ­£åˆ™åŒ–

**ä»£ç è¡Œæ•°**: ~150 è¡Œ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from attention import MultiHeadAttention

# åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›
mha = MultiHeadAttention(d_model=512, num_heads=8, dropout=0.1)

# è‡ªæ³¨æ„åŠ› (Self-Attention)
out, attn_weights = mha(x, x, mask=None)

# äº¤å‰æ³¨æ„åŠ› (Cross-Attention)
out, attn_weights = mha(query, key_value, mask=cross_mask)
```

**é¢è¯•é‡ç‚¹**:
- Q, K, V çš„æ¥æºå’Œä½œç”¨
- ä¸ºä»€ä¹ˆé™¤ä»¥ $\sqrt{d_k}$
- å¤šå¤´çš„åˆ†å‰²å’Œåˆå¹¶è¿‡ç¨‹
- Mask çš„åº”ç”¨æ—¶æœº

---

#### 3. `embedding.py` - ä½ç½®ç¼–ç  â­â­

**åŒ…å«ç±»**:
- `PositionalEncoding`: æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç 

**æ ¸å¿ƒå…¬å¼**:
$$
\begin{aligned}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{aligned}
$$

**å…³é”®ç‰¹æ€§**:
- âœ… å›ºå®šç¼–ç , æ— éœ€è®­ç»ƒå‚æ•°
- âœ… æ”¯æŒä»»æ„é•¿åº¦åºåˆ— (æœ€å¤§ `max_len`)
- âœ… ç›´æ¥åŠ åˆ° Token Embedding ä¸Š

**ä»£ç è¡Œæ•°**: ~40 è¡Œ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from embedding import PositionalEncoding

pos_enc = PositionalEncoding(d_model=512, max_len=5000, dropout=0.1)
x = pos_enc(x)  # x: (B, T, d_model)
```

**é¢è¯•é‡ç‚¹**:
- ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç 
- æ­£å¼¦/ä½™å¼¦çš„ä¼˜åŠ¿
- ä¸å¯å­¦ä¹ ä½ç½®ç¼–ç çš„åŒºåˆ«

---

#### 4. `feedforward.py` - å‰é¦ˆç½‘ç»œå’Œå½’ä¸€åŒ– â­â­

**åŒ…å«ç±»**:
- `FeedForward`: Position-wise Feed-Forward Network
- `ResidualLayerNorm`: æ®‹å·®è¿æ¥ + Layer Normalization

**æ ¸å¿ƒå…¬å¼**:
$$
\begin{aligned}
\text{FFN}(x) &= W_2 \cdot \sigma(W_1 \cdot x + b_1) + b_2 \\
\text{ResidualLN}(x, f) &= \text{LayerNorm}(x + f(x))
\end{aligned}
$$

**å…³é”®ç‰¹æ€§**:
- âœ… æ”¯æŒ ReLU å’Œ GELU æ¿€æ´»å‡½æ•°
- âœ… é€šå¸¸ $d_{\text{ff}} = 4 \times d_{\text{model}}$
- âœ… Post-LN å®ç° (ä¹Ÿå¯æ”¹ä¸º Pre-LN)

**ä»£ç è¡Œæ•°**: ~60 è¡Œ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from feedforward import FeedForward, ResidualLayerNorm

ffn = FeedForward(d_model=512, d_ff=2048, activation='gelu')
norm = ResidualLayerNorm(d_model=512)

# ä½¿ç”¨
out = ffn(x)
x = norm(x, out)  # æ®‹å·® + å½’ä¸€åŒ–
```

**é¢è¯•é‡ç‚¹**:
- FFN çš„ä½œç”¨ (éçº¿æ€§å˜æ¢)
- Post-LN vs Pre-LN çš„åŒºåˆ«
- ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥

---

#### 5. `mask.py` - æ©ç å·¥å…·å‡½æ•° â­â­

**åŒ…å«å‡½æ•°**:
- `make_pad_mask()`: æ„é€  Padding Mask
- `make_subsequent_mask()`: æ„é€  Causal Mask (ä¸‹ä¸‰è§’)

**æ©ç ç±»å‹**:

| æ©ç ç±»å‹ | ç”¨é€” | å½¢çŠ¶ | ç¤ºä¾‹ |
|---------|------|------|------|
| **Padding Mask** | å±è”½ PAD token | `(B, 1, T_q, T_k)` | Encoder/Decoder |
| **Causal Mask** | é˜²æ­¢çœ‹åˆ°æœªæ¥ | `(1, 1, T, T)` | Decoder Self-Attn |
| **Cross Mask** | å±è”½æºç«¯ PAD | `(B, 1, T_t, T_s)` | Decoder Cross-Attn |

**ä»£ç è¡Œæ•°**: ~50 è¡Œ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from mask import make_pad_mask, make_subsequent_mask

# Padding mask
pad_mask = make_pad_mask(T, T, src_pad, src_pad)

# Causal mask (ä¸‹ä¸‰è§’)
causal_mask = make_subsequent_mask(T)

# ç»„åˆ (Decoder è‡ªæ³¨æ„åŠ›)
tgt_mask = pad_mask & causal_mask if pad_mask is not None else causal_mask
```

**é¢è¯•é‡ç‚¹**:
- ä¸ºä»€ä¹ˆéœ€è¦ Padding Mask
- Causal Mask çš„ä½œç”¨ (è‡ªå›å½’)
- æ©ç çš„å–å€¼çº¦å®š (1=å¯è§, 0=å±è”½)

---

#### 6. `layers.py` - Encoder/Decoder å±‚ â­â­â­

**åŒ…å«ç±»**:
- `EncoderLayer`: Transformer Encoder å±‚
- `DecoderLayer`: Transformer Decoder å±‚

**æ¶æ„å¯¹æ¯”**:

| ç»„ä»¶ | EncoderLayer | DecoderLayer |
|------|--------------|--------------|
| **å­å±‚1** | Self-Attention | Masked Self-Attention |
| **å­å±‚2** | FFN | Cross-Attention |
| **å­å±‚3** | - | FFN |
| **æ©ç ** | Padding Mask | Causal + Padding Mask |
| **è¾“å…¥** | æºåºåˆ— | ç›®æ ‡åºåˆ— + Encoder è¾“å‡º |

**ä»£ç è¡Œæ•°**: ~120 è¡Œ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from layers import EncoderLayer, DecoderLayer

# Encoder Layer
enc_layer = EncoderLayer(d_model=512, num_heads=8, d_ff=2048)
x, attn = enc_layer(x, src_mask)

# Decoder Layer
dec_layer = DecoderLayer(d_model=512, num_heads=8, d_ff=2048)
y, (self_attn, cross_attn) = dec_layer(y, memory, tgt_mask, mem_mask)
```

**é¢è¯•é‡ç‚¹**:
- Encoder å’Œ Decoder çš„ç»“æ„å·®å¼‚
- Cross-Attention çš„ Q/K/V æ¥æº
- æ¯å±‚çš„è¾“å…¥è¾“å‡ºå½¢çŠ¶

---

#### 7. `transformer.py` - å®Œæ•´ Transformer æ¨¡å‹ â­â­â­

**åŒ…å«ç±»**:
- `Transformer`: å®Œæ•´çš„ Encoder-Decoder æ¨¡å‹

**æ ¸å¿ƒæ–¹æ³•**:
- `encode()`: Encoder å‰å‘ä¼ æ’­
- `decode()`: Decoder å‰å‘ä¼ æ’­
- `forward()`: å®Œæ•´å‰å‘ä¼ æ’­ (è®­ç»ƒ)
- `greedy_decode()`: è´ªå¿ƒè§£ç  (æ¨ç†)

**æ¨¡å‹å‚æ•°**:
```python
Transformer(
    src_vocab=10000,      # æºç«¯è¯è¡¨å¤§å°
    tgt_vocab=10000,      # ç›®æ ‡ç«¯è¯è¡¨å¤§å°
    d_model=512,          # æ¨¡å‹ç»´åº¦
    num_heads=8,          # æ³¨æ„åŠ›å¤´æ•°
    d_ff=2048,            # FFN éšè—å±‚ç»´åº¦
    num_layers=6,         # Encoder/Decoder å±‚æ•°
    dropout=0.1,          # Dropout æ¦‚ç‡
    max_len=5000          # æœ€å¤§åºåˆ—é•¿åº¦
)
```

**ä»£ç è¡Œæ•°**: ~200 è¡Œ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from transformer import Transformer

model = Transformer(src_vocab=10000, tgt_vocab=10000, d_model=512)

# è®­ç»ƒ
logits = model(src, tgt_inp, src_pad, tgt_pad)

# æ¨ç†
output = model.greedy_decode(src, bos_id=1, eos_id=2, max_new_tokens=50)
```

**é¢è¯•é‡ç‚¹**:
- Encoder-Decoder çš„äº¤äº’æ–¹å¼
- è®­ç»ƒå’Œæ¨ç†çš„åŒºåˆ«
- Teacher Forcing æœºåˆ¶

---

### å•ç‹¬æ¨¡å— (æ•™å­¦ç‰ˆæœ¬)

#### 8. `ScaledDotProductAttention.py` - ç‹¬ç«‹æ³¨æ„åŠ›æ¨¡å—

**ç‰¹ç‚¹**:
- âœ… å•æ–‡ä»¶å®ç°, æ— å¤–éƒ¨ä¾èµ–
- âœ… é€‚åˆé€è¡Œè®²è§£
- âœ… é¢è¯•æ‰‹æ’•é¦–é€‰

**ä»£ç è¡Œæ•°**: ~55 è¡Œ

**ä½¿ç”¨åœºæ™¯**:
- é¢è¯•æ—¶ä»é›¶å®ç°æ³¨æ„åŠ›æœºåˆ¶
- å­¦ä¹ æ³¨æ„åŠ›çš„æ ¸å¿ƒè®¡ç®—

---

#### 9. `MultiHeadAttention.py` - ç‹¬ç«‹å¤šå¤´æ³¨æ„åŠ›

**ç‰¹ç‚¹**:
- âœ… ä¾èµ– `ScaledDotProductAttention.py`
- âœ… å®Œæ•´çš„å¤šå¤´å®ç°
- âœ… åŒ…å«åˆ†å¤´å’Œåˆå¤´é€»è¾‘

**ä»£ç è¡Œæ•°**: ~110 è¡Œ

**ä½¿ç”¨åœºæ™¯**:
- åœ¨å®ç°å•å¤´æ³¨æ„åŠ›åæ‰©å±•åˆ°å¤šå¤´
- ç†è§£å¤šå¤´çš„åˆ†å‰²å’Œåˆå¹¶

---

### æµ‹è¯•ä¸ç¤ºä¾‹

#### 10. `test_attention.py` - æ³¨æ„åŠ›æ¨¡å—æµ‹è¯•

**åŒ…å«æµ‹è¯•**:
1. âœ… ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æµ‹è¯•
2. âœ… å¤šå¤´æ³¨æ„åŠ›æµ‹è¯•
3. âœ… è‡ªæ³¨æ„åŠ›æµ‹è¯• (Q=K=V)
4. âœ… äº¤å‰æ³¨æ„åŠ›æµ‹è¯• (Qâ‰ K,V)
5. âœ… å› æœæ©ç æµ‹è¯• (Causal Mask)

**ä»£ç è¡Œæ•°**: ~250 è¡Œ

**è¿è¡Œæ–¹å¼**:
```bash
python test_attention.py
```

**è¾“å‡ºç¤ºä¾‹**:
```
============================================================
æµ‹è¯• ScaledDotProductAttention
============================================================
è¾“å…¥å½¢çŠ¶:
  Q: torch.Size([2, 4, 5, 8])
  K: torch.Size([2, 4, 6, 8])
  V: torch.Size([2, 4, 6, 8])
...
âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!
```

---

#### 11. `ATTENTION_USAGE.md` - ä½¿ç”¨æŒ‡å—

**åŒ…å«å†…å®¹**:
- âœ… å¿«é€Ÿä½¿ç”¨ç¤ºä¾‹
- âœ… 4 ä¸ªè¯¦ç»†åœºæ™¯ (è‡ªæ³¨æ„åŠ›ã€äº¤å‰æ³¨æ„åŠ›ã€æ©ç ç­‰)
- âœ… å‚æ•°è¯´æ˜è¡¨æ ¼
- âœ… ç»´åº¦å˜æ¢æµç¨‹å›¾
- âœ… å¸¸è§é—®é¢˜è§£ç­”

**é€‚ç”¨åœºæ™¯**:
- å¿«é€Ÿä¸Šæ‰‹æ³¨æ„åŠ›æ¨¡å—
- æŸ¥é˜…å‚æ•°å’Œè¿”å›å€¼
- ç†è§£ä¸åŒä½¿ç”¨åœºæ™¯

---

### å‚è€ƒæ–‡æ¡£

#### 12. `Combined.py` - åŸå§‹åˆå¹¶ç‰ˆæœ¬

**è¯´æ˜**: 
- âš ï¸ å·²å¼ƒç”¨, ä»…ä¾›å‚è€ƒ
- âŒ åŒ…å«å¤šä¸ªé”™è¯¯ (å·²åœ¨å…¶ä»–æ–‡ä»¶ä¸­ä¿®æ­£)
- â„¹ï¸ å¯ç”¨äºå¯¹æ¯”æ¨¡å—åŒ–å‰åçš„å·®å¼‚

**å·²ä¿®æ­£çš„é”™è¯¯** (16 å¤„):
1. `nn.Moduel` â†’ `nn.Module`
2. `super.__init__()` â†’ `super().__init__()`
3. `self.W_k(x_kv)` â†’ `self.W_v(x_kv)`
4. ... (è§ä¸‹æ–‡å®Œæ•´åˆ—è¡¨)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: ä½¿ç”¨å®Œæ•´æ¨¡å‹ (æ¨è)

#### å®‰è£…ä¾èµ–

```bash
pip install torch
```

#### å¯¼å…¥å¹¶ä½¿ç”¨

```python
from transformer import Transformer
import torch

# åˆ›å»ºæ¨¡å‹
model = Transformer(
    src_vocab=10000,
    tgt_vocab=10000,
    d_model=512,
    num_heads=8,
    d_ff=2048,
    num_layers=6,
    dropout=0.1
)

# å‡†å¤‡æ•°æ®
B, T_s, T_t = 32, 20, 25
src = torch.randint(0, 10000, (B, T_s))      # æºåºåˆ—
tgt_inp = torch.randint(0, 10000, (B, T_t))  # ç›®æ ‡åºåˆ—è¾“å…¥
tgt_out = torch.randint(0, 10000, (B, T_t))  # ç›®æ ‡åºåˆ—è¾“å‡º (ç”¨äºè®¡ç®—æŸå¤±)

# è®­ç»ƒæ¨¡å¼
model.train()
logits = model(src, tgt_inp)  # (B, T_t, tgt_vocab)

# è®¡ç®—æŸå¤±
criterion = torch.nn.CrossEntropyLoss(ignore_index=0)
loss = criterion(logits.view(-1, 10000), tgt_out.view(-1))
print(f"Loss: {loss.item():.4f}")

# æ¨ç†æ¨¡å¼
model.eval()
output = model.greedy_decode(src, bos_id=1, eos_id=2, max_new_tokens=30)
print(f"Generated output shape: {output.shape}")
```

---

### æ–¹å¼ 2: ä½¿ç”¨å•ä¸ªç»„ä»¶

#### åªä½¿ç”¨æ³¨æ„åŠ›æ¨¡å—

```python
from attention import MultiHeadAttention
import torch

# åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›
mha = MultiHeadAttention(d_model=64, num_heads=8, dropout=0.1)

# è‡ªæ³¨æ„åŠ›
x = torch.randn(2, 10, 64)  # (B, T, d_model)
out, attn_weights = mha(x, x)

print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")           # (2, 10, 64)
print(f"æ³¨æ„åŠ›æƒé‡: {attn_weights.shape}")  # (2, 8, 10, 10)
```

#### ç»„åˆå¤šä¸ªç»„ä»¶

```python
from attention import MultiHeadAttention
from feedforward import FeedForward, ResidualLayerNorm
from embedding import PositionalEncoding
import torch

# åˆ›å»ºç»„ä»¶
pos_enc = PositionalEncoding(d_model=64, max_len=100)
mha = MultiHeadAttention(d_model=64, num_heads=8)
ffn = FeedForward(d_model=64, d_ff=256)
norm1 = ResidualLayerNorm(d_model=64)
norm2 = ResidualLayerNorm(d_model=64)

# æ¨¡æ‹Ÿä¸€ä¸ª Encoder Layer
x = torch.randn(2, 10, 64)
x = pos_enc(x)

# Self-Attention + Residual + Norm
attn_out, _ = mha(x, x)
x = norm1(x, attn_out)

# FFN + Residual + Norm
ffn_out = ffn(x)
x = norm2(x, ffn_out)

print(f"æœ€ç»ˆè¾“å‡º: {x.shape}")  # (2, 10, 64)
```

---

### æ–¹å¼ 3: ä½¿ç”¨æ•™å­¦ç‰ˆæœ¬ (å•æ–‡ä»¶)

```python
from ScaledDotProductAttention import ScaledDotProductAttention
from MultiHeadAttention import MultiHeadAttention
import torch

# 1. æµ‹è¯•ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
attn = ScaledDotProductAttention(dropout=0.1)
Q = torch.randn(2, 4, 5, 8)  # (B, H, T_q, d_k)
K = torch.randn(2, 4, 6, 8)  # (B, H, T_k, d_k)
V = torch.randn(2, 4, 6, 8)  # (B, H, T_k, d_v)
out, attn_weights = attn(Q, K, V)
print(f"Attention è¾“å‡º: {out.shape}")

# 2. æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
mha = MultiHeadAttention(d_model=64, num_heads=8)
x = torch.randn(2, 10, 64)
out, attn_weights = mha(x, x)
print(f"MHA è¾“å‡º: {out.shape}")
```

---

### è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•æ³¨æ„åŠ›æ¨¡å—
python test_attention.py

# é¢„æœŸè¾“å‡º:
# ============================================================
# æµ‹è¯• ScaledDotProductAttention
# ============================================================
# è¾“å…¥å½¢çŠ¶:
#   Q: torch.Size([2, 4, 5, 8])
#   ...
# âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!
```

---

## ğŸ“Š æ¨¡å‹æ¶æ„ä¸å‚æ•°é‡

### Transformer æ•´ä½“æ¶æ„

```
è¾“å…¥åºåˆ— (src)
    â†“
Token Embedding + Positional Encoding
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Encoder (N å±‚å †å )              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Multi-Head Self-Attention     â”‚  â”‚
â”‚  â”‚         â†“                     â”‚  â”‚
â”‚  â”‚ Add & Norm                    â”‚  â”‚
â”‚  â”‚         â†“                     â”‚  â”‚
â”‚  â”‚ Feed Forward Network          â”‚  â”‚
â”‚  â”‚         â†“                     â”‚  â”‚
â”‚  â”‚ Add & Norm                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         (é‡å¤ N æ¬¡)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Encoder Output (Memory)
    â†“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                          â”‚
ç›®æ ‡åºåˆ— (tgt)                  â”‚
    â†“                          â”‚
Token Embedding + Pos Encoding  â”‚
    â†“                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Decoder (N å±‚å †å )              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Masked Multi-Head Self-Attn   â”‚  â”‚ (é˜²æ­¢çœ‹åˆ°æœªæ¥)
â”‚  â”‚         â†“                     â”‚  â”‚
â”‚  â”‚ Add & Norm                    â”‚  â”‚
â”‚  â”‚         â†“                     â”‚  â”‚
â”‚  â”‚ Multi-Head Cross-Attention â†â”€â”€â”¼â”€â”€â”˜ (æŸ¥è¯¢ Encoder)
â”‚  â”‚         â†“                     â”‚  â”‚
â”‚  â”‚ Add & Norm                    â”‚  â”‚
â”‚  â”‚         â†“                     â”‚  â”‚
â”‚  â”‚ Feed Forward Network          â”‚  â”‚
â”‚  â”‚         â†“                     â”‚  â”‚
â”‚  â”‚ Add & Norm                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         (é‡å¤ N æ¬¡)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Linear (æŠ•å½±åˆ°è¯è¡¨)
    â†“
Softmax
    â†“
è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
```

---

### å‚æ•°é‡è®¡ç®—

ä»¥ BERT-Base é…ç½®ä¸ºä¾‹: $d_{\text{model}}=768$, $H=12$, $N=12$, $V=30000$

| ç»„ä»¶ | å‚æ•°é‡å…¬å¼ | å…·ä½“æ•°å€¼ | å æ¯” |
|------|-----------|---------|------|
| **Token Embedding** | $V \times d$ | $30000 \times 768 = 23.04\text{M}$ | ~21% |
| **Position Encoding** | 0 | 0 | 0% |
| **MHA** (å•å±‚) | $4d^2$ | $4 \times 768^2 = 2.36\text{M}$ | - |
| **FFN** (å•å±‚) | $8d^2$ | $8 \times 768^2 = 4.72\text{M}$ | - |
| **LayerNorm** (å•å±‚Ã—2) | $4d$ | $4 \times 768 = 3072$ | ~0% |
| **å•å±‚ Encoder** | $12d^2 + 4d$ | $\approx 7.08\text{M}$ | - |
| **N å±‚ Encoder** | $N(12d^2 + 4d)$ | $12 \times 7.08\text{M} = 84.96\text{M}$ | ~78% |
| **è¾“å‡ºæŠ•å½±** | $d \times V$ | $768 \times 30000 = 23.04\text{M}$ | - |
| **æ€»è®¡** | $\approx 2Vd + 12Nd^2$ | $\approx 110\text{M}$ | 100% |

**è¯´æ˜**:
- å®é™… BERT-Base çº¦ 110M å‚æ•° (ä¸è®¡ç®—ç›¸ç¬¦)
- GPT-2 (117M): $d=768$, $N=12$, $V=50257$
- GPT-3 (175B): $d=12288$, $N=96$, $V=50257$

---

### å…¸å‹é…ç½®å¯¹æ¯”

| æ¨¡å‹ | $d_{\text{model}}$ | $H$ | $N$ | $d_{\text{ff}}$ | å‚æ•°é‡ |
|------|-------------------|-----|-----|----------------|--------|
| **Transformer-Base** | 512 | 8 | 6 | 2048 | ~65M |
| **Transformer-Big** | 1024 | 16 | 6 | 4096 | ~213M |
| **BERT-Base** | 768 | 12 | 12 | 3072 | ~110M |
| **BERT-Large** | 1024 | 16 | 24 | 4096 | ~340M |
| **GPT-2** | 768 | 12 | 12 | 3072 | ~117M |
| **GPT-2 Large** | 1280 | 20 | 36 | 5120 | ~774M |
| **GPT-3** | 12288 | 96 | 96 | 49152 | ~175B |

---

### è®¡ç®—å¤æ‚åº¦åˆ†æ

å¯¹äºå•å±‚ Transformer, åºåˆ—é•¿åº¦ $T$, æ¨¡å‹ç»´åº¦ $d$:

| æ“ä½œ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | è¯´æ˜ |
|------|-----------|-----------|------|
| **Self-Attention** | $\mathcal{O}(T^2 \cdot d)$ | $\mathcal{O}(T^2)$ | ç“¶é¢ˆ: æ³¨æ„åŠ›çŸ©é˜µ |
| **FFN** | $\mathcal{O}(T \cdot d^2)$ | $\mathcal{O}(T \cdot d)$ | å½“ $T>d$ æ—¶è¾ƒå° |
| **å•å±‚æ€»è®¡** | $\mathcal{O}(T^2 \cdot d + T \cdot d^2)$ | $\mathcal{O}(T^2 + T \cdot d)$ | - |
| **N å±‚æ€»è®¡** | $\mathcal{O}(N(T^2 \cdot d + T \cdot d^2))$ | $\mathcal{O}(NT^2)$ | è®­ç»ƒæ—¶éœ€å­˜å‚¨æ¢¯åº¦ |

**å…³é”®å½±å“å› ç´ **:
- **åºåˆ—é•¿åº¦ $T$**: äºŒæ¬¡å¢é•¿ $\mathcal{O}(T^2)$ ğŸ”¥ğŸ”¥ğŸ”¥
- **æ¨¡å‹ç»´åº¦ $d$**: å¹³æ–¹å¢é•¿ $\mathcal{O}(d^2)$ ğŸ”¥
- **å±‚æ•° $N$**: çº¿æ€§å¢é•¿ $\mathcal{O}(N)$ âœ…

è¯¦è§æ•™ç¨‹ä¸­çš„"é¢è¯•é«˜é¢‘é—®é¢˜è¯¦è§£"ã€‚

---

## ğŸ”§ ä»£ç ä¿®æ­£è¯´æ˜

åŸå§‹ `Combined.py` æ–‡ä»¶ä¸­å­˜åœ¨çš„é”™è¯¯å·²å…¨éƒ¨ä¿®æ­£:

1. âœ… `nn.Moduel` â†’ `nn.Module` (æ‹¼å†™é”™è¯¯)
2. âœ… `super.__init__()` â†’ `super().__init__()` (è¯­æ³•é”™è¯¯)
3. âœ… `self.W_k(x_kv)` â†’ `self.W_v(x_kv)` (é€»è¾‘é”™è¯¯, V æŠ•å½±é”™è¯¯ä½¿ç”¨äº† K)
4. âœ… `torch.arange(0, d_model, 2)).float()` â†’ `torch.arange(0, d_model, 2).float()` (æ‹¬å·é”™è¯¯)
5. âœ… `self.act = nn.ReLU()` â†’ `self.act = nn.GELU()` (GELU åˆ†æ”¯é”™è¯¯)
6. âœ… `d_modle` â†’ `d_model` (æ‹¼å†™é”™è¯¯)
7. âœ… `q_pad is None` åˆ¤æ–­é€»è¾‘ä¼˜åŒ–
8. âœ… `__ini__` â†’ `__init__` (æ‹¼å†™é”™è¯¯)
9. âœ… `super.__init__()` â†’ `super().__init__()` (å¤šå¤„)
10. âœ… `self.tgt_vocab` â†’ `self.tgt_embed` (å˜é‡åé”™è¯¯)
11. âœ… `self.encode_layers` â†’ `self.encoder_layers` (ä¸€è‡´æ€§)
12. âœ… `self.decode_layers` â†’ `self.decoder_layers` (ä¸€è‡´æ€§)
13. âœ… `forwad` â†’ `forward` (æ‹¼å†™é”™è¯¯)
14. âœ… `torch.log` â†’ `torch.long` (ç±»å‹é”™è¯¯)
15. âœ… `tgt_vocab=None` â†’ `tgt_pad=None` (å‚æ•°åé”™è¯¯)
16. âœ… ç¼©è¿›é”™è¯¯ä¿®æ­£ (å¤šä¸ªæ–¹æ³•å®šä¹‰åœ¨ç±»å¤–)

---

## ğŸ¯ ä½¿ç”¨å»ºè®®

### é¢è¯•æ‰‹æ’•å»ºè®®

1. **ä¼˜å…ˆçº§æ’åº**:
   - å¿…é¡»ä¼š: `ScaledDotProductAttention`, `MultiHeadAttention`
   - é‡è¦: `EncoderLayer`, `DecoderLayer`
   - æ¬¡è¦: `PositionalEncoding`, `FeedForward`
   - å¯ç®€åŒ–: Mask å‡½æ•°, å®Œæ•´ Transformer ç±»

2. **ç®€åŒ–æŠ€å·§**:
   - åªå®ç° Encoder æˆ– Decoder (ä¸å¿…ä¸¤è€…éƒ½å†™)
   - å¿½ç•¥ Dropout
   - å‡è®¾æ²¡æœ‰ Padding (ç®€åŒ– Mask)
   - ä½¿ç”¨ä¼ªä»£ç æè¿°å¤æ‚éƒ¨åˆ†

3. **å…³é”®ç‚¹å¼ºè°ƒ**:
   - Attention å…¬å¼: $\frac{QK^T}{\sqrt{d_k}}$
   - å¤šå¤´åˆ†å‰²: `(B,T,d) â†’ (B,H,T,d_k)`
   - Mask æœºåˆ¶: Padding + Causal
   - æ®‹å·®è¿æ¥ + LayerNorm

---

## ğŸ“– å‚è€ƒèµ„æ–™

- åŸè®ºæ–‡: [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- The Annotated Transformer: http://nlp.seas.harvard.edu/annotated-transformer/
- PyTorch å®˜æ–¹æ–‡æ¡£: https://pytorch.org/docs/stable/nn.html#transformer-layers

---

## ğŸ“ TODO

- [ ] æ·»åŠ è®­ç»ƒè„šæœ¬ç¤ºä¾‹
- [ ] æ·»åŠ  Beam Search è§£ç 
- [ ] æ·»åŠ æ¨¡å‹å¯è§†åŒ–å·¥å…·
- [ ] æ·»åŠ å•å…ƒæµ‹è¯•
- [ ] æ”¯æŒ FlashAttention
- [ ] æ”¯æŒ Pre-LN ç‰ˆæœ¬

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ9æ—¥
