# Transformer å­¦ä¹ èµ„æº

æœ¬ç›®å½•åŒ…å« Transformer æ¨¡å‹çš„å®Œæ•´å­¦ä¹ èµ„æ–™ï¼ŒåŒ…æ‹¬äº¤äº’å¼æ•™ç¨‹å’Œæ¨¡å—åŒ–ä»£ç å®ç°ã€‚

---

## ğŸ“‚ ç›®å½•ç»“æ„

```
Transformer/
â”œâ”€â”€ tutorial.ipynb          # äº¤äº’å¼æ•™ç¨‹ (åŒ…å«é¢è¯•é«˜é¢‘é—®é¢˜è¯¦è§£)
â””â”€â”€ Code/                   # æ¨¡å—åŒ–ä»£ç å®ç°
    â”œâ”€â”€ README.md           # è¯¦ç»†ä»£ç æ–‡æ¡£ (763è¡Œ)
    â”œâ”€â”€ attention.py        # æ³¨æ„åŠ›æœºåˆ¶
    â”œâ”€â”€ embedding.py        # ä½ç½®ç¼–ç 
    â”œâ”€â”€ feedforward.py      # å‰é¦ˆç½‘ç»œ
    â”œâ”€â”€ mask.py            # æ©ç å·¥å…·
    â”œâ”€â”€ layers.py          # Encoder/Decoderå±‚
    â”œâ”€â”€ transformer.py     # å®Œæ•´æ¨¡å‹
    â””â”€â”€ test_attention.py  # æµ‹è¯•å¥—ä»¶
```

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### 1. äº¤äº’å¼å­¦ä¹  (æ¨èæ–°æ‰‹)

æ‰“å¼€ `tutorial.ipynb` Jupyter Notebookï¼š
- ğŸ“– Transformer åŸºç¡€æ¦‚å¿µ
- ğŸ” é¢è¯•é«˜é¢‘é—®é¢˜è¯¦è§£ (å¤æ‚åº¦ã€Embeddingã€åºåˆ—é•¿åº¦ç­‰)
- ğŸ’¡ å¸¦å…¬å¼å’Œå¯è§†åŒ–çš„è¯¦ç»†è§£é‡Š

### 2. ä»£ç å®ç°å­¦ä¹ 

è¿›å…¥ `Code/` ç›®å½•ï¼ŒæŸ¥çœ‹æ¨¡å—åŒ–å®ç°ï¼š

```python
# ä½¿ç”¨å®Œæ•´æ¨¡å‹
from Code.transformer import Transformer

model = Transformer(
    src_vocab_size=10000,
    tgt_vocab_size=10000,
    d_model=512,
    num_heads=8,
    num_encoder_layers=6,
    num_decoder_layers=6
)
```

è¯¦è§ `Code/README.md` è·å–å®Œæ•´ä½¿ç”¨æŒ‡å—ã€‚

---

## ğŸ“š å­¦ä¹ è·¯å¾„

1. **å…¥é—¨** â†’ é˜…è¯» `tutorial.ipynb` ç†è§£æ¦‚å¿µ
2. **æ·±å…¥** â†’ å­¦ä¹  `Code/attention.py` å’Œ `Code/embedding.py`
3. **å®è·µ** â†’ è¿è¡Œ `Code/test_attention.py` æµ‹è¯•
4. **é¢è¯•** â†’ å¤ä¹  `tutorial.ipynb` ä¸­çš„é¢è¯•é—®é¢˜

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ11æ—¥
