# Chapter 3: æ³¨æ„åŠ›æœºåˆ¶

æœ¬ç« æ·±å…¥æ¢è®¨æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanisms)ï¼Œè¿™æ˜¯ Transformer å’Œç°ä»£ LLM çš„æ ¸å¿ƒç»„ä»¶ã€‚

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

```
chap3-attention_mechanisms/
â”œâ”€â”€ self-attention.py          # è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°
â”œâ”€â”€ multihead-attention.py     # å¤šå¤´æ³¨æ„åŠ›å®ç°
â”œâ”€â”€ masked-attention.py        # å› æœæ©ç æ³¨æ„åŠ› (Causal Attention)
â”œâ”€â”€ dropout.py                 # Dropout æ­£åˆ™åŒ–
â””â”€â”€ saved-code-3.py            # æœ¬ç« å®Œæ•´ä»£ç 
```

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- âœ… ç†è§£è‡ªæ³¨æ„åŠ› (Self-Attention) åŸç†
- âœ… å®ç°å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)
- âœ… æŒæ¡å› æœæ©ç  (Causal Masking)
- âœ… åº”ç”¨ Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è‡ªæ³¨æ„åŠ›

```bash
python self-attention.py
```

**æ ¸å¿ƒå…¬å¼**:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### 2. å¤šå¤´æ³¨æ„åŠ›

```bash
python multihead-attention.py
```

**ç‰¹æ€§**:
- å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´
- æ•è·ä¸åŒä½ç½®å…³ç³»
- å¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›

### 3. å› æœæ©ç æ³¨æ„åŠ›

```bash
python masked-attention.py
```

**ç”¨é€”**:
- é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯
- GPT è§£ç å™¨çš„å…³é”®ç»„ä»¶
- è‡ªå›å½’ç”Ÿæˆ

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### Self-Attention (è‡ªæ³¨æ„åŠ›)

å°†è¾“å…¥åºåˆ—çš„æ¯ä¸ªä½ç½®ä¸æ‰€æœ‰ä½ç½®è¿›è¡Œäº¤äº’ï¼Œè®¡ç®—åŠ æƒå’Œã€‚

**æ­¥éª¤**:
1. è®¡ç®— Q (Query), K (Key), V (Value)
2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: $QK^T / \sqrt{d_k}$
3. Softmax å½’ä¸€åŒ–
4. åŠ æƒæ±‚å’Œ: Attention Ã— V

### Multi-Head Attention (å¤šå¤´æ³¨æ„åŠ›)

å¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ•è·ä¸åŒçš„ç‰¹å¾ã€‚

$$\text{MultiHead}(Q,K,V) = \text{Concat}(head_1, ..., head_h)W^O$$

å…¶ä¸­:
$$head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

### Causal Masking (å› æœæ©ç )

åœ¨è§£ç æ—¶é˜²æ­¢ä½ç½® $i$ çœ‹åˆ°ä½ç½® $j > i$ çš„ä¿¡æ¯ã€‚

```python
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
scores.masked_fill_(mask, float('-inf'))
```

---

## ğŸ’¡ ä»£ç ç¤ºä¾‹

```python
import torch
from multihead_attention import MultiHeadAttention

# åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›
mha = MultiHeadAttention(
    d_model=512,    # æ¨¡å‹ç»´åº¦
    num_heads=8,    # æ³¨æ„åŠ›å¤´æ•°
    dropout=0.1
)

# è¾“å…¥
x = torch.randn(2, 10, 512)  # (batch, seq_len, d_model)

# å‰å‘ä¼ æ’­
output, attention_weights = mha(x, x, x)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")           # (2, 10, 512)
print(f"æ³¨æ„åŠ›æƒé‡: {attention_weights.shape}") # (2, 8, 10, 10)
```

---

## ğŸ” æ³¨æ„åŠ›å¯è§†åŒ–

æ³¨æ„åŠ›æƒé‡çŸ©é˜µæ˜¾ç¤ºæ¯ä¸ªä½ç½®å…³æ³¨å…¶ä»–ä½ç½®çš„ç¨‹åº¦ï¼š

```
        Token1  Token2  Token3  Token4
Token1  [0.4    0.3     0.2     0.1  ]
Token2  [0.2    0.5     0.2     0.1  ]
Token3  [0.1    0.2     0.6     0.1  ]
Token4  [0.1    0.1     0.2     0.6  ]
```

---

## ğŸ”— ç›¸å…³ç« èŠ‚

- **ä¸Šä¸€ç« **: [Chapter 2 - æ–‡æœ¬æ•°æ®å¤„ç†](../chap2-work_with_text_data/)
- **ä¸‹ä¸€ç« **: [Chapter 4 - å®ç° GPT æ¨¡å‹](../chap4-implement_gpt_model/)

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ17æ—¥
