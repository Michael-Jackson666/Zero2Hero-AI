# Chapter 6: åˆ†ç±»ä»»åŠ¡å¾®è°ƒ

æœ¬ç« å±•ç¤ºå¦‚ä½•å°†é¢„è®­ç»ƒçš„ GPT æ¨¡å‹å¾®è°ƒç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆä»¥åƒåœ¾é‚®ä»¶åˆ†ç±»ä¸ºä¾‹ï¼‰ã€‚

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

```
chap6-fine-tuning-for-classification/
â”œâ”€â”€ fine-tuning.py                      # ä¸»å¾®è°ƒè„šæœ¬
â”œâ”€â”€ fine-tuning-all.py                  # å®Œæ•´å¾®è°ƒæµç¨‹
â”œâ”€â”€ gpt_class_finetune.py               # GPT åˆ†ç±»å™¨å¾®è°ƒ
â”œâ”€â”€ add_classification_head.py          # æ·»åŠ åˆ†ç±»å¤´
â”œâ”€â”€ create_data_loaders.py              # æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ prepare_dataset.py                  # æ•°æ®é›†å‡†å¤‡
â”œâ”€â”€ load_weights.py                     # åŠ è½½é¢„è®­ç»ƒæƒé‡
â”œâ”€â”€ gpt_download.py                     # ä¸‹è½½ GPT-2 æ¨¡å‹
â”œâ”€â”€ spam_classifier.py                  # åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨
â”œâ”€â”€ spam_classifier_simple.py           # ç®€åŒ–ç‰ˆåˆ†ç±»å™¨
â”œâ”€â”€ spam_classifier_inference.py        # æ¨ç†è„šæœ¬
â”œâ”€â”€ simple_spam_classifier.py           # åŸºç¡€åˆ†ç±»å™¨å®ç°
â”œâ”€â”€ previous_chapters.py                # å‰å‡ ç« ä»£ç 
â”œâ”€â”€ spam_classifier_full_finetune.pth   # å¾®è°ƒåæ¨¡å‹
â”œâ”€â”€ train.csv                           # è®­ç»ƒé›†
â”œâ”€â”€ validation.csv                      # éªŒè¯é›†
â”œâ”€â”€ test.csv                            # æµ‹è¯•é›†
â”œâ”€â”€ training_history_full.pkl           # è®­ç»ƒå†å²
â”œâ”€â”€ loss-plot.pdf                       # æŸå¤±æ›²çº¿
â”œâ”€â”€ accuracy-plot.pdf                   # å‡†ç¡®ç‡æ›²çº¿
â”œâ”€â”€ sms_spam_collection/                # SMS åƒåœ¾é‚®ä»¶æ•°æ®é›†
â””â”€â”€ gpt2/                               # GPT-2 é¢„è®­ç»ƒæ¨¡å‹
```

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- âœ… ç†è§£è¿ç§»å­¦ä¹ å’Œå¾®è°ƒ
- âœ… ä¸º GPT æ·»åŠ åˆ†ç±»å¤´
- âœ… æŒæ¡å¾®è°ƒç­–ç•¥ï¼ˆå…¨å‚æ•° vs éƒ¨åˆ†å‚æ•°ï¼‰
- âœ… å®ç°æ–‡æœ¬åˆ†ç±»æµç¨‹
- âœ… è¯„ä¼°åˆ†ç±»æ¨¡å‹æ€§èƒ½

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ•°æ®é›†

```bash
# ä¸‹è½½å¹¶å‡†å¤‡ SMS åƒåœ¾é‚®ä»¶æ•°æ®é›†
python prepare_dataset.py
```

**æ•°æ®æ ¼å¼**:
```
Label,Text
spam,"Congratulations! You've won a $1000 prize..."
ham,"Hey, are we still meeting for lunch?"
```

### 2. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹

```bash
python gpt_download.py
```

ä¸‹è½½ GPT-2 (124M) é¢„è®­ç»ƒæƒé‡ã€‚

### 3. å¾®è°ƒæ¨¡å‹

```bash
# å®Œæ•´å¾®è°ƒï¼ˆæ‰€æœ‰å‚æ•°ï¼‰
python fine-tuning-all.py

# æˆ–åªå¾®è°ƒæœ€åå‡ å±‚
python fine-tuning.py --freeze_layers 8
```

### 4. æ¨ç†é¢„æµ‹

```bash
python spam_classifier_inference.py
```

---

## ğŸ—ï¸ åˆ†ç±»å™¨æ¶æ„

```
Input Text
    â†“
GPT-2 Encoder (é¢„è®­ç»ƒ)
    â†“
Last Token Representation
    â†“
Classification Head
    â”œâ”€â”€ Linear(768 â†’ 768)
    â”œâ”€â”€ GELU
    â”œâ”€â”€ Dropout
    â””â”€â”€ Linear(768 â†’ 2)
    â†“
Softmax
    â†“
[Spam, Ham] Probabilities
```

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### è¿ç§»å­¦ä¹ 

1. **é¢„è®­ç»ƒ**: åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šå­¦ä¹ é€šç”¨è¯­è¨€è¡¨ç¤º
2. **å¾®è°ƒ**: åœ¨ç‰¹å®šä»»åŠ¡æ•°æ®ä¸Šè°ƒæ•´å‚æ•°

ä¼˜åŠ¿:
- éœ€è¦æ›´å°‘çš„æ ‡æ³¨æ•°æ®
- è®­ç»ƒé€Ÿåº¦æ›´å¿«
- æ€§èƒ½é€šå¸¸æ›´å¥½

### æ·»åŠ åˆ†ç±»å¤´

```python
class GPTClassifier(nn.Module):
    def __init__(self, gpt_model, num_classes=2):
        super().__init__()
        self.gpt = gpt_model
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(768, 768),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(768, num_classes)
        )
    
    def forward(self, input_ids):
        # GPT ç¼–ç 
        output = self.gpt(input_ids)  # (batch, seq_len, 768)
        
        # å–æœ€åä¸€ä¸ª token çš„è¡¨ç¤º
        last_token = output[:, -1, :]  # (batch, 768)
        
        # åˆ†ç±»
        logits = self.classifier(last_token)  # (batch, 2)
        return logits
```

### å¾®è°ƒç­–ç•¥

#### 1. å…¨å‚æ•°å¾®è°ƒ
å¾®è°ƒæ‰€æœ‰å‚æ•°ï¼ˆåŒ…æ‹¬ GPT ä¸»å¹²ï¼‰ã€‚

```python
# æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ
for param in model.parameters():
    param.requires_grad = True
```

#### 2. éƒ¨åˆ†å¾®è°ƒ
å†»ç»“å‰ N å±‚ï¼Œåªå¾®è°ƒåé¢çš„å±‚ã€‚

```python
# å†»ç»“å‰ 8 å±‚
for i in range(8):
    for param in model.gpt.layers[i].parameters():
        param.requires_grad = False
```

#### 3. ä»…è®­ç»ƒåˆ†ç±»å¤´
å†»ç»“æ•´ä¸ª GPTï¼Œåªè®­ç»ƒæ–°åŠ çš„åˆ†ç±»å¤´ã€‚

```python
# å†»ç»“ GPT
for param in model.gpt.parameters():
    param.requires_grad = False
    
# åˆ†ç±»å¤´å‚æ•°å¯è®­ç»ƒ
for param in model.classifier.parameters():
    param.requires_grad = True
```

---

## ğŸ’¡ å®Œæ•´è®­ç»ƒç¤ºä¾‹

```python
from gpt_class_finetune import GPTClassifier
from create_data_loaders import create_dataloaders

# åŠ è½½é¢„è®­ç»ƒ GPT-2
gpt_model = load_pretrained_gpt()

# åˆ›å»ºåˆ†ç±»å™¨
classifier = GPTClassifier(gpt_model, num_classes=2)

# å‡†å¤‡æ•°æ®
train_loader, val_loader, test_loader = create_dataloaders(
    train_csv='train.csv',
    val_csv='validation.csv', 
    test_csv='test.csv',
    batch_size=8
)

# è®­ç»ƒ
optimizer = torch.optim.AdamW(classifier.parameters(), lr=5e-5)
criterion = nn.CrossEntropyLoss()

for epoch in range(5):
    # è®­ç»ƒå¾ªç¯
    classifier.train()
    for input_ids, labels in train_loader:
        logits = classifier(input_ids)
        loss = criterion(logits, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # éªŒè¯
    classifier.eval()
    accuracy = evaluate(classifier, val_loader)
    print(f"Epoch {epoch}: Val Accuracy = {accuracy:.2%}")

# ä¿å­˜æ¨¡å‹
torch.save(classifier.state_dict(), 'spam_classifier.pth')
```

---

## ğŸ“Š æ€§èƒ½è¯„ä¼°

### è¯„ä¼°æŒ‡æ ‡

```bash
python spam_classifier_inference.py --evaluate
```

è¾“å‡º:
```
Accuracy:  98.5%
Precision: 97.2%
Recall:    96.8%
F1-Score:  97.0%

Confusion Matrix:
              Predicted
            Ham    Spam
Actual Ham   892     8
     Spam     12    88
```

### å¯è§†åŒ–

æŸ¥çœ‹è®­ç»ƒæ›²çº¿:
- `loss-plot.pdf`: è®­ç»ƒ/éªŒè¯æŸå¤±
- `accuracy-plot.pdf`: è®­ç»ƒ/éªŒè¯å‡†ç¡®ç‡

---

## ğŸ” æ¨ç†ç¤ºä¾‹

```python
from spam_classifier_inference import SpamClassifier

# åŠ è½½æ¨¡å‹
classifier = SpamClassifier('spam_classifier_full_finetune.pth')

# é¢„æµ‹
text = "Congratulations! You've won a free iPhone!"
result = classifier.predict(text)

print(f"Text: {text}")
print(f"Prediction: {result['label']}")  # "spam"
print(f"Confidence: {result['confidence']:.2%}")  # 98.5%
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ•°æ®å‡†å¤‡
- å¹³è¡¡æ•°æ®é›†ï¼ˆspam/ham æ¯”ä¾‹ï¼‰
- æ¸…æ´—æ–‡æœ¬ï¼ˆå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
- åˆç†åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†

### 2. è¶…å‚æ•°è°ƒæ•´
- å­¦ä¹ ç‡: `1e-5` ~ `5e-5`
- Batch Size: `4` ~ `16`
- Epochs: `3` ~ `10`
- Warmup Steps: `10%` æ€»æ­¥æ•°

### 3. é˜²æ­¢è¿‡æ‹Ÿåˆ
- ä½¿ç”¨ Dropout (0.1 ~ 0.3)
- Early Stopping
- æƒé‡è¡°å‡ (Weight Decay)
- æ•°æ®å¢å¼º

### 4. è¯„ä¼°
- ä¸åªçœ‹å‡†ç¡®ç‡ï¼Œå…³æ³¨ F1-Score
- åˆ†ææ··æ·†çŸ©é˜µ
- æµ‹è¯•è¾¹ç•Œæ ·æœ¬

---

## ğŸ”— ç›¸å…³ç« èŠ‚

- **ä¸Šä¸€ç« **: [Chapter 5 - é¢„è®­ç»ƒ](../chap5-pretraining/)
- **ä¸‹ä¸€ç« **: [Chapter 7 - æŒ‡ä»¤å¾®è°ƒ](../chap7-fine-tuning-to-follow-instruction/)

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ17æ—¥
