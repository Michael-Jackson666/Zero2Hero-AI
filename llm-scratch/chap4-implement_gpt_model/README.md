# Chapter 4: å®ç° GPT æ¨¡å‹

æœ¬ç« ä»é›¶å¼€å§‹å®ç°å®Œæ•´çš„ GPT (Generative Pre-trained Transformer) æ¨¡å‹ã€‚

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

```
chap4-implement_gpt_model/
â”œâ”€â”€ gpt-model.py                  # å®Œæ•´ GPT æ¨¡å‹å®ç°
â”œâ”€â”€ gpt.py                        # GPT æ¶æ„ä¸»æ–‡ä»¶
â”œâ”€â”€ transformer-block.py          # Transformer å—å®ç°
â”œâ”€â”€ feed-forward.py               # å‰é¦ˆç½‘ç»œ (FFN)
â”œâ”€â”€ layer-normlization.py         # Layer Normalization
â”œâ”€â”€ add-shortcut-connection.py    # æ®‹å·®è¿æ¥
â”œâ”€â”€ generating-text.py            # æ–‡æœ¬ç”Ÿæˆå‡½æ•°
â”œâ”€â”€ DummyGPTModel.py             # ç®€åŒ–ç‰ˆ GPT (æ•™å­¦ç”¨)
â””â”€â”€ saved-code-4.py              # æœ¬ç« å®Œæ•´ä»£ç 
```

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- âœ… ç†è§£ GPT æ¶æ„ç»„æˆ
- âœ… å®ç° Transformer Block
- âœ… æŒæ¡æ®‹å·®è¿æ¥å’Œ LayerNorm
- âœ… å®ç°æ–‡æœ¬ç”Ÿæˆé€»è¾‘

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ„å»º GPT æ¨¡å‹

```bash
python gpt-model.py
```

**æ¨¡å‹é…ç½®**:
```python
GPT_CONFIG = {
    "vocab_size": 50257,      # è¯æ±‡è¡¨å¤§å°
    "context_length": 1024,   # ä¸Šä¸‹æ–‡é•¿åº¦
    "emb_dim": 768,          # åµŒå…¥ç»´åº¦
    "n_heads": 12,           # æ³¨æ„åŠ›å¤´æ•°
    "n_layers": 12,          # Transformer å±‚æ•°
    "drop_rate": 0.1,        # Dropout æ¯”ç‡
    "qkv_bias": False        # QKV æ˜¯å¦ä½¿ç”¨åç½®
}
```

### 2. æ–‡æœ¬ç”Ÿæˆ

```bash
python generating-text.py
```

**ç”Ÿæˆæ–¹æ³•**:
- Greedy Decoding (è´ªå©ªè§£ç )
- Temperature Sampling (æ¸©åº¦é‡‡æ ·)
- Top-k Sampling (Top-k é‡‡æ ·)
- Top-p (Nucleus) Sampling

---

## ğŸ—ï¸ GPT æ¶æ„

```
Input Text
    â†“
Token Embedding + Positional Embedding
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block 1            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Multi-Head Self-Attention â”‚  â”‚
â”‚  â”‚         â†“                 â”‚  â”‚
â”‚  â”‚ Add & LayerNorm          â”‚  â”‚
â”‚  â”‚         â†“                 â”‚  â”‚
â”‚  â”‚ Feed Forward Network     â”‚  â”‚
â”‚  â”‚         â†“                 â”‚  â”‚
â”‚  â”‚ Add & LayerNorm          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    ... (é‡å¤ N å±‚)
    â†“
LayerNorm
    â†“
Linear (æŠ•å½±åˆ°è¯æ±‡è¡¨)
    â†“
Softmax
    â†“
Output Probabilities
```

---

## ğŸ“š æ ¸å¿ƒç»„ä»¶

### 1. Transformer Block

æ¯ä¸ª Transformer Block åŒ…å«:
- Multi-Head Causal Self-Attention
- Feed-Forward Network
- 2 ä¸ª LayerNorm
- 2 ä¸ªæ®‹å·®è¿æ¥

```python
class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(...)
        self.ff = FeedForward(...)
        self.norm1 = LayerNorm(...)
        self.norm2 = LayerNorm(...)
    
    def forward(self, x):
        # Self-Attention + æ®‹å·®
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)
        x = x + shortcut
        
        # Feed-Forward + æ®‹å·®
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = x + shortcut
        
        return x
```

### 2. Feed-Forward Network

```python
FFN(x) = GELU(xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚
```

é€šå¸¸ FFN ä¸­é—´å±‚ç»´åº¦æ˜¯ `4 Ã— emb_dim`ã€‚

### 3. Layer Normalization

```python
LayerNorm(x) = Î³ Â· (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
```

å½’ä¸€åŒ–æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦ã€‚

---

## ğŸ’¡ ä»£ç ç¤ºä¾‹

```python
from gpt_model import GPTModel
import torch

# åˆ›å»º GPT æ¨¡å‹
model = GPTModel(GPT_CONFIG)

# è¾“å…¥
input_ids = torch.randint(0, 50257, (2, 10))  # (batch, seq_len)

# å‰å‘ä¼ æ’­
logits = model(input_ids)
print(f"è¾“å‡ºå½¢çŠ¶: {logits.shape}")  # (2, 10, 50257)

# æ–‡æœ¬ç”Ÿæˆ
from generating_text import generate_text

prompt = "Once upon a time"
generated = generate_text(
    model=model,
    prompt=prompt,
    max_new_tokens=50,
    temperature=0.7,
    top_k=40
)
print(generated)
```

---

## ğŸ¯ æ–‡æœ¬ç”Ÿæˆç­–ç•¥

### Greedy Decoding
æ¯æ¬¡é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ tokenã€‚

### Temperature Sampling
è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒçš„"é”åº¦"ï¼š
```python
probs = torch.softmax(logits / temperature, dim=-1)
```
- `temperature < 1`: æ›´ç¡®å®šæ€§
- `temperature > 1`: æ›´éšæœº

### Top-k Sampling
åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­é‡‡æ ·ã€‚

### Top-p (Nucleus) Sampling
ä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„æœ€å° token é›†åˆä¸­é‡‡æ ·ã€‚

---

## ğŸ”— ç›¸å…³ç« èŠ‚

- **ä¸Šä¸€ç« **: [Chapter 3 - æ³¨æ„åŠ›æœºåˆ¶](../chap3-attention_mechanisms/)
- **ä¸‹ä¸€ç« **: [Chapter 5 - é¢„è®­ç»ƒ](../chap5-pretraining/)

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ17æ—¥
