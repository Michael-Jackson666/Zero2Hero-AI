# Appendix E: LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒ

æœ¬é™„å½•ä»‹ç» LoRA (Low-Rank Adaptation)ï¼Œä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ã€‚

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

```
appendix-E/
â”œâ”€â”€ LoRA.py                  # LoRA å®ç°
â”œâ”€â”€ gpt_download.py          # ä¸‹è½½ GPT-2 æ¨¡å‹
â”œâ”€â”€ previous_chapters.py     # å‰å‡ ç« ä»£ç 
â”œâ”€â”€ loss-plot.pdf            # è®­ç»ƒæŸå¤±æ›²çº¿
â”œâ”€â”€ train.csv                # è®­ç»ƒæ•°æ®
â”œâ”€â”€ validation.csv           # éªŒè¯æ•°æ®
â”œâ”€â”€ test.csv                 # æµ‹è¯•æ•°æ®
â”œâ”€â”€ sms_spam_collection/     # åƒåœ¾é‚®ä»¶æ•°æ®é›†
â””â”€â”€ gpt2/                    # GPT-2 æ¨¡å‹
```

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- âœ… ç†è§£ LoRA åŸç†
- âœ… å®ç°ä½ç§©çŸ©é˜µåˆ†è§£
- âœ… æŒæ¡å‚æ•°é«˜æ•ˆå¾®è°ƒ
- âœ… å¯¹æ¯”å…¨å‚æ•°å¾®è°ƒå’Œ LoRA

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹

```bash
python gpt_download.py
```

### 2. ä½¿ç”¨ LoRA å¾®è°ƒ

```bash
python LoRA.py \
    --model gpt2 \
    --rank 8 \
    --alpha 16 \
    --epochs 5
```

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### LoRA åŸç†

LoRA é€šè¿‡ä½ç§©çŸ©é˜µåˆ†è§£æ¥å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼š

$$W' = W + \Delta W = W + BA$$

å…¶ä¸­:
- $W \in \mathbb{R}^{d \times k}$: åŸå§‹æƒé‡çŸ©é˜µï¼ˆå†»ç»“ï¼‰
- $B \in \mathbb{R}^{d \times r}$: ä½ç§©çŸ©é˜µ Bï¼ˆå¯è®­ç»ƒï¼‰
- $A \in \mathbb{R}^{r \times k}$: ä½ç§©çŸ©é˜µ Aï¼ˆå¯è®­ç»ƒï¼‰
- $r \ll \min(d, k)$: ç§©ï¼ˆé€šå¸¸ 4-64ï¼‰

**å‚æ•°å¯¹æ¯”**:
- åŸå§‹å‚æ•°: $d \times k$
- LoRA å‚æ•°: $d \times r + r \times k$
- å½“ $r=8$, $d=k=768$ æ—¶: $589,824$ â†’ $12,288$ (98% å‡å°‘ï¼)

### ä¼˜åŠ¿

âœ… **å‚æ•°æ•ˆç‡**: åªéœ€å¾®è°ƒ 0.1%-1% çš„å‚æ•°  
âœ… **å­˜å‚¨æ•ˆç‡**: æ¯ä¸ªä»»åŠ¡åªéœ€ä¿å­˜å°çš„ LoRA æƒé‡  
âœ… **æ¨ç†æ•ˆç‡**: å¯ä»¥åˆå¹¶æƒé‡ï¼Œæ— é¢å¤–å¼€é”€  
âœ… **å¤šä»»åŠ¡**: ä¸€ä¸ªåŸºåº§æ¨¡å‹ + å¤šä¸ª LoRA æ¨¡å—  

---

## ğŸ’¡ LoRA å®ç°

```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(
        self,
        in_features,
        out_features,
        rank=8,
        alpha=16,
        dropout=0.1
    ):
        super().__init__()
        
        # åŸå§‹æƒé‡ï¼ˆå†»ç»“ï¼‰
        self.linear = nn.Linear(in_features, out_features, bias=False)
        self.linear.weight.requires_grad = False
        
        # LoRA å‚æ•°
        self.lora_A = nn.Parameter(torch.randn(in_features, rank))
        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))
        
        # ç¼©æ”¾å› å­
        self.scaling = alpha / rank
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
    
    def forward(self, x):
        # åŸå§‹è¾“å‡º
        result = self.linear(x)
        
        # LoRA å¢é‡
        lora_out = (x @ self.lora_A @ self.lora_B) * self.scaling
        lora_out = self.dropout(lora_out)
        
        return result + lora_out
    
    def merge_weights(self):
        """åˆå¹¶ LoRA æƒé‡åˆ°åŸå§‹æƒé‡ï¼ˆæ¨ç†ä¼˜åŒ–ï¼‰"""
        self.linear.weight.data += (
            self.lora_B @ self.lora_A.T * self.scaling
        )
```

### åº”ç”¨ LoRA åˆ° GPT

```python
def apply_lora_to_gpt(model, rank=8, alpha=16):
    """å°† LoRA åº”ç”¨åˆ° GPT çš„æ³¨æ„åŠ›å±‚"""
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # åªæ›¿æ¢æ³¨æ„åŠ›å±‚çš„ QKV æŠ•å½±
            if any(x in name for x in ['q_proj', 'k_proj', 'v_proj', 'out_proj']):
                in_features = module.in_features
                out_features = module.out_features
                
                # åˆ›å»º LoRA å±‚
                lora_layer = LoRALayer(
                    in_features,
                    out_features,
                    rank=rank,
                    alpha=alpha
                )
                
                # å¤åˆ¶åŸå§‹æƒé‡
                lora_layer.linear.weight.data = module.weight.data.clone()
                
                # æ›¿æ¢
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                parent = model.get_submodule(parent_name)
                setattr(parent, child_name, lora_layer)
    
    return model
```

---

## ğŸ“Š å‚æ•°å¯¹æ¯”

### GPT-2 Base (124M å‚æ•°)

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° | ç™¾åˆ†æ¯” | å­˜å‚¨å¤§å° |
|------|-----------|-------|---------|
| **å…¨å‚æ•°å¾®è°ƒ** | 124M | 100% | ~500 MB |
| **LoRA (r=4)** | 0.3M | 0.24% | ~1.2 MB |
| **LoRA (r=8)** | 0.6M | 0.48% | ~2.4 MB |
| **LoRA (r=16)** | 1.2M | 0.97% | ~4.8 MB |

### GPT-2 Large (774M å‚æ•°)

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° | ç™¾åˆ†æ¯” | å­˜å‚¨å¤§å° |
|------|-----------|-------|---------|
| **å…¨å‚æ•°å¾®è°ƒ** | 774M | 100% | ~3 GB |
| **LoRA (r=8)** | 4M | 0.52% | ~16 MB |

---

## ğŸ¯ è®­ç»ƒç¤ºä¾‹

```python
from LoRA import apply_lora_to_model, train_with_lora

# åŠ è½½é¢„è®­ç»ƒ GPT-2
model = load_gpt2()

# åº”ç”¨ LoRA
model = apply_lora_to_model(
    model,
    rank=8,
    alpha=16,
    target_modules=['q_proj', 'v_proj']  # åªåœ¨ QV æŠ•å½±ä½¿ç”¨ LoRA
)

# å†»ç»“é LoRA å‚æ•°
for name, param in model.named_parameters():
    if 'lora_' not in name:
        param.requires_grad = False

# æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params:.2%})")

# è®­ç»ƒ
optimizer = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=1e-4
)

train_with_lora(model, train_loader, optimizer, epochs=5)

# ä¿å­˜ LoRA æƒé‡ï¼ˆä»…å‡  MBï¼‰
save_lora_weights(model, 'lora_weights.pt')
```

---

## ğŸ”§ é«˜çº§æŠ€å·§

### 1. é€‰æ‹©ç›®æ ‡å±‚

```python
# åªåœ¨æ³¨æ„åŠ›å±‚ä½¿ç”¨ LoRA
target_modules = ['q_proj', 'v_proj']

# ä¹Ÿå¯ä»¥åœ¨ FFN ä½¿ç”¨
target_modules = ['q_proj', 'v_proj', 'fc1', 'fc2']
```

### 2. è°ƒæ•´ç§© (Rank)

- **r=4**: æœ€å°‘å‚æ•°ï¼Œé€‚åˆå°ä»»åŠ¡
- **r=8**: å¹³è¡¡é€‰æ‹©ï¼ˆæ¨èï¼‰
- **r=16+**: æ›´å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œæ¥è¿‘å…¨å‚æ•°å¾®è°ƒ

### 3. Alpha ç¼©æ”¾

```python
scaling = alpha / rank

# alpha=16, rank=8 â†’ scaling=2 (æ¨è)
# alpha=rank â†’ scaling=1
```

### 4. æ¨ç†ä¼˜åŒ–

```python
# è®­ç»ƒååˆå¹¶æƒé‡
model.merge_lora_weights()

# æ¨ç†æ—¶æ— é¢å¤–å¼€é”€
output = model(input)
```

---

## ğŸ“– å‚è€ƒèµ„æ–™

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685): LoRA: Low-Rank Adaptation of Large Language Models
- [PEFT åº“](https://github.com/huggingface/peft): Hugging Face çš„å‚æ•°é«˜æ•ˆå¾®è°ƒåº“

---

## ğŸ”— ç›¸å…³ç« èŠ‚

- [Chapter 6 - åˆ†ç±»ä»»åŠ¡å¾®è°ƒ](../chap6-fine-tuning-for-classification/)
- [Chapter 7 - æŒ‡ä»¤å¾®è°ƒ](../chap7-fine-tuning-to-follow-instruction/)

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ17æ—¥
