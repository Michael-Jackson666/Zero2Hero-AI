# Chapter 5: é¢„è®­ç»ƒ

æœ¬ç« å®ç° GPT æ¨¡å‹çš„é¢„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬è®­ç»ƒå¾ªç¯ã€æŸå¤±è®¡ç®—ã€æ¨¡å‹ä¿å­˜å’Œæ–‡æœ¬ç”Ÿæˆã€‚

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

```
chap5-pretraining/
â”œâ”€â”€ train-llm.py                  # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ complete_training.py          # å®Œæ•´è®­ç»ƒæµç¨‹
â”œâ”€â”€ load-train.py                 # åŠ è½½æ•°æ®å’Œè®­ç»ƒ
â”œâ”€â”€ evaluate-text.py              # æ¨¡å‹è¯„ä¼°
â”œâ”€â”€ generate_new.py               # æ–°ç‰ˆæ–‡æœ¬ç”Ÿæˆ
â”œâ”€â”€ generate_old.py               # æ—§ç‰ˆæ–‡æœ¬ç”Ÿæˆ
â”œâ”€â”€ cross_entropy_explanation.py  # äº¤å‰ç†µæŸå¤±è¯¦è§£
â”œâ”€â”€ temperature-scaling.py        # æ¸©åº¦ç¼©æ”¾å®éªŒ
â”œâ”€â”€ top-k_sampling.py             # Top-k é‡‡æ ·å®ç°
â”œâ”€â”€ load-save-weight.py           # æ¨¡å‹æƒé‡ä¿å­˜/åŠ è½½
â”œâ”€â”€ check_model_size.py           # æ£€æŸ¥æ¨¡å‹å‚æ•°é‡
â”œâ”€â”€ seed_experiment.py            # éšæœºç§å­å®éªŒ
â”œâ”€â”€ previous_chapters.py          # å‰å‡ ç« ä»£ç å¯¼å…¥
â”œâ”€â”€ the-verdict.txt               # è®­ç»ƒæ•°æ®ç¤ºä¾‹
â”œâ”€â”€ loss-plot.pdf                 # è®­ç»ƒæŸå¤±æ›²çº¿
â”œâ”€â”€ temperature-plot.pdf          # æ¸©åº¦é‡‡æ ·å¯¹æ¯”
â””â”€â”€ load-gpt-model/               # é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
```

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- âœ… å®ç°å®Œæ•´çš„é¢„è®­ç»ƒå¾ªç¯
- âœ… ç†è§£äº¤å‰ç†µæŸå¤±
- âœ… æŒæ¡æ¨¡å‹è¯„ä¼°æ–¹æ³•
- âœ… å­¦ä¹ å„ç§é‡‡æ ·ç­–ç•¥
- âœ… æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®Œæ•´è®­ç»ƒæµç¨‹

```bash
python complete_training.py
```

**è®­ç»ƒå‚æ•°**:
```python
TRAINING_CONFIG = {
    "batch_size": 4,
    "max_epochs": 10,
    "learning_rate": 0.0001,
    "warmup_steps": 100,
    "weight_decay": 0.01,
    "grad_clip": 1.0
}
```

### 2. è¯„ä¼°æ¨¡å‹

```bash
python evaluate-text.py
```

**è¯„ä¼°æŒ‡æ ‡**:
- è®­ç»ƒæŸå¤± (Training Loss)
- éªŒè¯æŸå¤± (Validation Loss)
- å›°æƒ‘åº¦ (Perplexity)

### 3. ç”Ÿæˆæ–‡æœ¬

```bash
python generate_new.py
```

**ç”Ÿæˆç¤ºä¾‹**:
```python
from generate_new import generate

prompt = "The future of AI is"
output = generate(
    model=model,
    prompt=prompt,
    max_tokens=100,
    temperature=0.7,
    top_k=50
)
print(output)
```

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### é¢„è®­ç»ƒä»»åŠ¡

GPT ä½¿ç”¨**è‡ªå›å½’è¯­è¨€å»ºæ¨¡** (Autoregressive Language Modeling):

ç»™å®šå‰ $t-1$ ä¸ª tokenï¼Œé¢„æµ‹ç¬¬ $t$ ä¸ª tokenï¼š

$$P(x_t | x_1, x_2, ..., x_{t-1})$$

### æŸå¤±å‡½æ•°

ä½¿ç”¨**äº¤å‰ç†µæŸå¤±** (Cross-Entropy Loss):

$$\mathcal{L} = -\frac{1}{T}\sum_{t=1}^{T} \log P(x_t | x_1, ..., x_{t-1})$$

```python
loss = F.cross_entropy(
    logits.view(-1, vocab_size),
    targets.view(-1)
)
```

### å›°æƒ‘åº¦ (Perplexity)

è¡¡é‡æ¨¡å‹é¢„æµ‹è´¨é‡çš„æŒ‡æ ‡ï¼š

$$\text{PPL} = \exp(\mathcal{L})$$

- è¶Šä½è¶Šå¥½
- è¡¨ç¤ºæ¨¡å‹å¯¹ä¸‹ä¸€ä¸ª token çš„"å›°æƒ‘ç¨‹åº¦"

---

## ğŸ¯ è®­ç»ƒæµç¨‹

```python
def train_epoch(model, train_loader, optimizer):
    model.train()
    total_loss = 0
    
    for batch_idx, (input_ids, targets) in enumerate(train_loader):
        # å‰å‘ä¼ æ’­
        logits = model(input_ids)
        
        # è®¡ç®—æŸå¤±
        loss = F.cross_entropy(
            logits.view(-1, vocab_size),
            targets.view(-1)
        )
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), 
            max_norm=1.0
        )
        
        # å‚æ•°æ›´æ–°
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(train_loader)
```

---

## ğŸ“Š è®­ç»ƒç›‘æ§

### æŸå¤±æ›²çº¿

æŸ¥çœ‹ `loss-plot.pdf`:
- è®­ç»ƒæŸå¤±åº”ç¨³å®šä¸‹é™
- éªŒè¯æŸå¤±ä¸åº”æŒç»­ä¸Šå‡ï¼ˆè¿‡æ‹Ÿåˆè­¦å‘Šï¼‰

### ç”Ÿæˆè´¨é‡

å®šæœŸç”Ÿæˆæ ·æœ¬æ–‡æœ¬æ£€æŸ¥ï¼š
```bash
# Epoch 1: "The cat sat sat sat..."  (é‡å¤)
# Epoch 5: "The cat sat on mat."    (è¯­æ³•æ­£ç¡®)
# Epoch 10: "The cat sat on the comfortable mat." (æµç•…)
```

---

## ğŸ’¡ é‡‡æ ·ç­–ç•¥å¯¹æ¯”

### Temperature Scaling

```python
# temperature = 0.1 (æ›´ç¡®å®š)
"The cat sat on the mat."

# temperature = 1.0 (å¹³è¡¡)
"The cat wandered through the garden."

# temperature = 2.0 (æ›´éšæœº)
"The purple elephant danced joyfully."
```

æŸ¥çœ‹ `temperature-plot.pdf` äº†è§£æ¦‚ç‡åˆ†å¸ƒå˜åŒ–ã€‚

### Top-k Sampling

```python
# top_k = 1 (è´ªå©ª)
"The most common response is yes."

# top_k = 10
"The best answer might be yes."

# top_k = 50
"Perhaps the answer could be yes."
```

---

## ğŸ”§ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

### ä¿å­˜æ¨¡å‹

```python
# ä¿å­˜å®Œæ•´æ¨¡å‹
torch.save(model.state_dict(), 'gpt_model.pth')

# ä¿å­˜è®­ç»ƒçŠ¶æ€
checkpoint = {
    'model': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'epoch': epoch,
    'loss': loss
}
torch.save(checkpoint, 'checkpoint.pth')
```

### åŠ è½½æ¨¡å‹

```python
# åŠ è½½æƒé‡
model.load_state_dict(torch.load('gpt_model.pth'))

# æ¢å¤è®­ç»ƒ
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model'])
optimizer.load_state_dict(checkpoint['optimizer'])
start_epoch = checkpoint['epoch'] + 1
```

---

## ğŸ” å®ç”¨å·¥å…·

### æ£€æŸ¥æ¨¡å‹å¤§å°

```bash
python check_model_size.py
```

è¾“å‡º:
```
Total parameters: 124,439,808 (124.4M)
Trainable parameters: 124,439,808
Model size: 474.4 MB
```

### éšæœºç§å­å®éªŒ

```bash
python seed_experiment.py
```

éªŒè¯ä¸åŒéšæœºç§å­å¯¹è®­ç»ƒçš„å½±å“ã€‚

---

## ğŸ”— ç›¸å…³ç« èŠ‚

- **ä¸Šä¸€ç« **: [Chapter 4 - å®ç° GPT æ¨¡å‹](../chap4-implement_gpt_model/)
- **ä¸‹ä¸€ç« **: [Chapter 6 - åˆ†ç±»ä»»åŠ¡å¾®è°ƒ](../chap6-fine-tuning-for-classification/)

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ17æ—¥
