# Hugging Face å¤§æ¨¡å‹éƒ¨ç½²å­¦ä¹ ç¬”è®°

æœ¬ç›®å½•åŒ…å«ä½¿ç”¨ Hugging Face Transformers åº“è¿›è¡Œå¤§æ¨¡å‹éƒ¨ç½²å’Œåº”ç”¨çš„å­¦ä¹ ä»£ç å’Œç¬”è®°ã€‚

---

## ğŸ“š å­¦ä¹ ç›®æ ‡

- ğŸ¯ æŒæ¡ Hugging Face Transformers åº“çš„ä½¿ç”¨
- ğŸš€ å­¦ä¹ å¦‚ä½•éƒ¨ç½²å’Œè°ƒç”¨é¢„è®­ç»ƒå¤§æ¨¡å‹
- ğŸ’¡ ç†è§£å„ç§ NLP ä»»åŠ¡çš„å®ç°æ–¹æ³•
- ğŸ”§ å®è·µæ¨¡å‹æ¨ç†å’Œä¼˜åŒ–æŠ€å·§

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

```
huggingface/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ api_test01.py               # åŸºç¡€ç¤ºä¾‹ï¼šæ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€ç¿»è¯‘ï¼ˆæ¨èåœ¨ç»ˆç«¯è¿è¡Œï¼‰
â”œâ”€â”€ huggingface_examples.py     # å®Œæ•´ç¤ºä¾‹ï¼š8ç§å¸¸ç”¨NLPä»»åŠ¡ï¼ˆæ¨èåœ¨ç»ˆç«¯è¿è¡Œï¼‰
â””â”€â”€ demo.ipynb                  # Gradioäº¤äº’å¼æ¼”ç¤ºï¼ˆJupyter Notebookï¼‰
```

### æ–‡ä»¶ç”¨é€”

- **api_test01.py**: æ¼”ç¤º3ä¸ªåŸºç¡€NLPä»»åŠ¡ï¼ˆæ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€ç¿»è¯‘ï¼‰
- **huggingface_examples.py**: åŒ…å«8ä¸ªå¸¸ç”¨NLPä»»åŠ¡çš„å®Œæ•´ç¤ºä¾‹
- **demo.ipynb**: Gradioå¯è§†åŒ–ç•Œé¢æ¼”ç¤ºï¼ŒåŒ…å«ç®€åŒ–çš„æƒ…æ„Ÿåˆ†ç±»ç¤ºä¾‹

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n huggingface python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate huggingface

# å®‰è£…ä¾èµ–
pip install transformers datasets accelerate tokenizers huggingface_hub torch
```

### è¿è¡Œç¤ºä¾‹

```bash
# æ–¹å¼1: è¿è¡ŒPythonè„šæœ¬ï¼ˆæ¨èï¼‰
python api_test01.py                 # åŸºç¡€ç¤ºä¾‹
python huggingface_examples.py       # å®Œæ•´ç¤ºä¾‹

# æ–¹å¼2: è¿è¡ŒJupyter Notebook
jupyter notebook demo.ipynb          # Gradioäº¤äº’å¼æ¼”ç¤º
```

**âš ï¸ é‡è¦æç¤º**ï¼š
- **Pythonè„šæœ¬**ï¼šåœ¨ç»ˆç«¯è¿è¡Œï¼Œæ‰€æœ‰transformersåŠŸèƒ½æ­£å¸¸
- **Jupyter Notebook**ï¼šç”±äºPyTorchä¾èµ–é—®é¢˜ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæ¼”ç¤º
- å»ºè®®ä¼˜å…ˆä½¿ç”¨Pythonè„šæœ¬å­¦ä¹ Hugging Faceæ¨¡å‹

---

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ

### 1. Pipeline API

Hugging Face æä¾›çš„é«˜çº§æ¥å£ï¼Œç®€åŒ–äº†æ¨¡å‹ä½¿ç”¨æµç¨‹ï¼š

```python
from transformers import pipeline

# åˆ›å»ºç®¡é“
generator = pipeline('text-generation', model='gpt2')

# ä½¿ç”¨ç®¡é“
result = generator("Hello world")
```

**ä¼˜åŠ¿**:
- è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å’Œåˆ†è¯å™¨
- å¤„ç†è¾“å…¥é¢„å¤„ç†å’Œè¾“å‡ºåå¤„ç†
- æ”¯æŒæ‰¹å¤„ç†å’Œæµå¼è¾“å‡º

### 2. å¸¸ç”¨ä»»åŠ¡ç±»å‹

| ä»»åŠ¡ | Pipelineåç§° | è¯´æ˜ |
|------|-------------|------|
| æ–‡æœ¬ç”Ÿæˆ | `text-generation` | GPTç³»åˆ—æ¨¡å‹ |
| æ–‡æœ¬åˆ†ç±» | `text-classification` | æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±» |
| é—®ç­” | `question-answering` | åŸºäºä¸Šä¸‹æ–‡çš„é—®ç­” |
| ç¿»è¯‘ | `translation_XX_to_YY` | å¤šè¯­è¨€ç¿»è¯‘ |
| æ‘˜è¦ | `summarization` | æ–‡æœ¬æ‘˜è¦ç”Ÿæˆ |
| NER | `ner` | å‘½åå®ä½“è¯†åˆ« |
| é›¶æ ·æœ¬åˆ†ç±» | `zero-shot-classification` | æ— éœ€è®­ç»ƒçš„åˆ†ç±» |
| å¡«å…… | `fill-mask` | BERTé£æ ¼çš„æ©ç å¡«å…… |

### 3. æ¨¡å‹åŠ è½½æ–¹å¼

```python
# æ–¹å¼1: ä½¿ç”¨ pipelineï¼ˆæ¨èï¼‰
generator = pipeline('text-generation', model='gpt2')

# æ–¹å¼2: æ‰‹åŠ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')

# ä½¿ç”¨æ¨¡å‹
inputs = tokenizer("Hello", return_tensors="pt")
outputs = model.generate(**inputs)
text = tokenizer.decode(outputs[0])
```

---

## ğŸ¯ å®è·µæ¡ˆä¾‹

### æ¡ˆä¾‹1: æ–‡æœ¬ç”Ÿæˆï¼ˆapi_test01.pyï¼‰

```python
from transformers import pipeline

generator = pipeline('text-generation', model='gpt2')
result = generator(
    "Explain the theory of relativity in simple terms.",
    max_length=100,
    num_return_sequences=1
)
print(result[0]['generated_text'])
```

**å…³é”®å‚æ•°**:
- `max_length`: ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦
- `num_return_sequences`: è¿”å›ç»“æœæ•°é‡
- `temperature`: æ§åˆ¶éšæœºæ€§ï¼ˆ0.7-1.0ï¼‰
- `top_k`: Top-Ké‡‡æ ·
- `top_p`: Nucleusé‡‡æ ·

### æ¡ˆä¾‹2: é—®ç­”ç³»ç»Ÿ

```python
qa = pipeline('question-answering')
result = qa(
    question="What is AI?",
    context="AI is artificial intelligence..."
)
print(result['answer'])
```

### æ¡ˆä¾‹3: å¤šè¯­è¨€ç¿»è¯‘

```python
translator = pipeline('translation_en_to_fr', model='t5-small')
result = translator("Hello, how are you?")
print(result[0]['translation_text'])
```

---

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. ä½¿ç”¨ GPU/MPS åŠ é€Ÿ

```python
# è‡ªåŠ¨ä½¿ç”¨å¯ç”¨çš„åŠ é€Ÿè®¾å¤‡
generator = pipeline('text-generation', model='gpt2', device=0)

# macOS ä½¿ç”¨ MPS
import torch
device = "mps" if torch.backends.mps.is_available() else "cpu"
generator = pipeline('text-generation', model='gpt2', device=device)
```

### 2. æ‰¹å¤„ç†

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªè¾“å…¥
texts = ["Text 1", "Text 2", "Text 3"]
results = generator(texts, batch_size=8)
```

### 3. æ¨¡å‹é‡åŒ–

```python
# ä½¿ç”¨é‡åŒ–æ¨¡å‹å‡å°‘å†…å­˜å ç”¨
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    'gpt2',
    load_in_8bit=True,  # 8ä½é‡åŒ–
    device_map='auto'
)
```

### 4. ç¼“å­˜ç®¡ç†

```python
# æ¨¡å‹é»˜è®¤ç¼“å­˜ä½ç½®: ~/.cache/huggingface/

# è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
import os
os.environ['TRANSFORMERS_CACHE'] = '/path/to/cache'

# æ¸…ç†ç¼“å­˜
# rm -rf ~/.cache/huggingface/hub/*
```

---

## ğŸ“Š å¸¸ç”¨æ¨¡å‹æ¨è

### æ–‡æœ¬ç”Ÿæˆ
- **GPT-2**: `gpt2`, `gpt2-medium`, `gpt2-large`
- **GPT-Neo**: `EleutherAI/gpt-neo-1.3B`, `EleutherAI/gpt-neo-2.7B`
- **BLOOM**: `bigscience/bloom-560m`, `bigscience/bloom-1b7`

### é—®ç­”ç³»ç»Ÿ
- **BERT**: `bert-large-uncased-whole-word-masking-finetuned-squad`
- **RoBERTa**: `deepset/roberta-base-squad2`
- **ELECTRA**: `google/electra-base-discriminator`

### ç¿»è¯‘
- **T5**: `t5-small`, `t5-base`, `t5-large`
- **mBART**: `facebook/mbart-large-50-many-to-many-mmt`
- **MarianMT**: `Helsinki-NLP/opus-mt-en-zh`

### ä¸­æ–‡æ¨¡å‹
- **ChatGLM**: `THUDM/chatglm-6b`, `THUDM/chatglm2-6b`
- **Qwen**: `Qwen/Qwen-7B-Chat`
- **Baichuan**: `baichuan-inc/Baichuan2-7B-Chat`

---

## ğŸ› ï¸ å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Ÿ
```bash
# ä½¿ç”¨é•œåƒæº
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
git lfs install
git clone https://huggingface.co/gpt2
```

### Q2: å†…å­˜ä¸è¶³ï¼Ÿ
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ `gpt2` è€Œé `gpt2-large`ï¼‰
- å¯ç”¨æ¨¡å‹é‡åŒ–ï¼ˆ8bit/4bitï¼‰
- å‡å° `batch_size`
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

### Q3: é€Ÿåº¦å¤ªæ…¢ï¼Ÿ
- ä½¿ç”¨ GPU/MPS åŠ é€Ÿ
- å¯ç”¨æ‰¹å¤„ç†
- ä½¿ç”¨æ›´å°çš„ `max_length`
- è€ƒè™‘ä½¿ç”¨ ONNX Runtime

### Q4: å¦‚ä½•ä½¿ç”¨ç§æœ‰æ¨¡å‹ï¼Ÿ
```bash
# ç™»å½• Hugging Face
huggingface-cli login

# è¾“å…¥ Access Token
```

### Q5: Jupyter Notebookä¸­transformersæ— æ³•ä½¿ç”¨ï¼Ÿ
**é—®é¢˜æè¿°**: notebook kernelä¸­PyTorchæŸåï¼Œå¯¼è‡´ `AttributeError: module 'torch' has no attribute 'Tensor'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: åœ¨ç»ˆç«¯è¿è¡ŒPythonè„šæœ¬ï¼ˆæ¨èï¼‰
python api_test01.py

# æ–¹æ¡ˆ2: ä½¿ç”¨demo.ipynbä¸­çš„ç®€åŒ–ç‰ˆGradioæ¼”ç¤º
jupyter notebook demo.ipynb

# æ–¹æ¡ˆ3: é‡å»ºcondaç¯å¢ƒ
conda env remove -n huggingface
conda create -n huggingface python=3.10 -y
conda activate huggingface
pip install transformers datasets accelerate tokenizers torch gradio
```

**æ³¨æ„**: ç»ˆç«¯è¿è¡Œçš„Pythonè„šæœ¬å¯ä»¥æ­£å¸¸ä½¿ç”¨æ‰€æœ‰åŠŸèƒ½

---

## ğŸ“– å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers/)
- [Hugging Face Hub](https://huggingface.co/models)
- [Datasets æ–‡æ¡£](https://huggingface.co/docs/datasets/)

### æ•™ç¨‹
- [Hugging Face Course](https://huggingface.co/course/)
- [Fine-tuning Tutorial](https://huggingface.co/docs/transformers/training)
- [Pipeline API](https://huggingface.co/docs/transformers/main_classes/pipelines)

### ç¤¾åŒº
- [Hugging Face Forums](https://discuss.huggingface.co/)
- [GitHub Issues](https://github.com/huggingface/transformers/issues)

---

## ğŸ“ å­¦ä¹ è¿›åº¦

- [x] ç¯å¢ƒæ­å»ºå’ŒåŸºç¡€é…ç½®
- [x] Pipeline API åŸºç¡€ä½¿ç”¨
- [x] æ–‡æœ¬ç”Ÿæˆä»»åŠ¡å®è·µ
- [x] é—®ç­”ç³»ç»Ÿå®ç°
- [x] ç¿»è¯‘åŠŸèƒ½æµ‹è¯•
- [x] Gradioå¯è§†åŒ–ç•Œé¢åˆ›å»º
- [x] åŸºç¡€NLPä»»åŠ¡æ¼”ç¤ºï¼ˆ8ç§ä»»åŠ¡ï¼‰
- [ ] æ¨¡å‹å¾®è°ƒï¼ˆFine-tuningï¼‰
- [ ] è‡ªå®šä¹‰æ•°æ®é›†è®­ç»ƒ
- [ ] æ¨¡å‹é‡åŒ–å’Œä¼˜åŒ–
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [ ] APIæœåŠ¡æ­å»ºï¼ˆFastAPIï¼‰
- [ ] è§£å†³Jupyter Notebookä¸­çš„PyTorchä¾èµ–é—®é¢˜

---

## ğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’

1. **æ¨¡å‹å¾®è°ƒ**: åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
2. **æ€§èƒ½ä¼˜åŒ–**: ç ”ç©¶é‡åŒ–ã€å‰ªæç­‰ä¼˜åŒ–æŠ€æœ¯
3. **éƒ¨ç½²å®è·µ**: ä½¿ç”¨ FastAPI æ­å»ºæ¨¡å‹æ¨ç†æœåŠ¡
4. **å¤šæ¨¡æ€**: æ¢ç´¢å›¾æ–‡ã€è¯­éŸ³ç­‰å¤šæ¨¡æ€æ¨¡å‹
5. **RAGç³»ç»Ÿ**: æ„å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ

---

## ğŸ’» å¼€å‘ç¯å¢ƒ

- **ç³»ç»Ÿ**: macOS (Apple Silicon)
- **Python**: 3.10
- **åŠ é€Ÿ**: MPS (Metal Performance Shaders)
- **ä¸»è¦ä¾èµ–**:
  - transformers: 4.57.1
  - torch: 2.9.1
  - datasets: 2.12.0
  - accelerate: 1.11.0
  - gradio: 5.49.1
  - peft: 0.18.0
  - optimum: 2.0.0
  - sentencepiece: 0.2.1

### å·²çŸ¥é—®é¢˜

âš ï¸ **Jupyter Notebook PyTorché—®é¢˜**:
- **ç—‡çŠ¶**: `AttributeError: module 'torch' has no attribute 'Tensor'`
- **å½±å“**: Notebookä¸­æ— æ³•ä½¿ç”¨transformersåº“
- **ä¸´æ—¶æ–¹æ¡ˆ**: ä½¿ç”¨ç»ˆç«¯è¿è¡ŒPythonè„šæœ¬
- **çŠ¶æ€**: å¾…è§£å†³

âœ… **ç»ˆç«¯è¿è¡Œæ­£å¸¸**:
- æ‰€æœ‰Pythonè„šæœ¬åœ¨ç»ˆç«¯ä¸­è¿è¡Œå®Œå…¨æ­£å¸¸
- transformersã€torchç­‰åº“åŠŸèƒ½å®Œæ•´

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ”¹è¿›å­¦ä¹ ç¬”è®°ï¼

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ20æ—¥

**å­¦ä¹ çŠ¶æ€**: ğŸš€ è¿›è¡Œä¸­

**æ¨èä½¿ç”¨æ–¹å¼**: 
- ğŸ“ å­¦ä¹ ï¼šè¿è¡ŒPythonè„šæœ¬ï¼ˆ`api_test01.py`, `huggingface_examples.py`ï¼‰
- ğŸ¨ æ¼”ç¤ºï¼šä½¿ç”¨Gradioç•Œé¢ï¼ˆ`demo.ipynb`ï¼Œç®€åŒ–ç‰ˆï¼‰
- ğŸ”§ ç”Ÿäº§ï¼šç­‰å¾…PyTorché—®é¢˜è§£å†³åä½¿ç”¨å®Œæ•´åŠŸèƒ½
