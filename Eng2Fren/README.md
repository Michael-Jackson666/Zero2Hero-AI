# English â†” French Translator

åŸºäº Transformer æ¶æ„çš„è‹±æ³•åŒå‘ç¿»è¯‘æ¨¡å‹å®ç°ã€‚

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

```
Eng2Fren/
â”œâ”€â”€ transformer.py                    # Transformer æ¨¡å‹æ ¸å¿ƒå®ç°
â”œâ”€â”€ transformer-d2l.py                # è®­ç»ƒè„šæœ¬ (åŸºäº D2L)
â”œâ”€â”€ transformer_inference.py          # æ¨ç†æ¨¡å—
â”œâ”€â”€ simple_translator.py              # äº¤äº’å¼ç¿»è¯‘å™¨ (æ¨è)
â”œâ”€â”€ mini_translator.py                # è½»é‡çº§ç¿»è¯‘å™¨
â”œâ”€â”€ batch_translate.py                # æ‰¹é‡ç¿»è¯‘å·¥å…·
â”œâ”€â”€ transformer_fra_eng.pth           # è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ example_input.txt                 # ç¤ºä¾‹è¾“å…¥æ–‡ä»¶
â”œâ”€â”€ example_input_translated.txt      # ç¤ºä¾‹ç¿»è¯‘ç»“æœ
â””â”€â”€ batch_translation_results.txt     # æ‰¹é‡ç¿»è¯‘ç»“æœ
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install torch d2l numpy matplotlib
```

### 2. è®­ç»ƒæ¨¡å‹

```bash
# ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ
python transformer-d2l.py

# è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
python transformer-d2l.py --num_epochs 50 --batch_size 128
```

**è®­ç»ƒå‚æ•°è¯´æ˜**:
- `--num_epochs`: è®­ç»ƒè½®æ•° (é»˜è®¤: 30)
- `--batch_size`: æ‰¹æ¬¡å¤§å° (é»˜è®¤: 64)
- `--num_hiddens`: éšè—å±‚ç»´åº¦ (é»˜è®¤: 256)
- `--num_heads`: æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: 8)
- `--num_layers`: Encoder/Decoder å±‚æ•° (é»˜è®¤: 2)

### 3. ä½¿ç”¨ç¿»è¯‘

#### æ–¹å¼1: äº¤äº’å¼ç¿»è¯‘å™¨ (æ¨è)

```bash
python simple_translator.py
```

**åŠŸèƒ½ç‰¹æ€§**:
- âœ… è‡ªåŠ¨è¯­è¨€æ£€æµ‹
- âœ… å®æ—¶ç¿»è¯‘
- âœ… å†å²è®°å½•
- âœ… å‹å¥½çš„äº¤äº’ç•Œé¢

#### æ–¹å¼2: å‘½ä»¤è¡Œç¿»è¯‘

```bash
# å¿«é€Ÿå•å¥ç¿»è¯‘
python mini_translator.py "Hello, how are you?"

# æŒ‡å®šç¿»è¯‘æ–¹å‘
python mini_translator.py "Bonjour" --direction fr-en
```

#### æ–¹å¼3: æ‰¹é‡æ–‡ä»¶ç¿»è¯‘

```bash
# ç¿»è¯‘æ–‡æœ¬æ–‡ä»¶
python batch_translate.py --input example_input.txt --output output.txt

# å¹¶è¡Œå¤„ç†
python batch_translate.py --input large_file.txt --output result.txt --parallel 4
```

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### Python API è°ƒç”¨

```python
from transformer_inference import Translator

# åˆå§‹åŒ–ç¿»è¯‘å™¨
translator = Translator(model_path='transformer_fra_eng.pth')

# è‹±è¯‘æ³•
result = translator.translate("Hello world", direction='en-fr')
print(result)  # "Bonjour le monde"

# æ³•è¯‘è‹±
result = translator.translate("Bonjour", direction='fr-en')
print(result)  # "Hello"
```

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

åŸºäº "Attention Is All You Need" è®ºæ–‡å®ç°çš„æ ‡å‡† Transformer æ¶æ„:

- **Encoder**: Multi-Head Self-Attention + Feed Forward
- **Decoder**: Masked Multi-Head Self-Attention + Cross-Attention + Feed Forward
- **ä½ç½®ç¼–ç **: æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç 
- **ä¼˜åŒ–å™¨**: Adam with learning rate scheduling

**é»˜è®¤é…ç½®**:
- æ¨¡å‹ç»´åº¦: 256
- æ³¨æ„åŠ›å¤´æ•°: 8
- Encoder/Decoder å±‚æ•°: 2
- å‰é¦ˆç½‘ç»œç»´åº¦: 1024
- Dropout: 0.1

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

åœ¨æ ‡å‡†æµ‹è¯•é›†ä¸Šçš„è¡¨ç°:

| æ–¹å‘ | BLEU Score | è®­ç»ƒæ—¶é—´ (GPU) |
|------|-----------|---------------|
| EN â†’ FR | ~35-40 | 2-3 å°æ—¶ |
| FR â†’ EN | ~33-38 | 2-3 å°æ—¶ |

**æµ‹è¯•ç¯å¢ƒ**: Tesla V100 / RTX 4090

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Ÿ
A: è¯·å…ˆè¿è¡Œ `python transformer-d2l.py` è®­ç»ƒæ¨¡å‹ã€‚

### Q: CUDA out of memoryï¼Ÿ
A: å‡å° `--batch_size` å‚æ•°ï¼Œæˆ–ä½¿ç”¨ `--num_hiddens 128` å‡å°æ¨¡å‹å¤§å°ã€‚

### Q: ç¿»è¯‘è´¨é‡ä¸ä½³ï¼Ÿ
A: å¢åŠ è®­ç»ƒè½®æ•° `--num_epochs 50`ï¼Œæˆ–ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ `--num_hiddens 512`ã€‚

### Q: ç¿»è¯‘é€Ÿåº¦æ…¢ï¼Ÿ
A: ä½¿ç”¨ GPU æ¨ç†ï¼Œæˆ–å‡å° `beam_size` å‚æ•°ã€‚

---

## ğŸ“– å‚è€ƒèµ„æ–™

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - åŸè®ºæ–‡
- [D2L.ai](https://d2l.ai/) - æ·±åº¦å­¦ä¹ æ•™ç¨‹
- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/) - æ³¨é‡Šç‰ˆå®ç°

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ16æ—¥
